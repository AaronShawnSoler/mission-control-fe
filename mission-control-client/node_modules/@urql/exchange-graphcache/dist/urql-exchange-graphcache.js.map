{"version":3,"file":"urql-exchange-graphcache.js","sources":["../src/ast/node.ts","../src/helpers/help.ts","../src/store/keys.ts","../src/store/timing.ts","../src/store/data.ts","../src/operations/shared.ts","../src/operations/write.ts","../src/operations/invalidate.ts","../src/store/store.ts","../src/ast/variables.ts","../src/ast/schemaPredicates.ts","../src/ast/traversal.ts","../src/operations/query.ts","../src/cacheExchange.ts","../src/populateExchange.ts"],"sourcesContent":["import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  GraphQLOutputType,\n  Kind,\n  isWrappingType,\n} from 'graphql';\n\nimport { SelectionSet, GraphQLFlatType } from '../types';\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias !== undefined ? node.alias.value : getName(node);\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet =>\n  node.selectionSet !== undefined ? node.selectionSet.selections : [];\n\nexport const getTypeCondition = ({\n  typeCondition,\n}: {\n  typeCondition?: NamedTypeNode;\n}): string | null =>\n  typeCondition !== undefined ? getName(typeCondition) : null;\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n\nexport const unwrapType = (\n  type: null | undefined | GraphQLOutputType\n): GraphQLFlatType | null => {\n  if (isWrappingType(type)) {\n    return unwrapType(type.ofType);\n  }\n\n  return type || null;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/help.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\nimport { ErrorCode } from '../types';\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\nconst helpUrl =\n  '\\nhttps://github.com/FormidableLabs/urql-exchange-graphcache/blob/master/docs/help.md#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import { stringifyVariables } from 'urql/core';\nimport { Variables, FieldInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: null | Variables) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\n/** Prefix key with its owner type Link / Record */\nexport const prefixKey = (owner: 'l' | 'r', key: string) => `${owner}|${key}`;\n","export const defer: (fn: () => void) => void =\n  process.env.NODE_ENV === 'production' && typeof Promise !== 'undefined'\n    ? Promise.prototype.then.bind(Promise.resolve())\n    : fn => setTimeout(fn, 0);\n","import {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n} from '../types';\nimport { invariant, currentDebugStack } from '../helpers/help';\nimport { fieldInfoOfKey, joinKeys, prefixKey } from './keys';\nimport { defer } from './timing';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OptimisticMap<T> = Record<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OptimisticMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n  keys: number[];\n}\n\nexport interface InMemoryData {\n  persistenceScheduled: boolean;\n  persistenceBatch: SerializedEntries;\n  gcScheduled: boolean;\n  gcBatch: Set<string>;\n  queryRootKey: string;\n  refCount: Dict<number>;\n  refLock: OptimisticMap<Dict<number>>;\n  records: NodeMap<EntityField>;\n  links: NodeMap<Link>;\n  storage: StorageAdapter | null;\n}\n\nexport const makeDict = (): any => Object.create(null);\n\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Set<string> = null;\nlet currentOptimisticKey: null | number = null;\n\nconst makeNodeMap = <T>(): NodeMap<T> => ({\n  optimistic: makeDict(),\n  base: new Map(),\n  keys: [],\n});\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  data: InMemoryData,\n  optimisticKey: number | null\n) => {\n  currentData = data;\n  currentDependencies = new Set();\n  currentOptimisticKey = optimisticKey;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  const data = currentData!;\n\n  if (!data.gcScheduled && data.gcBatch.size > 0) {\n    data.gcScheduled = true;\n    defer(() => {\n      gc(data);\n    });\n  }\n\n  if (data.storage && !data.persistenceScheduled) {\n    data.persistenceScheduled = true;\n    defer(() => {\n      data.storage!.write(data.persistenceBatch);\n      data.persistenceScheduled = false;\n      data.persistenceBatch = makeDict();\n    });\n  }\n\n  currentData = null;\n  currentDependencies = null;\n  currentOptimisticKey = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Set<string> => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  persistenceScheduled: false,\n  persistenceBatch: makeDict(),\n  gcScheduled: false,\n  queryRootKey,\n  gcBatch: new Set(),\n  refCount: makeDict(),\n  refLock: makeDict(),\n  links: makeNodeMap(),\n  records: makeNodeMap(),\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  let keymap: KeyMap<Dict<T | undefined>>;\n  if (currentOptimisticKey) {\n    // If the optimistic map doesn't exist yet, it' created, and\n    // the optimistic key is stored (in order of priority)\n    if (map.optimistic[currentOptimisticKey] === undefined) {\n      map.optimistic[currentOptimisticKey] = new Map();\n      map.keys.unshift(currentOptimisticKey);\n    }\n\n    keymap = map.optimistic[currentOptimisticKey];\n  } else {\n    keymap = map.base;\n  }\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    const node = optimistic.get(entityKey);\n    // If the node and node value exists it is returned, including undefined\n    if (node !== undefined && fieldKey in node) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  const node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Clears an optimistic layers from a NodeMap */\nconst clearOptimisticNodes = <T>(map: NodeMap<T>, optimisticKey: number) => {\n  // Check whether the optimistic layer exists on the NodeMap\n  const index = map.keys.indexOf(optimisticKey);\n  if (index > -1) {\n    // Then delete it and splice out the optimisticKey\n    delete map.optimistic[optimisticKey];\n    map.keys.splice(index, 1);\n  }\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gcBatch */\nconst updateRCForEntity = (\n  gcBatch: void | Set<string>,\n  refCount: Dict<number>,\n  entityKey: string,\n  by: number\n) => {\n  // Retrieve the reference count\n  const count = refCount[entityKey] !== undefined ? refCount[entityKey] : 0;\n  // Adjust it by the \"by\" value\n  const newCount = (refCount[entityKey] = (count + by) | 0);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gcBatch !== undefined) {\n    if (newCount <= 0) gcBatch.add(entityKey);\n    else if (count <= 0 && newCount > 0) gcBatch.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gcBatch */\nconst updateRCForLink = (\n  gcBatch: void | Set<string>,\n  refCount: Dict<number>,\n  link: Link | undefined,\n  by: number\n) => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gcBatch, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      const entityKey = link[i];\n      if (entityKey) {\n        updateRCForEntity(gcBatch, refCount, entityKey, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n) => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = (data: InMemoryData) => {\n  // Reset gcScheduled flag\n  data.gcScheduled = false;\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  data.gcBatch.forEach(entityKey => {\n    // Check first whether the reference count is still 0\n    const rc = data.refCount[entityKey] || 0;\n    if (rc <= 0) {\n      // Each optimistic layer may also still contain some references to marked entities\n      for (const optimisticKey in data.refLock) {\n        const refCount = data.refLock[optimisticKey];\n        const locks = refCount[entityKey] || 0;\n        // If the optimistic layer has any references to the entity, don't GC it,\n        // otherwise delete the reference count from the optimistic layer\n        if (locks > 0) return;\n        delete refCount[entityKey];\n      }\n\n      // All conditions are met: The entity can be deleted\n\n      // Delete the reference count, and delete the entity from the GC batch\n      delete data.refCount[entityKey];\n      data.gcBatch.delete(entityKey);\n\n      // Delete the record and for each of its fields, delete them on the persistence\n      // layer if one is present\n      // No optimistic data needs to be deleted, as the entity is not being referenced by\n      // anything and optimistic layers will eventually be deleted anyway\n      const recordsNode = data.records.base.get(entityKey);\n      if (recordsNode !== undefined) {\n        data.records.base.delete(entityKey);\n        if (data.storage) {\n          for (const fieldKey in recordsNode) {\n            const key = prefixKey('r', joinKeys(entityKey, fieldKey));\n            data.persistenceBatch[key] = undefined;\n          }\n        }\n      }\n\n      // Delete all the entity's links, but also update the reference count\n      // for those links (which can lead to an unrolled recursive GC of the children)\n      const linkNode = data.links.base.get(entityKey);\n      if (linkNode !== undefined) {\n        data.links.base.delete(entityKey);\n        for (const fieldKey in linkNode) {\n          // Delete all links from the persistence layer if one is present\n          if (data.storage) {\n            const key = prefixKey('l', joinKeys(entityKey, fieldKey));\n            data.persistenceBatch[key] = undefined;\n          }\n\n          updateRCForLink(data.gcBatch, data.refCount, linkNode[fieldKey], -1);\n        }\n      }\n    } else {\n      data.gcBatch.delete(entityKey);\n    }\n  });\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies!.add(entityKey);\n    } else if (fieldKey !== undefined) {\n      currentDependencies!.add(joinKeys(entityKey, fieldKey));\n    }\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n  if (currentData!.storage && !currentOptimisticKey) {\n    const key = prefixKey('r', joinKeys(entityKey, fieldKey));\n    currentData!.persistenceBatch[key] = value;\n  }\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: Dict<number>;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gcBatch: void | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount =\n      data.refLock[currentOptimisticKey] ||\n      (data.refLock[currentOptimisticKey] = makeDict());\n    links = data.links.optimistic[currentOptimisticKey];\n  } else {\n    if (data.storage) {\n      const key = prefixKey('l', joinKeys(entityKey, fieldKey));\n      data.persistenceBatch[key] = link;\n    }\n    refCount = data.refCount;\n    links = data.links.base;\n    gcBatch = data.gcBatch;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links !== undefined ? links.get(entityKey) : undefined;\n  const prevLink = prevLinkNode !== undefined ? prevLinkNode[fieldKey] : null;\n\n  // Update dependencies\n  updateDependencies(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gcBatch, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gcBatch, refCount, link, 1);\n};\n\n/** Removes an optimistic layer of links and records */\nexport const clearOptimistic = (data: InMemoryData, optimisticKey: number) => {\n  // We also delete the optimistic reference locks\n  delete data.refLock[optimisticKey];\n  clearOptimisticNodes(data.records, optimisticKey);\n  clearOptimisticNodes(data.links, optimisticKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState(data, 0);\n  for (const key in entries) {\n    const dotIndex = key.indexOf('.');\n    const entityKey = key.slice(2, dotIndex);\n    const fieldKey = key.slice(dotIndex + 1);\n    switch (key.charCodeAt(0)) {\n      case 108:\n        writeLink(entityKey, fieldKey, entries[key] as Link);\n        break;\n      case 114:\n        writeRecord(entityKey, fieldKey, entries[key]);\n        break;\n    }\n  }\n  clearDataState();\n  data.storage = storage;\n};\n","import { FieldNode, InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, pushDebugNode } from '../helpers/help';\nimport { hasField } from '../store/data';\nimport { Store, keyOfField } from '../store';\n\nimport {\n  Fragments,\n  Variables,\n  SelectionSet,\n  DataField,\n  NullArray,\n  Data,\n} from '../types';\n\nimport {\n  SchemaPredicates,\n  getTypeCondition,\n  getFieldArguments,\n  shouldInclude,\n  isFieldNode,\n  isInlineFragment,\n  getSelectionSet,\n  getName,\n} from '../ast';\n\ninterface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  ctx: Context\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return !getSelectionSet(node).some(node => {\n    if (!isFieldNode(node)) return false;\n    const fieldKey = keyOfField(\n      getName(node),\n      getFieldArguments(node, ctx.variables)\n    );\n    return !hasField(entityKey, fieldKey);\n  });\n};\n\nexport class SelectionIterator {\n  typename: void | string;\n  entityKey: string;\n  indexStack: number[];\n  context: Context;\n  selectionStack: SelectionSet[];\n\n  constructor(\n    typename: void | string,\n    entityKey: string,\n    select: SelectionSet,\n    ctx: Context\n  ) {\n    this.typename = typename;\n    this.entityKey = entityKey;\n    this.context = ctx;\n    this.indexStack = [0];\n    this.selectionStack = [select];\n  }\n\n  next(): void | FieldNode {\n    while (this.indexStack.length !== 0) {\n      const index = this.indexStack[this.indexStack.length - 1]++;\n      const select = this.selectionStack[this.selectionStack.length - 1];\n      if (index >= select.length) {\n        this.indexStack.pop();\n        this.selectionStack.pop();\n        continue;\n      } else {\n        const node = select[index];\n        if (!shouldInclude(node, this.context.variables)) {\n          continue;\n        } else if (!isFieldNode(node)) {\n          // A fragment is either referred to by FragmentSpread or inline\n          const fragmentNode = !isInlineFragment(node)\n            ? this.context.fragments[getName(node)]\n            : node;\n\n          if (fragmentNode !== undefined) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(this.typename, fragmentNode);\n            }\n\n            const isMatching =\n              this.context.schemaPredicates !== undefined\n                ? this.context.schemaPredicates.isInterfaceOfType(\n                    getTypeCondition(fragmentNode),\n                    this.typename\n                  )\n                : isFragmentHeuristicallyMatching(\n                    fragmentNode,\n                    this.typename,\n                    this.entityKey,\n                    this.context\n                  );\n\n            if (isMatching) {\n              this.indexStack.push(0);\n              this.selectionStack.push(getSelectionSet(fragmentNode));\n            }\n          }\n\n          continue;\n        } else if (getName(node) === '__typename') {\n          continue;\n        } else {\n          return node;\n        }\n      }\n    }\n\n    return undefined;\n  }\n}\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x === undefined ? null : (x as Data | NullArray<Data>);\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  getSelectionSet,\n  normalizeVariables,\n  getFragmentTypeName,\n  getName,\n  getFieldArguments,\n  SchemaPredicates,\n} from '../ast';\n\nimport {\n  NullArray,\n  Fragments,\n  Variables,\n  Data,\n  Link,\n  SelectionSet,\n  OperationRequest,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  makeDict,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { invariant, warn, pushDebugNode } from '../helpers/help';\nimport { SelectionIterator, ensureData } from './shared';\n\nexport interface WriteResult {\n  dependencies: Set<string>;\n}\n\ninterface Context {\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  result: WriteResult;\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  optimistic?: boolean;\n  schemaPredicates?: SchemaPredicates;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n): WriteResult => {\n  initDataState(store.data, 0);\n  const result = startWrite(store, request, data);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { dependencies: getCurrentDependencies() };\n\n  const select = getSelectionSet(operation);\n  const operationName = store.getRootKey(operation.operation);\n\n  const ctx: Context = {\n    parentTypeName: operationName,\n    parentKey: operationName,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    result,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  if (operationName === ctx.store.getRootKey('query')) {\n    writeSelection(ctx, operationName, select, data);\n  } else {\n    writeRoot(ctx, operationName, select, data);\n  }\n\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  optimisticKey: number\n): WriteResult => {\n  initDataState(store.data, optimisticKey);\n\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { dependencies: getCurrentDependencies() };\n\n  const mutationRootKey = store.getRootKey('mutation');\n  const operationName = store.getRootKey(operation.operation);\n  invariant(\n    operationName === mutationRootKey,\n    'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n      'This case is unsupported and should never occur.',\n    10\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  const ctx: Context = {\n    parentTypeName: mutationRootKey,\n    parentKey: mutationRootKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    result,\n    store,\n    schemaPredicates: store.schemaPredicates,\n    optimistic: true,\n  };\n\n  const data = makeDict();\n  const iter = new SelectionIterator(\n    operationName,\n    operationName,\n    getSelectionSet(operation),\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    if (node.selectionSet !== undefined) {\n      const fieldName = getName(node);\n      const resolver = ctx.store.optimisticMutations[fieldName];\n\n      if (resolver !== undefined) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        ctx.fieldName = fieldName;\n\n        const fieldArgs = getFieldArguments(node, ctx.variables);\n        const resolverValue = resolver(fieldArgs || makeDict(), ctx.store, ctx);\n        const resolverData = ensureData(resolverValue);\n        writeRootField(ctx, resolverData, getSelectionSet(node));\n        data[fieldName] = resolverValue;\n        const updater = ctx.store.updates[mutationRootKey][fieldName];\n        if (updater !== undefined) {\n          updater(data, fieldArgs || makeDict(), ctx.store, ctx);\n        }\n      }\n    }\n  }\n\n  clearDataState();\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Data,\n  variables?: Variables\n) => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    return warn(\n      'writeFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      11\n    );\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const writeData = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(writeData);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx: Context = {\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: variables || {},\n    fragments,\n    result: { dependencies: getCurrentDependencies() },\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), writeData);\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.getRootKey('query');\n  const typename = isQuery ? entityKey : data.__typename;\n  if (typeof typename !== 'string') return;\n\n  InMemoryData.writeRecord(entityKey, '__typename', typename);\n\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldValue = data[getFieldAlias(node)];\n    const key = joinKeys(entityKey, fieldKey);\n\n    if (process.env.NODE_ENV !== 'production') {\n      if (fieldValue === undefined) {\n        const advice = ctx.optimistic\n          ? '\\nYour optimistic result may be missing a field!'\n          : '';\n\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.' +\n            advice,\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.schemaPredicates && typename) {\n        ctx.schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n      }\n    }\n\n    if (node.selectionSet === undefined) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(entityKey, fieldKey, fieldValue);\n    } else {\n      // Process the field and write links for the child entities that have been written\n      const fieldData = ensureData(fieldValue);\n      const link = writeField(ctx, key, getSelectionSet(node), fieldData);\n      InMemoryData.writeLink(entityKey, fieldKey, link);\n    }\n  }\n};\n\nconst writeField = (\n  ctx: Context,\n  parentFieldKey: string,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>\n): Link => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      const item = data[i];\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = joinKeys(parentFieldKey, `${i}`);\n      // Recursively write array data\n      const links = writeField(ctx, indexKey, select, item);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n    }\n\n    return newData;\n  } else if (data === null) {\n    return null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const key = entityKey !== null ? entityKey : parentFieldKey;\n  const typename = data.__typename;\n\n  if (\n    ctx.store.keys[data.__typename] === undefined &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !typename.endsWith('Connection') &&\n    !typename.endsWith('Edge') &&\n    typename !== 'PageInfo'\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  writeSelection(ctx, key, select, data);\n  return key;\n};\n\n// This is like writeSelection but assumes no parent entity exists\nconst writeRoot = (\n  ctx: Context,\n  typename: string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isRootField =\n    typename === ctx.store.getRootKey('mutation') ||\n    typename === ctx.store.getRootKey('subscription');\n\n  const iter = new SelectionIterator(typename, typename, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = joinKeys(typename, keyOfField(fieldName, fieldArgs));\n    if (node.selectionSet !== undefined) {\n      const fieldValue = ensureData(data[getFieldAlias(node)]);\n      writeRootField(ctx, fieldValue, getSelectionSet(node));\n    }\n\n    if (isRootField) {\n      // We have to update the context to reflect up-to-date ResolveInfo\n      ctx.parentTypeName = typename;\n      ctx.parentKey = typename;\n      ctx.parentFieldKey = fieldKey;\n      ctx.fieldName = fieldName;\n\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater !== undefined) {\n        updater(data, fieldArgs || makeDict(), ctx.store, ctx);\n      }\n    }\n  }\n};\n\n// This is like writeField but doesn't fall back to a generated key\nconst writeRootField = (\n  ctx: Context,\n  data: null | Data | NullArray<Data>,\n  select: SelectionSet\n) => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++)\n      newData[i] = writeRootField(ctx, data[i], select);\n    return newData;\n  } else if (data === null) {\n    return;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(data);\n  if (entityKey !== null) {\n    writeSelection(ctx, entityKey, select, data);\n  } else {\n    const typename = data.__typename;\n    writeRoot(ctx, typename, select, data);\n  }\n};\n","import { FieldNode } from 'graphql';\n\nimport {\n  getMainOperation,\n  normalizeVariables,\n  getFragments,\n  getSelectionSet,\n  getName,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  EntityField,\n  OperationRequest,\n  Variables,\n  Fragments,\n  SelectionSet,\n} from '../types';\n\nimport * as InMemoryData from '../store/data';\nimport { Store, keyOfField } from '../store';\nimport { SchemaPredicates } from '../ast';\nimport { SelectionIterator } from './shared';\n\ninterface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nexport const invalidate = (store: Store, request: OperationRequest) => {\n  const operation = getMainOperation(request.query);\n\n  const ctx: Context = {\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  invalidateSelection(\n    ctx,\n    ctx.store.getRootKey('query'),\n    getSelectionSet(operation)\n  );\n};\n\nexport const invalidateSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet\n) => {\n  const isQuery = entityKey === 'Query';\n\n  let typename: EntityField;\n  if (!isQuery) {\n    typename = InMemoryData.readRecord(entityKey, '__typename');\n    if (typeof typename !== 'string') {\n      return;\n    } else {\n      InMemoryData.writeRecord(entityKey, '__typename', undefined);\n    }\n  } else {\n    typename = entityKey;\n  }\n\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldKey = keyOfField(\n      fieldName,\n      getFieldArguments(node, ctx.variables)\n    );\n\n    if (\n      process.env.NODE_ENV !== 'production' &&\n      ctx.schemaPredicates &&\n      typename\n    ) {\n      ctx.schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    if (node.selectionSet === undefined) {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    } else {\n      const fieldSelect = getSelectionSet(node);\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n\n      if (Array.isArray(link)) {\n        for (let i = 0, l = link.length; i < l; i++) {\n          const childLink = link[i];\n          if (childLink !== null) {\n            invalidateSelection(ctx, childLink, fieldSelect);\n          }\n        }\n      } else if (link) {\n        invalidateSelection(ctx, link, fieldSelect);\n      }\n    }\n  }\n};\n","import { DocumentNode } from 'graphql';\nimport { createRequest } from 'urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  Data,\n  QueryInput,\n  UpdatesConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n} from '../types';\n\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidate } from '../operations/invalidate';\nimport { SchemaPredicates } from '../ast';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport class Store implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: UpdatesConfig;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schemaPredicates?: SchemaPredicates;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(\n    schemaPredicates?: SchemaPredicates,\n    resolvers?: ResolverConfig,\n    updates?: Partial<UpdatesConfig>,\n    optimisticMutations?: OptimisticMutationConfig,\n    keys?: KeyingConfig\n  ) {\n    this.resolvers = resolvers || {};\n    this.optimisticMutations = optimisticMutations || {};\n    this.keys = keys || {};\n    this.schemaPredicates = schemaPredicates;\n\n    this.updates = {\n      Mutation: (updates && updates.Mutation) || {},\n      Subscription: (updates && updates.Subscription) || {},\n    } as UpdatesConfig;\n\n    if (schemaPredicates) {\n      const { schema } = schemaPredicates;\n      const queryType = schema.getQueryType();\n      const mutationType = schema.getMutationType();\n      const subscriptionType = schema.getSubscriptionType();\n\n      const queryName = queryType ? queryType.name : 'Query';\n      const mutationName = mutationType ? mutationType.name : 'Mutation';\n      const subscriptionName = subscriptionType\n        ? subscriptionType.name\n        : 'Subscription';\n\n      this.rootFields = {\n        query: queryName,\n        mutation: mutationName,\n        subscription: subscriptionName,\n      };\n\n      this.rootNames = {\n        [queryName]: 'query',\n        [mutationName]: 'mutation',\n        [subscriptionName]: 'subscription',\n      };\n    } else {\n      this.rootFields = {\n        query: 'Query',\n        mutation: 'Mutation',\n        subscription: 'Subscription',\n      };\n\n      this.rootNames = {\n        Query: 'query',\n        Mutation: 'mutation',\n        Subscription: 'subscription',\n      };\n    }\n\n    this.data = InMemoryData.make(this.getRootKey('query'));\n  }\n\n  gcScheduled = false;\n  gc = () => {\n    InMemoryData.gc(this.data);\n    this.gcScheduled = false;\n  };\n\n  keyOfField = keyOfField;\n\n  getRootKey(name: RootField) {\n    return this.rootFields[name];\n  }\n\n  keyOfEntity(data: Data) {\n    const { __typename: typename, id, _id } = data;\n    if (!typename) {\n      return null;\n    } else if (this.rootNames[typename] !== undefined) {\n      return typename;\n    }\n\n    let key: string | null | void;\n    if (this.keys[typename]) {\n      key = this.keys[typename](data);\n    } else if (id !== undefined && id !== null) {\n      key = `${id}`;\n    } else if (_id !== undefined && _id !== null) {\n      key = `${_id}`;\n    }\n\n    return key ? `${typename}:${key}` : null;\n  }\n\n  resolveFieldByKey(entity: Data | string | null, fieldKey: string): DataField {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    if (entityKey === null) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link ? link : null;\n  }\n\n  resolve(\n    entity: Data | string | null,\n    field: string,\n    args?: Variables\n  ): DataField {\n    return this.resolveFieldByKey(entity, keyOfField(field, args));\n  }\n\n  invalidateQuery(query: string | DocumentNode, variables?: Variables) {\n    invalidate(this, createRequest(query, variables));\n  }\n\n  inspectFields(entity: Data | string | null): FieldInfo[] {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    return entityKey !== null ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery(\n    input: QueryInput,\n    updater: (data: Data | null) => Data | null\n  ): void {\n    const request = createRequest(input.query, input.variables);\n    const output = updater(this.readQuery(request as QueryInput));\n    if (output !== null) {\n      startWrite(this, request, output);\n    }\n  }\n\n  readQuery(input: QueryInput): Data | null {\n    return read(this, createRequest(input.query, input.variables)).data;\n  }\n\n  readFragment(\n    dataFragment: DocumentNode,\n    entity: string | Data,\n    variables?: Variables\n  ): Data | null {\n    return readFragment(this, dataFragment, entity, variables);\n  }\n\n  writeFragment(\n    dataFragment: DocumentNode,\n    data: Data,\n    variables?: Variables\n  ): void {\n    writeFragment(this, dataFragment, data, variables);\n  }\n}\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\nimport { makeDict } from '../store';\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  if (node.arguments === undefined || node.arguments.length === 0) {\n    return null;\n  }\n\n  const args = makeDict();\n  let argsSize = 0;\n\n  for (let i = 0, l = node.arguments.length; i < l; i++) {\n    const arg = node.arguments[i];\n    const value = valueFromASTUntyped(arg.value, vars);\n    if (value !== undefined && value !== null) {\n      args[getName(arg)] = value;\n      argsSize++;\n    }\n  }\n\n  return argsSize > 0 ? args : null;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n): Variables => {\n  if (node.variableDefinitions === undefined) {\n    return {};\n  }\n\n  const args: Variables = (input as Variables) || {};\n\n  return node.variableDefinitions.reduce((vars, def) => {\n    const name = getName(def.variable);\n    let value = args[name];\n    if (value === undefined) {\n      if (def.defaultValue !== undefined) {\n        value = valueFromASTUntyped(def.defaultValue, args);\n      } else {\n        return vars;\n      }\n    }\n\n    vars[name] = value;\n    return vars;\n  }, makeDict());\n};\n","import {\n  buildClientSchema,\n  isNullableType,\n  isListType,\n  isNonNullType,\n  GraphQLSchema,\n  GraphQLAbstractType,\n  GraphQLObjectType,\n  GraphQLInterfaceType,\n  GraphQLUnionType,\n} from 'graphql';\n\nimport { invariant, warn } from '../helpers/help';\n\nexport class SchemaPredicates {\n  schema: GraphQLSchema;\n\n  constructor(schema: object) {\n    this.schema = buildClientSchema(schema as any);\n  }\n\n  isFieldNullable(typename: string, fieldName: string): boolean {\n    const field = getField(this.schema, typename, fieldName);\n    if (field === undefined) return false;\n    return isNullableType(field.type);\n  }\n\n  isListNullable(typename: string, fieldName: string): boolean {\n    const field = getField(this.schema, typename, fieldName);\n    if (field === undefined) return false;\n    const ofType = isNonNullType(field.type) ? field.type.ofType : field.type;\n    return isListType(ofType) && isNullableType(ofType.ofType);\n  }\n\n  isFieldAvailableOnType(typename: string, fieldname: string): boolean {\n    return !!getField(this.schema, typename, fieldname);\n  }\n\n  isInterfaceOfType(\n    typeCondition: null | string,\n    typename: string | void\n  ): boolean {\n    if (!typename || !typeCondition) return false;\n    if (typename === typeCondition) return true;\n\n    const abstractType = this.schema.getType(typeCondition);\n    const objectType = this.schema.getType(typename);\n\n    if (abstractType instanceof GraphQLObjectType) {\n      return abstractType === objectType;\n    }\n\n    expectAbstractType(abstractType, typeCondition);\n    expectObjectType(objectType, typename);\n    return this.schema.isPossibleType(abstractType, objectType);\n  }\n}\n\nconst getField = (\n  schema: GraphQLSchema,\n  typename: string,\n  fieldName: string\n) => {\n  const object = schema.getType(typename);\n  expectObjectType(object, typename);\n\n  const field = object.getFields()[fieldName];\n  if (field === undefined) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n\n    return undefined;\n  }\n\n  return field;\n};\n\nfunction expectObjectType(x: any, typename: string): asserts x is GraphQLObjectType {\n  invariant(\n    x instanceof GraphQLObjectType,\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(x: any, typename: string): asserts x is GraphQLAbstractType {\n  invariant(\n    x instanceof GraphQLInterfaceType || x instanceof GraphQLUnionType,\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n","import {\n  SelectionNode,\n  DefinitionNode,\n  DocumentNode,\n  FragmentDefinitionNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { invariant } from '../helpers/help';\nimport { getName } from './node';\nimport { Fragments, Variables } from '../types';\n\nconst isFragmentNode = (node: DefinitionNode): node is FragmentDefinitionNode =>\n  node.kind === Kind.FRAGMENT_DEFINITION;\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  const operation = doc.definitions.find(\n    node => node.kind === Kind.OPERATION_DEFINITION\n  ) as OperationDefinitionNode;\n\n  invariant(\n    !!operation,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n\n  return operation;\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments =>\n  doc.definitions.filter(isFragmentNode).reduce((map: Fragments, node) => {\n    map[getName(node)] = node;\n    return map;\n  }, {});\n\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  const { directives } = node;\n  if (directives === undefined) {\n    return true;\n  }\n\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0, l = directives.length; i < l; i++) {\n    const directive = directives[i];\n    const name = getName(directive);\n\n    // Ignore other directives\n    const isInclude = name === 'include';\n    if (!isInclude && name !== 'skip') continue;\n\n    // Get the first argument and expect it to be named \"if\"\n    const arg = directive.arguments ? directive.arguments[0] : null;\n    if (!arg || getName(arg) !== 'if') continue;\n\n    const value = valueFromASTUntyped(arg.value, vars);\n    if (typeof value !== 'boolean' && value !== null) continue;\n\n    // Return whether this directive forces us to skip\n    // `@include(if: false)` or `@skip(if: true)`\n    return isInclude ? !!value : !value;\n  }\n\n  return true;\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  getFragments,\n  getMainOperation,\n  getSelectionSet,\n  normalizeVariables,\n  getName,\n  getFieldArguments,\n  getFieldAlias,\n  getFragmentTypeName,\n} from '../ast';\n\nimport {\n  Fragments,\n  Variables,\n  Data,\n  DataField,\n  Link,\n  SelectionSet,\n  OperationRequest,\n  NullArray,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  makeDict,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode } from '../helpers/help';\nimport { SelectionIterator, ensureData } from './shared';\nimport { SchemaPredicates } from '../ast';\n\nexport interface QueryResult {\n  dependencies: Set<string>;\n  partial: boolean;\n  data: null | Data;\n}\n\ninterface Context {\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  partial: boolean;\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data\n): QueryResult => {\n  initDataState(store.data, 0);\n  const result = read(store, request, data);\n  clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.getRootKey(operation.operation);\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx: Context = {\n    parentTypeName: rootKey,\n    parentKey: rootKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    partial: false,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  let data = input || makeDict();\n  data =\n    rootKey !== ctx.store.getRootKey('query')\n      ? readRoot(ctx, rootKey, rootSelect, data)\n      : readSelection(ctx, rootKey, rootSelect, data);\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: data === undefined ? false : ctx.partial,\n    data: data === undefined ? null : data,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  originalData: Data\n): Data => {\n  if (typeof originalData.__typename !== 'string') {\n    return originalData;\n  }\n\n  const iter = new SelectionIterator(entityKey, entityKey, select, ctx);\n  const data = makeDict();\n  data.__typename = originalData.__typename;\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = originalData[fieldAlias];\n    if (node.selectionSet !== undefined && fieldValue !== null) {\n      const fieldData = ensureData(fieldValue);\n      data[fieldAlias] = readRootField(ctx, getSelectionSet(node), fieldData);\n    } else {\n      data[fieldAlias] = fieldValue;\n    }\n  }\n\n  return data;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: null | Data | NullArray<Data>\n): Data | NullArray<Data> | null => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    for (let i = 0, l = originalData.length; i < l; i++)\n      newData[i] = readRootField(ctx, select, originalData[i]);\n    return newData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    const fieldValue = readSelection(ctx, entityKey, select, makeDict());\n    return fieldValue === undefined ? null : fieldValue;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Data | string,\n  variables?: Variables\n): Data | null => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    warn(\n      'readFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      6\n    );\n\n    return null;\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename) {\n    entity.__typename = typename;\n  }\n\n  const entityKey =\n    typeof entity !== 'string'\n      ? store.keyOfEntity({ __typename: typename, ...entity } as Data)\n      : entity;\n\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx: Context = {\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: variables || {},\n    fragments,\n    partial: false,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  return (\n    readSelection(ctx, entityKey, getSelectionSet(fragment), makeDict()) || null\n  );\n};\n\nconst readSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  data: Data\n): Data | undefined => {\n  const { store, schemaPredicates } = ctx;\n  const isQuery = entityKey === store.getRootKey('query');\n\n  // Get the __typename field for a given entity to check that it exists\n  const typename = !isQuery\n    ? InMemoryData.readRecord(entityKey, '__typename')\n    : entityKey;\n  if (typeof typename !== 'string') {\n    return undefined;\n  }\n\n  data.__typename = typename;\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iter.next()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const key = joinKeys(entityKey, fieldKey);\n\n    if (process.env.NODE_ENV !== 'production' && schemaPredicates && typename) {\n      schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n\n    const resolvers = store.resolvers[typename];\n    if (resolvers !== undefined && typeof resolvers[fieldName] === 'function') {\n      // We have to update the information in context to reflect the info\n      // that the resolver will receive\n      ctx.parentTypeName = typename;\n      ctx.parentKey = entityKey;\n      ctx.parentFieldKey = key;\n      ctx.fieldName = fieldName;\n\n      // We have a resolver for this field.\n      // Prepare the actual fieldValue, so that the resolver can use it\n      if (fieldValue !== undefined) {\n        data[fieldAlias] = fieldValue;\n      }\n\n      dataFieldValue = resolvers[fieldName](\n        data,\n        fieldArgs || makeDict(),\n        store,\n        ctx\n      );\n\n      if (node.selectionSet !== undefined) {\n        // When it has a selection set we are resolving an entity with a\n        // subselection. This can either be a list or an object.\n        dataFieldValue = resolveResolverResult(\n          ctx,\n          typename,\n          fieldName,\n          key,\n          getSelectionSet(node),\n          (data[fieldAlias] as Data) || makeDict(),\n          dataFieldValue\n        );\n      }\n\n      if (\n        schemaPredicates !== undefined &&\n        dataFieldValue === null &&\n        !schemaPredicates.isFieldNullable(typename, fieldName)\n      ) {\n        // Special case for when null is not a valid value for the\n        // current field\n        return undefined;\n      }\n    } else if (node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly\n      dataFieldValue = fieldValue;\n    } else {\n      // We have a selection set which means that we'll be checking for links\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      schemaPredicates !== undefined &&\n      schemaPredicates.isFieldNullable(typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return isQuery && hasPartials && !hasFields ? undefined : data;\n};\n\nconst readResolverResult = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  data: Data,\n  result: Data\n): Data | undefined => {\n  const { store, schemaPredicates } = ctx;\n  const entityKey = store.keyOfEntity(result) || key;\n  const resolvedTypename = result.__typename;\n  const typename =\n    InMemoryData.readRecord(entityKey, '__typename') || resolvedTypename;\n\n  if (\n    typeof typename !== 'string' ||\n    (resolvedTypename && typename !== resolvedTypename)\n  ) {\n    // TODO: This may be an invalid error for resolvers that return interfaces\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return undefined;\n  }\n\n  // The following closely mirrors readSelection, but differs only slightly for the\n  // sake of resolving from an existing resolver result\n  data.__typename = typename;\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iter.next()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(\n      fieldName,\n      getFieldArguments(node, ctx.variables)\n    );\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result[fieldName];\n\n    if (process.env.NODE_ENV !== 'production' && schemaPredicates && typename) {\n      schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (node.selectionSet === undefined) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        data[fieldAlias] as Data,\n        resultValue\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      schemaPredicates !== undefined &&\n      schemaPredicates.isFieldNullable(typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return !hasFields ? undefined : data;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | Data | Data[],\n  result: void | DataField\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { schemaPredicates } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const isListNullable =\n      schemaPredicates === undefined ||\n      schemaPredicates.isListNullable(typename, fieldName);\n    const data = new Array(result.length);\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        // Get the inner previous data from prevData\n        prevData !== undefined ? prevData[i] : undefined,\n        result[i]\n      );\n\n      if (childResult === undefined && !isListNullable) {\n        return undefined;\n      } else {\n        data[i] = childResult !== undefined ? childResult : null;\n      }\n    }\n\n    return data;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (isDataOrKey(result)) {\n    const data = prevData === undefined ? makeDict() : prevData;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readResolverResult(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | Data | Data[]\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { schemaPredicates } = ctx;\n    const isListNullable =\n      schemaPredicates !== undefined &&\n      schemaPredicates.isListNullable(typename, fieldName);\n    const newLink = new Array(link.length);\n    for (let i = 0, l = link.length; i < l; i++) {\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData !== undefined ? prevData[i] : undefined\n      );\n      if (childLink === undefined && !isListNullable) {\n        return undefined;\n      } else {\n        newLink[i] = childLink !== undefined ? childLink : null;\n      }\n    }\n\n    return newLink;\n  } else if (link === null) {\n    return null;\n  } else {\n    return readSelection(\n      ctx,\n      link,\n      select,\n      prevData === undefined ? makeDict() : prevData\n    );\n  }\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import { IntrospectionQuery } from 'graphql';\n\nimport {\n  Exchange,\n  formatDocument,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from 'urql/core';\n\nimport {\n  filter,\n  map,\n  merge,\n  pipe,\n  share,\n  tap,\n  fromPromise,\n  fromArray,\n  buffer,\n  take,\n  mergeMap,\n  concat,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { SchemaPredicates } from './ast';\nimport { hydrateData } from './store/data';\nimport { makeDict, Store, clearOptimistic } from './store';\n\nimport {\n  UpdatesConfig,\n  ResolverConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  StorageAdapter,\n} from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n};\n\ntype OperationMap = Map<number, Operation>;\n\ninterface DependentOperations {\n  [key: string]: number[];\n}\n\n// Returns the given operation result with added cacheOutcome meta field\nconst addCacheOutcome = (op: Operation, outcome: CacheOutcome): Operation => ({\n  ...op,\n  context: {\n    ...op.context,\n    meta: {\n      ...op.context.meta,\n      cacheOutcome: outcome,\n    },\n  },\n});\n\n// Returns the given operation with added __typename fields on its query\nconst addTypeNames = (op: Operation): Operation => ({\n  ...op,\n  query: formatDocument(op.query),\n});\n\n// Retrieves the requestPolicy from an operation\nconst getRequestPolicy = (op: Operation) => op.context.requestPolicy;\n\n// Returns whether an operation is a query\nconst isQueryOperation = (op: Operation): boolean =>\n  op.operationName === 'query';\n\n// Returns whether an operation is a mutation\nconst isMutationOperation = (op: Operation): boolean =>\n  op.operationName === 'mutation';\n\n// Returns whether an operation can potentially be read from cache\nconst isCacheableQuery = (op: Operation): boolean => {\n  return isQueryOperation(op) && getRequestPolicy(op) !== 'network-only';\n};\n\n// Returns whether an operation potentially triggers an optimistic update\nconst isOptimisticMutation = (op: Operation): boolean => {\n  return isMutationOperation(op) && getRequestPolicy(op) !== 'network-only';\n};\n\n// Copy an operation and change the requestPolicy to skip the cache\nconst toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => ({\n  ...operation,\n  context: {\n    ...operation.context,\n    requestPolicy,\n  },\n});\n\nexport interface CacheExchangeOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionQuery;\n  storage?: StorageAdapter;\n}\n\nexport const cacheExchange = (opts?: CacheExchangeOpts): Exchange => ({\n  forward,\n  client,\n}) => {\n  if (!opts) opts = {};\n\n  const store = new Store(\n    opts.schema ? new SchemaPredicates(opts.schema) : undefined,\n    opts.resolvers,\n    opts.updates,\n    opts.optimistic,\n    opts.keys\n  );\n\n  let hydration: void | Promise<void>;\n  if (opts.storage) {\n    const storage = opts.storage;\n    hydration = storage.read().then(entries => {\n      hydrateData(store.data, storage, entries);\n    });\n  }\n\n  const optimisticKeys = new Set();\n  const ops: OperationMap = new Map();\n  const deps: DependentOperations = makeDict();\n\n  const collectPendingOperations = (\n    pendingOperations: Set<number>,\n    dependencies: void | Set<string>\n  ) => {\n    if (dependencies !== undefined) {\n      // Collect operations that will be updated due to cache changes\n      dependencies.forEach(dep => {\n        const keys = deps[dep];\n        if (keys !== undefined) {\n          deps[dep] = [];\n          for (let i = 0, l = keys.length; i < l; i++) {\n            pendingOperations.add(keys[i]);\n          }\n        }\n      });\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Set<number>\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    pendingOperations.forEach(key => {\n      if (key !== operation.key) {\n        const op = ops.get(key);\n        if (op !== undefined) {\n          ops.delete(key);\n          client.reexecuteOperation(toRequestPolicy(op, 'cache-first'));\n        }\n      }\n    });\n  };\n\n  // This executes an optimistic update for mutations and registers it if necessary\n  const optimisticUpdate = (operation: Operation) => {\n    if (isOptimisticMutation(operation)) {\n      const { key } = operation;\n      const { dependencies } = writeOptimistic(store, operation, key);\n      if (dependencies.size !== 0) {\n        optimisticKeys.add(key);\n        const pendingOperations = new Set<number>();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Set<string>) => {\n    dependencies.forEach(dep => {\n      const keys = deps[dep] || (deps[dep] = []);\n      keys.push(op.key);\n\n      if (!ops.has(op.key)) {\n        ops.set(\n          op.key,\n          getRequestPolicy(op) === 'network-only'\n            ? toRequestPolicy(op, 'cache-and-network')\n            : op\n        );\n      }\n    });\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const { data, dependencies, partial } = query(store, operation);\n    let cacheOutcome: CacheOutcome;\n\n    if (data === null) {\n      cacheOutcome = 'miss';\n    } else {\n      updateDependencies(operation, dependencies);\n      cacheOutcome =\n        !partial || getRequestPolicy(operation) === 'cache-only'\n          ? 'hit'\n          : 'partial';\n    }\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (result: OperationResult): OperationResult => {\n    const { operation, error, extensions } = result;\n    const isQuery = isQueryOperation(operation);\n    let { data } = result;\n\n    // Clear old optimistic values from the store\n    const { key } = operation;\n    if (optimisticKeys.has(key)) {\n      optimisticKeys.delete(key);\n      clearOptimistic(store.data, key);\n    }\n\n    let writeDependencies: Set<string> | void;\n    let queryDependencies: Set<string> | void;\n    if (data !== null && data !== undefined) {\n      writeDependencies = write(store, operation, data).dependencies;\n\n      if (isQuery) {\n        const queryResult = query(store, operation);\n        data = queryResult.data;\n        queryDependencies = queryResult.dependencies;\n      } else {\n        data = query(store, operation, data).data;\n      }\n    }\n\n    // Collect all write dependencies and query dependencies for queries\n    const pendingOperations = new Set<number>();\n    collectPendingOperations(pendingOperations, writeDependencies);\n    if (isQuery) {\n      collectPendingOperations(pendingOperations, queryDependencies);\n    }\n\n    // Execute all pending operations related to changed dependencies\n    executePendingOperations(result.operation, pendingOperations);\n\n    // Update this operation's dependencies if it's a query\n    if (isQuery && queryDependencies !== undefined) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Buffer operations while waiting on hydration to finish\n    // If no hydration takes place we replace this stream with an empty one\n    const bufferedOps$ = hydration\n      ? pipe(\n          sharedOps$,\n          buffer(fromPromise(hydration)),\n          take(1),\n          mergeMap(fromArray)\n        )\n      : (empty as Source<Operation>);\n\n    const inputOps$ = pipe(\n      concat([bufferedOps$, sharedOps$]),\n      map(addTypeNames),\n      tap(optimisticUpdate),\n      share\n    );\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cache$ = pipe(\n      inputOps$,\n      filter(op => isCacheableQuery(op)),\n      map(operationResultFromCache),\n      share\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheOps$ = pipe(\n      cache$,\n      filter(res => res.outcome === 'miss'),\n      map(res => addCacheOutcome(res.operation, res.outcome))\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cache$,\n      filter(res => res.outcome !== 'miss'),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome } = res;\n          const policy = getRequestPolicy(operation);\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            policy === 'cache-and-network' ||\n            (policy === 'cache-first' && outcome === 'partial')\n          ) {\n            result.stale = true;\n            client.reexecuteOperation(\n              toRequestPolicy(operation, 'network-only')\n            );\n          }\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      forward(\n        merge([\n          pipe(\n            inputOps$,\n            filter(op => !isCacheableQuery(op))\n          ),\n          cacheOps$,\n        ])\n      ),\n      map(updateCacheWithResult)\n    );\n\n    return merge([result$, cacheResult$]);\n  };\n};\n","import {\n  DocumentNode,\n  buildClientSchema,\n  visitWithTypeInfo,\n  TypeInfo,\n  FragmentDefinitionNode,\n  GraphQLSchema,\n  IntrospectionQuery,\n  FragmentSpreadNode,\n  NameNode,\n  ASTNode,\n  isCompositeType,\n  isAbstractType,\n  Kind,\n  visit,\n} from 'graphql';\n\nimport { pipe, tap, map } from 'wonka';\nimport { Exchange, Operation } from 'urql/core';\n\nimport { getName, getSelectionSet, unwrapType } from './ast';\nimport { makeDict } from './store';\nimport { invariant, warn } from './helpers/help';\n\ninterface PopulateExchangeOpts {\n  schema: IntrospectionQuery;\n}\n\n/** An exchange for auto-populating mutations with a required response body. */\nexport const populateExchange = ({\n  schema: ogSchema,\n}: PopulateExchangeOpts): Exchange => ({ forward }) => {\n  const schema = buildClientSchema(ogSchema);\n  /** List of operation keys that have already been parsed. */\n  const parsedOperations = new Set<number>();\n  /** List of operation keys that have not been torn down. */\n  const activeOperations = new Set<number>();\n  /** Collection of fragments used by the user. */\n  const userFragments: UserFragmentMap = makeDict();\n  /** Collection of actively in use type fragments. */\n  const activeTypeFragments: TypeFragmentMap = makeDict();\n\n  /** Handle mutation and inject selections + fragments. */\n  const handleIncomingMutation = (op: Operation) => {\n    if (op.operationName !== 'mutation') {\n      return op;\n    }\n\n    const activeSelections: TypeFragmentMap = makeDict();\n    for (const name in activeTypeFragments) {\n      activeSelections[name] = activeTypeFragments[name].filter(s =>\n        activeOperations.has(s.key)\n      );\n    }\n\n    return {\n      ...op,\n      query: addFragmentsToQuery(\n        schema,\n        op.query,\n        activeSelections,\n        userFragments\n      ),\n    };\n  };\n\n  /** Handle query and extract fragments. */\n  const handleIncomingQuery = ({ key, operationName, query }: Operation) => {\n    if (operationName !== 'query') {\n      return;\n    }\n\n    activeOperations.add(key);\n    if (parsedOperations.has(key)) {\n      return;\n    }\n\n    parsedOperations.add(key);\n\n    const [extractedFragments, newFragments] = extractSelectionsFromQuery(\n      schema,\n      query\n    );\n\n    for (let i = 0, l = extractedFragments.length; i < l; i++) {\n      const fragment = extractedFragments[i];\n      userFragments[getName(fragment)] = fragment;\n    }\n\n    for (let i = 0, l = newFragments.length; i < l; i++) {\n      const fragment = newFragments[i];\n      const type = getName(fragment.typeCondition);\n      const current =\n        activeTypeFragments[type] || (activeTypeFragments[type] = []);\n\n      (fragment as any).name.value += current.length;\n      current.push({ key, fragment });\n    }\n  };\n\n  const handleIncomingTeardown = ({ key, operationName }: Operation) => {\n    if (operationName === 'teardown') {\n      activeOperations.delete(key);\n    }\n  };\n\n  return ops$ => {\n    return pipe(\n      ops$,\n      tap(handleIncomingQuery),\n      tap(handleIncomingTeardown),\n      map(handleIncomingMutation),\n      forward\n    );\n  };\n};\n\ntype UserFragmentMap<T extends string = string> = Record<\n  T,\n  FragmentDefinitionNode\n>;\n\ntype TypeFragmentMap<T extends string = string> = Record<T, TypeFragment[]>;\n\ninterface TypeFragment {\n  /** Operation key where selection set is being used. */\n  key: number;\n  /** Selection set. */\n  fragment: FragmentDefinitionNode;\n}\n\n/** Gets typed selection sets and fragments from query */\nexport const extractSelectionsFromQuery = (\n  schema: GraphQLSchema,\n  query: DocumentNode\n) => {\n  const extractedFragments: FragmentDefinitionNode[] = [];\n  const newFragments: FragmentDefinitionNode[] = [];\n  const typeInfo = new TypeInfo(schema);\n\n  visit(\n    query,\n    visitWithTypeInfo(typeInfo, {\n      Field: node => {\n        if (node.selectionSet) {\n          const type = getTypeName(typeInfo);\n          newFragments.push({\n            kind: Kind.FRAGMENT_DEFINITION,\n            typeCondition: {\n              kind: Kind.NAMED_TYPE,\n              name: nameNode(type),\n            },\n            name: nameNode(`${type}_PopulateFragment_`),\n            selectionSet: node.selectionSet,\n          });\n        }\n      },\n      FragmentDefinition: node => {\n        extractedFragments.push(node);\n      },\n    })\n  );\n\n  return [extractedFragments, newFragments];\n};\n\n/** Replaces populate decorator with fragment spreads + fragments. */\nexport const addFragmentsToQuery = (\n  schema: GraphQLSchema,\n  query: DocumentNode,\n  activeTypeFragments: TypeFragmentMap,\n  userFragments: UserFragmentMap\n) => {\n  const typeInfo = new TypeInfo(schema);\n\n  const requiredUserFragments: Record<\n    string,\n    FragmentDefinitionNode\n  > = makeDict();\n\n  const additionalFragments: Record<\n    string,\n    FragmentDefinitionNode\n  > = makeDict();\n\n  /** Fragments provided and used by the current query */\n  const existingFragmentsForQuery: Set<string> = new Set();\n\n  return visit(\n    query,\n    visitWithTypeInfo(typeInfo, {\n      Field: {\n        enter: node => {\n          if (!node.directives) {\n            return;\n          }\n\n          const directives = node.directives.filter(\n            d => getName(d) !== 'populate'\n          );\n          if (directives.length === node.directives.length) {\n            return;\n          }\n\n          const possibleTypes = getTypes(schema, typeInfo);\n          const newSelections = possibleTypes.reduce((p, possibleType) => {\n            const typeFrags = activeTypeFragments[possibleType.name];\n            if (!typeFrags) {\n              return p;\n            }\n\n            for (let i = 0, l = typeFrags.length; i < l; i++) {\n              const { fragment } = typeFrags[i];\n              const fragmentName = getName(fragment);\n              const usedFragments = getUsedFragments(fragment);\n\n              // Add used fragment for insertion at Document node\n              for (let j = 0, l = usedFragments.length; j < l; j++) {\n                const name = usedFragments[j];\n                if (!existingFragmentsForQuery.has(name)) {\n                  requiredUserFragments[name] = userFragments[name];\n                }\n              }\n\n              // Add fragment for insertion at Document node\n              additionalFragments[fragmentName] = fragment;\n\n              p.push({\n                kind: Kind.FRAGMENT_SPREAD,\n                name: nameNode(fragmentName),\n              });\n            }\n\n            return p;\n          }, [] as FragmentSpreadNode[]);\n\n          const existingSelections = getSelectionSet(node);\n\n          const selections =\n            existingSelections.length + newSelections.length !== 0\n              ? [...newSelections, ...existingSelections]\n              : [\n                  {\n                    kind: Kind.FIELD,\n                    name: nameNode('__typename'),\n                  },\n                ];\n\n          return {\n            ...node,\n            directives,\n            selectionSet: {\n              kind: Kind.SELECTION_SET,\n              selections,\n            },\n          };\n        },\n      },\n      Document: {\n        enter: node => {\n          node.definitions.reduce((set, definition) => {\n            if (definition.kind === 'FragmentDefinition') {\n              set.add(definition.name.value);\n            }\n            return set;\n          }, existingFragmentsForQuery);\n        },\n        leave: node => {\n          const definitions = [...node.definitions];\n          for (const key in additionalFragments)\n            definitions.push(additionalFragments[key]);\n          for (const key in requiredUserFragments)\n            definitions.push(requiredUserFragments[key]);\n          return { ...node, definitions };\n        },\n      },\n    })\n  );\n};\n\nconst nameNode = (value: string): NameNode => ({\n  kind: Kind.NAME,\n  value,\n});\n\n/** Get all possible types for node with TypeInfo. */\nconst getTypes = (schema: GraphQLSchema, typeInfo: TypeInfo) => {\n  const type = unwrapType(typeInfo.getType());\n  if (!isCompositeType(type)) {\n    warn(\n      'Invalid type: The type ` + type + ` is used with @populate but does not exist.',\n      17\n    );\n    return [];\n  }\n\n  return isAbstractType(type) ? schema.getPossibleTypes(type) : [type];\n};\n\n/** Get name of non-abstract type for adding to 'activeTypeFragments'. */\nconst getTypeName = (typeInfo: TypeInfo) => {\n  const type = unwrapType(typeInfo.getType());\n  invariant(\n    type && !isAbstractType(type),\n    'Invalid TypeInfo state: Found no flat schema type when one was expected.',\n    18\n  );\n\n  return type.toString();\n};\n\n/** Get fragment names referenced by node. */\nconst getUsedFragments = (node: ASTNode) => {\n  const names: string[] = [];\n\n  visit(node, {\n    FragmentSpread: f => {\n      names.push(getName(f));\n    },\n  });\n\n  return names;\n};\n"],"names":["const","getName","node","getFragmentTypeName","getFieldAlias","getSelectionSet","getTypeCondition","ref","typeCondition","isFieldNode","Kind","isInlineFragment","unwrapType","type","helpUrl","cache","Set","currentDebugStack","pushDebugNode","typename","identifier","getDebugOutput","condition","message","code","process","errorMessage","error","Error","console","keyOfField","fieldName","args","stringifyVariables","fieldInfoOfKey","fieldKey","parenIndex","arguments","JSON","joinKeys","parentKey","key","prefixKey","owner","defer","Promise","fn","makeDict","currentData","currentDependencies","currentOptimisticKey","makeNodeMap","optimistic","base","Map","keys","initDataState","data","optimisticKey","clearDataState","gc","getCurrentDependencies","invariant","setNode","map","entityKey","value","undefined","keymap","entity","getNode","i","l","clearOptimisticNodes","index","updateRCForEntity","gcBatch","refCount","by","count","newCount","updateRCForLink","link","Array","extractNodeFields","fieldInfos","seenFieldKeys","extractNodeMapFields","linkNode","updateDependencies","readRecord","readLink","writeRecord","writeLink","links","prevLinkNode","prevLink","hydrateData","storage","entries","dotIndex","isFragmentHeuristicallyMatching","ctx","warn","getFieldArguments","hasField","SelectionIterator","select","this","shouldInclude","fragmentNode","ensureData","x","write","store","request","startWrite","operation","getMainOperation","result","dependencies","operationName","parentTypeName","parentFieldKey","variables","normalizeVariables","fragments","getFragments","schemaPredicates","writeSelection","writeRoot","writeOptimistic","mutationRootKey","iter","resolver","resolverData","resolverValue","updater","fieldArgs","writeFragment","query","names","Object","fragment","_extends","__typename","writeData","InMemoryData","fieldValue","advice","expected","fieldData","writeField","newData","item","indexKey","isRootField","writeRootField","invalidateSelection","fieldSelect","let","childLink","Store","resolvers","updates","optimisticMutations","queryType","mutationType","schema","subscriptionType","queryName","mutationName","subscriptionName","queryRootKey","persistenceScheduled","persistenceBatch","gcScheduled","refLock","records","name","id","_id","field","createRequest","input","output","dataFragment","vars","argsSize","arg","valueFromASTUntyped","def","SchemaPredicates","buildClientSchema","getField","isNullableType","isNonNullType","ofType","fieldname","abstractType","objectType","expectAbstractType","object","isFragmentNode","doc","directives","directive","isInclude","read","rootKey","rootSelect","partial","readRoot","readSelection","originalData","fieldAlias","readRootField","readFragment","isQuery","hasFields","hasPartials","dataFieldValue","resolveResolverResult","resolveLink","prevData","childResult","isListNullable","isDataOrKey","resolvedTypename","resultValue","readResolverResult","newLink","addCacheOutcome","op","outcome","context","meta","cacheOutcome","addTypeNames","formatDocument","getRequestPolicy","isQueryOperation","isCacheableQuery","toRequestPolicy","requestPolicy","res","extractSelectionsFromQuery","extractedFragments","newFragments","typeInfo","TypeInfo","visitWithTypeInfo","Field","getTypeName","kind","nameNode","selectionSet","FragmentDefinition","d","set","definition","addFragmentsToQuery","activeTypeFragments","userFragments","p","possibleType","typeFrags","fragmentName","usedFragments","getUsedFragments","j","requiredUserFragments","additionalFragments","existingFragmentsForQuery","enter","newSelections","getTypes","existingSelections","selections","Document","leave","definitions","isAbstractType","FragmentSpread","f","opts","policy","extensions","client","hydration","optimisticKeys","ops","deps","collectPendingOperations","pendingOperations","dep","executePendingOperations","optimisticUpdate","isOptimisticMutation","operationResultFromCache","updateCacheWithResult","clearOptimistic","writeDependencies","queryResult","queryDependencies","ops$","sharedOps$","share","bufferedOps$","mergeMap","fromArray","take","buffer","fromPromise","empty","tap","concat","cache$","filter","inputOps$","forward","merge","cacheOps$","result$","cacheResult$","s","ogSchema","parsedOperations","activeOperations","handleIncomingMutation","activeSelections","handleIncomingQuery","current","handleIncomingTeardown"],"mappings":";;;;;;;;;;;;;;;;AAgBOA,IAAMC,mBAAWC;;GAEXC,+BAAuBD;;GAIvBE,yBAAiBF;oBAC5BA,UAA2BA,gBAAmBD,QAAQC;GAG3CG,2BAAmBH;oBAG9BA,iBAAkCA,4BAA+B;GAEtDI,4BAAoBC;4CAKDN,QAAQO,KAAiB;GAE5CC,uBAAeP;oBACZQ;GAEHC,4BACXT;oBAC6CQ;GAElCE,sBACXC;gCAEmBA,KACVD,WAAWC,YAGbA,KAAQ;GC1CXC,UACJ,0FACIC,QAAQ,IAAIC,KAELC,oBAA8B,IAE9BC,yBAAiBC,GAAyBjB;MACjDkB,IAAa;aACCV,+BAChBU,IAAaD,6BACcA,UACvB,oBACKjB,WAAcQ,oCAEvBU,KADalB,eAAgBA,qBAAqB,mBAC1BA,cACfA,WAAcQ,qCACvBU,IAAa,MAAIlB;OAIjBe,uBAAuBG;GAIrBC;oCAEA,mBAAmBJ,uBAAuB,QAAQ,MAClD;;;mBAGJK,GACAC,GACAC;OAEKF;cACgBC,KAAW,oBAAoBC,IAAO,MAC5B,iBAAzBC,yBACFC,KAAgBL;KAGZM,IAAYC,MAAMF,IAAeZ,UAAUU,WACpC,oBACPG;;;;cAIWJ,GAAiBC;EAC/BT,UAAUQ,OACbM,aAAaN,IAAUF,mBAAmBP,UAAUU,IACpDT,UAAUQ;;;ACxDPvB,IAAM8B,sBAAcC,GAAmBC;aAClCD,UAAaE,wBAAmBD,WAAWD;GAE1CG,0BAAkBC;MACvBC,IAAaD,UAAiB;cAChCC,IACK;cACLD;IACAJ,WAAWI,QAAe,GAAGC;IAC7BC,WAAWC,WAAWH,QAAeC,IAAa;MAG7C;cACLD;IACAJ,WAAWI;IACXE,WAAW;;GAKJE,oBAAYC,GAAmBC;mBAC1BA;GAGLC,qBAAaC,GAAkBF;mBAA4BA;GC3B3DG,QACc,iBAAzBnB,wBAA4D,gCACxDoB,4BAA4BA,8BAC5BC;oBAAiBA,GAAI;GC+BdC;uBAAoC;GAE7CC,cAAmC,MACnCC,sBAA0C,MAC1CC,uBAAsC,MAEpCC;SAAoC;IACxCC,YAAYL;IACZM,MAAM,IAAIC;IACVC,MAAM;;GAIKC,yBACXC,GACAC;EAEAV,cAAcS;wBACQ,IAAIzC;yBACH0C;mBACnBjC,yBACFR,2BAA2B;GAKlB0C;MACLF,IAAOT;GAERS,iBAAwC,IAApBA,mBACvBA,iBAAmB,GACnBb;IACEgB,GAAGH;;gBAIcA,2BACnBA,0BAA4B,GAC5Bb;IACEa,gBAAoBA;8BACQ;yBACJV;;yBAK5BE,sBADAD,cAAc;mBAGVvB,yBACFR,2BAA2B;GAKlB4C;EACXC,UAC0B,SAAxBb,6DACA,0KAGA;;GAoBEc,mBACJC,GACAC,GACA9B,GACA+B;EAKIhB,6BAG2CiB,MAAzCH,aAAed,0BACjBc,aAAed,wBAAwB,IAAII;EAC3CU,eAAiBd,wBAGnBkB,IAASJ,aAAed,yBAExBkB,IAASJ;MAIPK,IAASD,MAAWH;aACpBI,KACFD,MAAWH,GAAYI,IAAStB;aAM9BmB,KAAwBhB,uBAG1BmB,EAAOlC,KAAY+B,WAFZG,EAAOlC;GAOZmC,mBACJN,GACAC,GACA9B;OACe,IAENoC,IAAI,GAAGC,IAAIR,eAAiBO,IAAIC,GAAGD,KAAK;QAEzCrE,IADa8D,aAAeA,OAASO,QACfN;aAEfE,MAATjE,KAAsBiC;eACZA;;;qBAKVjC,IAAO8D,WAAaC,MACE/D,EAAKiC,UAAYgC;GAIzCM,gCAA2BT,GAAiBN;MAE1CgB,IAAQV,eAAiBN;OAC3BgB,aAEKV,aAAeN,IACtBM,cAAgBU,GAAO;GAKrBC,6BACJC,GACAC,GACAZ,GACAa;MAGMC,SAAgCZ,MAAxBU,EAASZ,KAA2BY,EAASZ,KAAa;MAEtDY,EAASZ,KAAcc,IAAQD,IAAM;aAGnDF,MACc,KAAZI,IAAeJ,MAAYX,KACb,KAATc,KAAyB,IAAXC,KAAcJ,SAAeX;GAKlDgB,2BACJL,GACAC,GACAK,GACAJ;MAEoB;IAClBH,kBAAkBC,GAASC,GAAUK,GAAMJ;aAClCK,cAAcD;SAAO,IACrBX,IAAI,GAAGC,IAAIU,UAAaX,IAAIC,GAAGD,KAAK;UACrCN,IAAYiB,EAAKX;WAErBI,kBAAkBC,GAASC,GAAUZ,GAAWa;;;GAOlDM,6BACJC,GACAC,GACApF;WAEaiE,MAATjE;SACGF,IAAMmC;MACJmD,MAAkBnD,OAGrBkD,OAAgBnD,eAAeC,KAC/BmD,MAAkBnD;;;GAOpBoD,gCACJF,GACAC,GACArB,GACAD;EAGAoB,kBAAkBC,GAAYC,GAAetB,WAAaC;OAF1D,IAKSM,IAAI,GAAGC,IAAIR,eAAiBO,IAAIC,GAAGD;IAE1Ca,kBAAkBC,GAAYC,GADXtB,aAAeA,OAASO,QACiBN;;GAKnDL,cAAMH;EAEjBA,iBAAmB;8BAIEQ;QAGT,MADCR,WAAcQ,MAAc,IAC1B;WAENjE,IAAM0D,gBAA+B;YAClCmB,IAAWpB,UAAaC;YAIlB,KAHEmB,EAASZ,MAAc;;;eAI9BY,EAASZ;;aAMXR,WAAcQ;uBACDA;eAOAE,WADAV,mBAAsBQ,QAExCR,sBAAyBQ,IACrBR;aACGzD,IAAMmC;UACHM,IAAMC,UAAU,KAAKH,SAAS0B,GAAW9B,KAC/CsB,mBAAsBhB,UAAO0B;;;eAQlBA,WADAV,iBAAoBQ,KACT;QAC1BR,oBAAuBQ;aAClBjE,IAAMmC;UAELsB,cACIhB,IAAMC,UAAU,KAAKH,SAAS0B,GAAW9B,KAC/CsB,mBAAsBhB,UAAO0B;UAG/Bc,gBAAgBxB,WAAcA,YAAe+B,EAASrD;;;;uBAItC8B;;;GAKpBwB,8BAAsBxB,GAAmB9B;EAC5B,iBAAbA,MACE8B,MAAcjB,2BAChBC,wBAAyBgB,UACHE,MAAbhC,KACTc,wBAAyBV,SAAS0B,GAAW9B;GAMtCuD,sBACXzB,GACA9B;EAEAsD,mBAAmBxB,GAAW9B;iBACfa,qBAAsBiB,GAAW9B;GAIrCwD,oBACX1B,GACA9B;EAEAsD,mBAAmBxB,GAAW9B;iBACfa,mBAAoBiB,GAAW9B;GAInCyD,uBACX3B,GACA9B,GACA+B;EAEAuB,mBAAmBxB,GAAW9B;UACtBa,qBAAsBiB,GAAW9B,GAAU+B;0BACtBhB,yBACrBT,IAAMC,UAAU,KAAKH,SAAS0B,GAAW9B;EAC/Ca,6BAA8BP,KAAOyB;GAS5B2B,qBACX5B,GACA9B,GACA+C;MAEMzB,IAAOT;MAOTE,sBAAsB;IAGxB2B,IAAAA,IACEpB,UAAaP,0BACZO,UAAaP,wBAAwBH;QACxC+C,IAAQrC,mBAAsBP;SACzB;IACDO,cACIhB,IAAMC,UAAU,KAAKH,SAAS0B,GAAW9B,KAC/CsB,mBAAsBhB,KAAOyC;QAEpBzB;QACHA;QACRmB,IAAUnB;;WAKsBU,OAD5B4B,SAAyB5B,MAAV2B,IAAsBA,MAAU7B,UAAaE,KACpB4B,EAAa5D,KAAY;qBAGpD8B,GAAW9B;UAEtBsB,SAAYQ,GAAW9B,GAAU+C;kBAEzBN,GAASC,GAAUmB;kBAEnBpB,GAASC,GAAUK,GAAM;GAyB9Be,uBACXxC,GACAyC,GACAC;EAEA3C,cAAcC,GAAM;OACfzD,IAAMyC,QAAgB;QACnB2D,IAAW3D,UAAY,MACvBwB,IAAYxB,QAAU,GAAG2D;QACd3D,QAAU2D,IAAW;YAC9B3D,aAAe;UAChB;MACHoD,UAAU5B,GAAW9B,GAAUgE,EAAQ1D;;;UAEpC;MACHmD,YAAY3B,GAAW9B,GAAUgE,EAAQ1D;;;EAI/CkB;cACeuC;GCnaXG,2CACJnG,GACAiB,GACA8C,GACAqC;OAEKnF;YAAiB;;MAChBX,IAAgBF,iBAAiBJ;MACnCiB,MAAaX;YAAsB;;2CAEvC+F,KACE,6EACEpF,IACA,wCAEAX,IACA,6CACAA,IACA,iJAGF;UAGMH,gBAAgBH,kBAAWA;SAC5BO,YAAYP;cAAc;;QACd4B,WACf7B,QAAQC,IACRsG,kBAAkBtG,GAAMoG;qBD6SLrC,GAAmB9B;wBAC1CuD,WAAWzB,GAAW9B,WACYgC,MAAlCwB,SAAS1B,GAAW9B;KC7SVsE,CAASxC,GAAW9B;;GAInBuE,oBAOXA,SACEvF,GACA8C,GACA0C,GACAL;kBAEgBnF;mBACC8C;iBACFqC;oBACG,EAAC;wBACG,EAACK;;;;QAIW,MAA3BC,0BAA8B;QAC7BlC,IAAQkC,gBAAgBA,yBAAyB,MACjDD,IAASC,oBAAoBA,6BAA6B;QAC5DlC,KAASiC;;eAMNE,cADC3G,IAAOyG,EAAOjC,IACKkC;MAElB,IAAKnG,YAAYP;YA+BK,iBAAlBD,QAAQC;;;sBAzBIiE,OAJf2C,IAAgBnG,iBAAiBT,KAEnCA,IADA0G,uBAAuB3G,QAAQC,SAIJ,iBAAzBuB,sCACYmF,eAAeE;WAIK3C,MAAlCyC,gCACIA,gDACEtG,iBAAiBwG,IACjBF,iBAEFP,gCACES,GACAF,eACAA,gBACAA;6BAIe,6BACIvG,gBAAgByG;;;;;;IAiB1CC,sBAAcC;oBACzBA,IAAkB,OAAQA;GCvFfC,iBACXC,GACAC,GACA1D;EAEAD,cAAc0D,QAAY;MACXE,WAAWF,GAAOC,GAAS1D;;;GAK/B2D,sBACXF,GACAC,GACA1D;MAEM4D,IAAYC,iBAAiBH,UAC7BI,IAAsB;IAAEC,cAAc3D;KAEtC8C,IAAStG,gBAAgBgH,IACzBI,IAAgBP,aAAiBG;MAElB;IACnBK,gBAAgBD;IAChBjF,WAAWiF;IACXE,gBAAgB;IAChB5F,WAAW;IACX6F,WAAWC,mBAAmBR,GAAWF;IACzCW,WAAWC,aAAaZ;YACxBI;WACAL;IACAc,kBAAkBd;;mBAGhBzF,wBACFP,cAAcuG,GAAeJ;QAGTf,mBAAqB,WACzC2B,eAAe3B,GAAKmB,GAAed,GAAQlD,KAE3CyE,UAAU5B,GAAKmB,GAAed,GAAQlD;;GAM7B0E,2BACXjB,GACAC,GACAzD;EAEAF,cAAc0D,QAAYxD;MAEpB2D,IAAYC,iBAAiBH;MACP;IAAEK,cAAc3D;;MAEtCuE,IAAkBlB,aAAiB,aACnCO,IAAgBP,aAAiBG;YAErCI,MAAkBW,2CAClB,oIAEA;mBAGE3G,wBACFP,cAAcuG,GAAeJ;MAGV;IACnBK,gBAAgBU;IAChB5F,WAAW4F;IACXT,gBAAgB;IAChB5F,WAAW;IACX6F,WAAWC,mBAAmBR,GAAWF;IACzCW,WAAWC,aAAaZ;YACxBI;WACAL;IACAc,kBAAkBd;IAClB9D,aAAY;;MAGDL;MACA,IAAI2D,kBACfe,GACAA,GACApH,gBAAgBgH,IAChBf;WAGEpG,QAC4BiE,OAAxBjE,IAAOmI;aACalE,MAAtBjE,gBAAiC;UAC7B6B,IAAY9B,QAAQC,IACpBoI,IAAWhC,4BAA8BvE;eAE9BoC,MAAbmE,GAAwB;QAE1BhC,cAAgBvE;YAGMuG,OADJ9B,kBAAkBtG,GAAMoG,iBACEvD,YAAYuD,SAAWA;YAC7DiC,IAAexB,WAAWyB;uBACjBlC,GAAKiC,GAAclI,gBAAgBH;UAC7C6B,KAAayG;wBACFlC,gBAAkB8B,GAAiBrG,OAEjD0G,EAAQhF,GAAMiF,KAAa3F,YAAYuD,SAAWA;;;;EAM1D3C;;GAIWgF,yBACXzB,GACA0B,GACAnF,GACAmE;EAEME,IAAYC,aAAaa;MACzBC,IAAQC,YAAYhB;WAET3D,WADA2D,EAAUe,EAAM;mDAExBtC,KACL,mIAEA;;MAIEpF,IAAWhB,oBAAoB4I;MACnBC;IAAEC,YAAY9H;KAAasC;MACvCQ,IAAYiD,cAAkBgC;OAC/BjF;mDACIsC,KACL,sIAEEpF,IACA,MACF;;mBAIAM,wBACFP,cAAcC,GAAU4H;MAGL;IACnBrB,gBAAgBvG;IAChBqB,WAAWyB;IACX0D,gBAAgB;IAChB5F,WAAW;IACX6F,WAAWA,KAAa;eACxBE;IACAP,QAAQ;MAAEC,cAAc3D;;WACxBqD;IACAc,kBAAkBd;;iBAGLZ,GAAKrC,GAAW5D,gBAAgB0I,IAAWG;GAGtDjB,0BACJ3B,GACArC,GACA0C,GACAlD;MAGMtC,IADU8C,MAAcqC,mBAAqB,WACxBrC,IAAYR;MACf;IAExB0F,YAAyBlF,GAAW,cAAc9C;QAErC,IAAIuF,kBAAkBvF,GAAU8C,GAAW0C,GAAQL;aAE5DpG,QAC4BiE,OAAxBjE,IAAOmI,aAA4B;UACnCtG,IAAY9B,QAAQC,IACpBwI,IAAYlC,kBAAkBtG,GAAMoG;UACzBxE,WAAWC,GAAW2G;UACjCU,IAAa3F,EAAKrD,cAAcF,KAChCuC,IAAMF,SAAS0B,GAAW9B;UAEH,iBAAzBV;iBACiB0C,MAAfiF,GAA0B;UACtBC,IAAS/C,eACX,qDACA;mBAGoBnC,MAAtBjE,iBACI,kCACA;mDAENqG,KACE,sCACEpE,IACA,uDACAmH,IACA,qBACAD,GACF;;;gCAI+BlI,KACjCmF,0CAA4CnF,GAAUY;;;iBAItD7B,iBAEFiJ,YAAyBlF,GAAW9B,GAAUiH,MAGxCG,IAAYxC,WAAWqC,IACvBlE,IAAOsE,WAAWlD,GAAK7D,GAAKpC,gBAAgBH,IAAOqJ;MACzDJ,UAAuBlF,GAAW9B,GAAU+C;;;GAK5CsE,sBACJlD,GACAqB,GACAhB,GACAlD;MAEI0B,cAAc1B,IAAO;aACjBgG,IAActE,MAAM1B,WACjBc,IAAI,GAAGC,IAAIf,UAAac,IAAIC,GAAGD,KAAK;UACrCmF,IAAOjG,EAAKc,IAEZoF,IAAWpH,SAASoF,QAAmBpD;UAE/BiF,WAAWlD,GAAKqD,GAAUhD,GAAQ+C;QAExCnF,KAAKuB;;;;EAIV,IAAa,SAATrC;;;MAKe,cADR6C,oBAAsB7C,MACPQ,IAAY0D;MAC5BlE;aAGf6C,aAAe7C,iBACD,SAAdQ,KACoB,wBACnB9C,WAAkB,iBAClBA,WAAkB,WACN,eAAbA,8CAEAoF,KACE,qDACEoB,IACA,6LAIAxG,IACA,mIAGAA,IACA,+BACF;iBAIWmF,GAAK7D,GAAKkE,GAAQlD;;GAK7ByE,qBACJ5B,GACAnF,GACAwF,GACAlD;MAEMmG,IACJzI,MAAamF,mBAAqB,eAClCnF,MAAamF,mBAAqB;MAEvB,IAAII,kBAAkBvF,GAAUA,GAAUwF,GAAQL;WAE3DpG,QAC4BiE,OAAxBjE,IAAOmI,aAA4B;QACnCtG,IAAY9B,QAAQC,IACpBwI,IAAYlC,kBAAkBtG,GAAMoG,cACpCnE,IAAWI,SAASpB,GAAUW,WAAWC,GAAW2G;aAChCvE,MAAtBjE,gBAAiC;UAC7BkJ,IAAarC,WAAWtD,EAAKrD,cAAcF;qBAClCoG,GAAK8C,GAAY/I,gBAAgBH;;IAG9C0J,MAEFtD,mBAAqBnF,GACrBmF,cAAgBnF,GAChBmF,mBAAqBnE,GACrBmE,cAAgBvE;SAKAoC,OADVsE,IAAUnC,gBAAkBnF,GAAUY,OAE1C0G,EAAQhF,GAAMiF,KAAa3F,YAAYuD,SAAWA;;GAOpDuD,0BACJvD,GACA7C,GACAkD;MAEIxB,cAAc1B,IAAO;aACjBgG,IAActE,MAAM1B,WACjBc,IAAI,GAAGC,IAAIf,UAAac,IAAIC,GAAGD;MACtCkF,EAAQlF,KAAKsF,eAAevD,GAAK7C,EAAKc,IAAIoC;;;;EAE1B,SAATlD,MAMO,UADZQ,IAAYqC,oBAAsB7C,MAEtCwE,eAAe3B,GAAKrC,GAAW0C,GAAQlD,KAGvCyE,UAAU5B,GADO7C,cACQkD,GAAQlD;GCjWxBqG,+BACXxD,GACArC,GACA0C;MAE8B,YAAd1C,GAGF;IACZ9C,IAAAA,IAAWgI,WAAwBlF,GAAW;QACtB;;;gBAGGA,GAAW,mBAAcE;;QAGzCF;;MAGA,IAAIyC,kBAAkBvF,GAAU8C,GAAW0C,GAAQL;WAE5DpG,QAC4BiE,OAAxBjE,IAAOmI,aAA4B;QACnCtG,IAAY9B,QAAQC,IACpBiC,IAAWL,WACfC,GACAyE,kBAAkBtG,GAAMoG;qBAIxB7E,wBACA6E,sBACAnF,KAEAmF,0CAA4CnF,GAAUY;aAG9BoC,MAAtBjE;MACFiJ,YAAyBlF,GAAW9B,QAAUgC;eAExC4F,IAAc1J,gBAAgBH,IAC9BgF,IAAOiE,SAAsBlF,GAAW9B,IAE9CgH,UAAuBlF,GAAW9B,QAAUgC;IAC5CgF,YAAyBlF,GAAW9B,QAAUgC,IAE1CgB,cAAcD,IAAO;MACdX,IAAI;WAARyF,IAAWxF,IAAIU,UAAaX,IAAIC,GAAGD,KAAK;YACrC0F,IAAY/E,EAAKX;iBACnB0F,KACFH,oBAAoBxD,GAAK2D,GAAWF;;;WAIxCD,oBAAoBxD,GAAKpB,GAAM6E;;;GC7E1BG,QAYXA,SACElC,GACAmC,GACAC,GACAC,GACA9G;;sBAoDY;;OAEIqD;qBACG;;oBAGR9E;mBAxDMqI,KAAa;6BACHE,KAAuB;cACtC9G,KAAQ;0BACIyE;iBAET;cACFoC,KAAWA,cAAqB;kBAC5BA,KAAWA,kBAAyB;;OAK7CE,mCACAC,IAAeC,qBACfC,IAAmBD;oBAQP;WANZE,IAAYJ,IAAYA,SAAiB;cACzCK,IAAeJ,IAAeA,SAAoB;kBAClDK,IAAmBH,IACrBA,SACA;4BAQa,IACdC,KAAY,WACZC,KAAe,cACfC,KAAmB;2BAGJ;WACT;cACG;kBACI;sBAGC;WACR;cACG;kBACI;;uBJaDC;WAAwC;MAC3DC,uBAAsB;MACtBC,kBAAkBhI;MAClBiI,cAAa;oBACbH;MACAjG,SAAS,IAAI5D;MACb6D,UAAU9B;MACVkI,SAASlI;MACT+C,OAAO3C;MACP+H,SAAS/H;MACT+C,SAAS;;GInBKiD,CAAkBvC,gBAAgB;;;sCAWrCuE;yBACcA;;;uCAGb1H;;OAELtC;;;WAEmCgD,MAA7ByC,eAAezF;;;MAItBsB;YACUtB,SACNyF,UAAUzF,GAAUsC,KACVU,QAAPiH,IACT3I,IAAM,KAAG2I,IACQjH,QAARkH,MACT5I,IAAM,KAAG4I;aAGKlK,UAAYsB,IAAQ;;;6CAGpB4B,GAA8BlC;MAK5B,UAJZ8B,IACO,SAAXI,KAAqC,uBACjCuC,iBAAiBvC,KACjBA;;;MAEA+E,IAAaD,WAAwBlF,GAAW9B;oBAClDiH,IAAiCA,KAC/BlE,IAAOiE,SAAsBlF,GAAW9B,MAChC+C,IAAO;;;mCAIrBb,GACAiH,GACAtJ;gCAE8BqC,GAAQvC,WAAWwJ,GAAOtJ;;;2CAG1C4G,GAA8BhB;YDnHrBV,GAAcC;QACjCE,IAAYC,iBAAiBH;QAEd;MACnBS,WAAWC,mBAAmBR,GAAWF;MACzCW,WAAWC,aAAaZ;aACxBD;MACAc,kBAAkBd;;wBAIlBZ,GACAA,mBAAqB,UACrBjG,gBAAgBgH;ICuGLT,MAAM2E,mBAAc3C,GAAOhB;;;yCAG1BvD;mBACNJ,IACO,SAAXI,KAAqC,uBACjCuC,iBAAiBvC,KACjBA,cJyQoBJ;wDAEtBoB,IAA0B,IAC1BC,IAA6B,IAAItE;uBAEpBiD;yBAGEoB,GAAYC,GAAerB,GAAW6B;yBACtCT,GAAYC,GAAerB,GAAWiH;;GIjR7B/B,CAA2BlF,KAAa;;;uCAIpEuH,GACA/C;EAEMtB,IAAUoE,mBAAcC,SAAaA;gBAC5B/C,EAAQ7B,eAAeO,mBAEzBP,MAAMO,GAASsE;;;qCAIpBD;cACI5E,MAAM2E,mBAAcC,SAAaA;;;wCAI7CE,GACArH,GACAuD;sBAEoBhB,MAAM8E,GAAcrH,GAAQuD;;;yCAIhD8D,GACAjI,GACAmE;gBAEchB,MAAM8E,GAAcjI,GAAMmE;;;IC/K/BpB,6BACXtG,GACAyL;WAEuBxH,MAAnBjE,eAA0D,MAA1BA;;;WAI9B8B,IAAOe,YACT6I,IAAW,GAENrH,IAAI,GAAGC,IAAItE,oBAAuBqE,IAAIC,GAAGD,KAAK;QAC/CsH,IAAM3L,YAAeqE,IACrBL,IAAQ4H,4BAAoBD,SAAWF;YACzCzH,MACFlC,EAAK/B,QAAQ4L,MAAQ3H,GACrB0H;;aAIGA,IAAe5J,IAAO;GAIlB6F,8BACX3H,GACAsL;WAEiCrH,MAA7BjE;WACK;;MAGH8B,IAAmBwJ,KAAuB;gDAERG,GAAMI;QACtCZ,IAAOlL,QAAQ8L,aACjB7H,IAAQlC,EAAKmJ;aACHhH,MAAVD;eACuBC,MAArB4H;QACF7H,IAAQ4H,4BAAoBC,gBAAkB/J;;;;;MAM7CmJ,KAAQjH;;MAEZnB;GC5CQiJ,mBAGXA,SAAYxB;gBACIyB,0BAAkBzB;;;sDAGlBrJ,GAAkBY;qBAC1BuJ,IAAQY,SAAStF,aAAazF,GAAUY,OACd,IACzBoK,uBAAeb;;;qDAGTnK,GAAkBY;WAEjBoC,OADRmH,IAAQY,SAAStF,aAAazF,GAAUY;YACd;;MACjBqK,sBAAcd,UAAcA,gBAAoBA;4BAC7Ce,MAAWF,uBAAeE;;;6DAGvBlL,GAAkBmL;WAC9BJ,SAAStF,aAAazF,GAAUmL;;;wDAIzC9L,GACAW;OAEKA,MAAaX;YAAsB;;MACpCW,MAAaX;YAAsB;;MAEjC+L,IAAe3F,oBAAoBpG,IACnCgM,IAAa5F,oBAAoBzF;MAEnCoL;iBACsBC;;GA+C9BC,4BAA4BzF,GAAQ7F;IAClC2C,UACEkD,6CAAqCA,+EACrC,sCACE7F,IACA,2IAEF;IAnDmBoL,GAAc/L;mBAChBgM,GAAYrL;oCACKoL,GAAcC;;;IAI9CN,oBACJ1B,GACArJ,GACAY;mBAEM2K,IAASlC,UAAerJ,IACLA;WAGXgD,WADAuI,cAAmB3K;6CAE/BwE,KACE,+BACExE,IACA,0BACAZ,IACA,2HAGF;;;;;;0BASoB6F,GAAQ7F;EAChC2C,UACEkD,gFACA,oCACE7F,IACA,6FAEF;;;AC9EJnB,IAAM2M,0BAAkBzM;oBACRQ;;;cAOZR;oBAAsBQ;;;AAJnBV,IAAMsH,4BACXsF;eAEMvF,IAAYuF,mEAMhB,wIAEA;;;;eAQ6C5I,GAAgB9D;EAC7D8D,EAAI/D,QAAQC,MAASA;;;;AAFlBF,IAAM+H,wBAAgB6E;8BACJD,8BAGpB;GAEQ9F,yBACX3G,GACAyL;WAGmBxH;YACV;;OAHA,IAOAI,IAAI,GAAGC,IAAIqI,UAAmBtI,IAAIC,GAAGD,KAAK;QAC3CuI,IAAYD,EAAWtI,IACvB4G,IAAOlL,QAAQ6M,IAGfC,IAAqB,cAAT5B;SACb4B,KAAsB,WAAT5B,OAGZU,IAAMiB,cAAsBA,YAAoB,KAAK,SAC9B,SAAjB7M,QAAQ4L,OAGC,qBADf3H,IAAQ4H,4BAAoBD,SAAWF,OACD,SAAVzH;mBAIbA,KAASA;;;UAGzB;GCfI0E,iBACX1B,GACAC,GACA1D;EAEAD,cAAc0D,QAAY;MACX8F,KAAK9F,GAAOC,GAAS1D;;;GAKzBuJ,gBACX9F,GACAC,GACAqE;MAEMnE,IAAYC,iBAAiBH,UAC7B8F,IAAU/F,aAAiBG,cAC3B6F,IAAa7M,gBAAgBgH;MAEd;IACnBK,gBAAgBuF;IAChBzK,WAAWyK;IACXtF,gBAAgB;IAChB5F,WAAW;IACX6F,WAAWC,mBAAmBR,GAAWF;IACzCW,WAAWC,aAAaZ;IACxBgG,UAAS;WACTjG;IACAc,kBAAkBd;;mBAGhBzF,wBACFP,cAAc+L,GAAS5F;MAGdmE,KAASzI;MAElBkK,MAAY3G,mBAAqB,WAC7B8G,SAAS9G,GAAK2G,GAASC,GAAYzJ,KACnC4J,cAAc/G,GAAK2G,GAASC,GAAYzJ;SAEvC;IACL+D,cAAc3D;IACdsJ,cAAkBhJ,MAATV,KAAqB,IAAQ6C;IACtC7C,WAAeU,MAATV,IAAqB,OAAOA;;GAIhC2J,oBACJ9G,GACArC,GACA0C,GACA2G;MAEuC;;;MAI1B,IAAI5G,kBAAkBzC,GAAWA,GAAW0C,GAAQL;OACpDvD,yBACKuK;WAEdpN,QAC4BiE,OAAxBjE,IAAOmI,aAA4B;QACnCkF,IAAanN,cAAcF,IAC3BkJ,IAAakE,EAAaC;eAC5BrN,kBAAkD,SAAfkJ,KAC/BG,IAAYxC,WAAWqC,IAC7B3F,EAAK8J,KAAcC,cAAclH,GAAKjG,gBAAgBH,IAAOqJ,MAE7D9F,EAAK8J,KAAcnE;;;GAOnBoE,yBACJlH,GACAK,GACA2G;MAEInI,cAAcmI,IAAe;aACzB7D,IAActE,MAAMmI,WACjB/I,IAAI,GAAGC,IAAI8I,UAAqB/I,IAAIC,GAAGD;MAC9CkF,EAAQlF,KAAKiJ,cAAclH,GAAKK,GAAQ2G,EAAa/I;;;;EAElD,IAAqB,SAAjB+I;;;uBAKOhH,oBAAsBgH,WAKhBnJ,OADhBiF,IAAaiE,cAAc/G,GAAKrC,GAAW0C,GAAQ5D,eACvB,OAAOqG,IAElCgE,SAAS9G,GAAKgH,cAAyB3G,GAAQ2G;GAI7CG,wBACXvG,GACA0B,GACAvE,GACAuD;EAEME,IAAYC,aAAaa;MACzBC,IAAQC,YAAYhB;WAET3D,WADA2D,EAAUe,EAAM;oDAE/BtC,KACE,kIAEA;IAGK;;MAGHpF,IAAWhB,oBAAoB4I;0BACF1E,iBACjCA,eAAoBlD;YAIF,uBACd+F,cAAkB8B;IAAEC,YAAY9H;KAAakD,MAC7CA;oDAGJkC,KACE,gIAEEpF,IACA,MACF;IAGK;;mBAGLM,wBACFP,cAAcC,GAAU4H;uBAGLzC;IACnBoB,gBAAgBvG;IAChBqB,WAAWyB;IACX0D,gBAAgB;IAChB5F,WAAW;IACX6F,WAAWA,KAAa;eACxBE;IACAqF,UAAS;WACTjG;IACAc,kBAAkBd;KAICjD,GAAW5D,gBAAgB0I,IAAWhG,eAAe;GAItEsK,yBACJ/G,GACArC,GACA0C,GACAlD;2CAGMiK,IAAUzJ,MAAciD,aAAiB,UAGzC/F,IAAYuM,IAEdzJ,IADAkF,WAAwBlF,GAAW;MAEf;IAIxBR,eAAkBtC;QACL,IAAIuF,kBAAkBvF,GAAU8C,GAAW0C,GAAQL;aAE5DpG,GACAyN,KAAY,GACZC,KAAc,QACczJ,OAAxBjE,IAAOmI,aAA4B;UAEnCtG,IAAY9B,QAAQC,IACpBwI,IAAYlC,kBAAkBtG,GAAMoG,cACpCiH,IAAanN,cAAcF,IAC3BiC,IAAWL,WAAWC,GAAW2G,IACjCU,IAAaD,WAAwBlF,GAAW9B,IAChDM,IAAMF,SAAS0B,GAAW9B;uBAE5BV,wBAAyCuG,KAAoB7G,KAC/D6G,yBAAwC7G,GAAUY;UAKhD8L,YAEE1D,IAAYjD,YAAgB/F;eAChBgD,MAAdgG,KAA2D,uBAAfpI;YAG9CuE,mBAAqBnF,GACrBmF,cAAgBrC,GAChBqC,mBAAqB7D,GACrB6D,cAAgBvE;aAIGoC,MAAfiF,MACF3F,EAAK8J,KAAcnE,IAGrByE,IAAiB1D,EAAUpI,GACzB0B,GACAiF,KAAa3F,YACbmE,GACAZ,SAGwBnC,MAAtBjE,mBAGF2N,IAAiBC,sBACfxH,GACAnF,GACAY,GACAU,GACApC,gBAAgBH,IACfuD,EAAK8J,MAAwBxK,YAC9B8K;aAKmB1J,MAArB6D,KACmB,SAAnB6F,MACC7F,kBAAiC7G,GAAUY;;;;mBAMrC7B,iBAET2N,IAAiBzE,SAIJjF,OADPe,IAAOiE,SAAsBlF,GAAW9B,MAE5C0L,IAAiBE,YACfzH,GACApB,GACA/D,GACAY,GACA1B,gBAAgBH,IAChBuD,EAAK8J,MAEwB,wBAA2B,SAAfnE,MAE3CyE,IAAiBzE;;eAQAjF,MAAnB0J,UACqB1J,MAArB6D,KACAA,kBAAiC7G,GAAUY;QAI3C6L,KAAc,GACdnK,EAAK8J,KAAc;aACd;QAAA,SAAuBpJ,MAAnB0J;;;aAKG;UACPN,KAAcM;;;IAInBD,MAAatH,aAAc;gBACbsH,MAAgBD,SAAYxJ,IAAYV;;GAyHtDqK,iCACJxH,GACAnF,GACAY,GACAU,GACAkE,GACAqH,GACAzG;MAEIpC,cAAcoC,IAAS;;aAKFpD,MAArB6D,KACAA,iBAAgC7G,GAAUY;aACtC0B,IAAW0B,MAAMoC,WACdhD,IAAI,GAAGC,IAAI+C,UAAehD,IAAIC,GAAGD,KAAK;UAEvC0J,IAAcH,sBAClBxH,GACAnF,GACAY,GACAQ,SAASE,QAAQ8B,IACjBoC,QAEaxC,MAAb6J,IAAyBA,EAASzJ,UAAKJ,GACvCoD,EAAOhD;eAGWJ,MAAhB8J,KAA8BC;QAGhCzK,EAAKc,UAAqBJ,MAAhB8J,IAA4BA,IAAc;;;;;;;EAKnD,IAAe,QAAX1G;;;MAEA4G,YAAY5G;oBACKpD,MAAb6J,IAAyBjL,aAAaiL,GAC1B,uBACrBX,cAAc/G,GAAKiB,GAAQZ,GAAQlD,cAhKzC6C,GACA7D,GACAkE,GACAlD,GACA8D;;8BAGoCA,MAAW9E;UACzC2L,IAAmB7G,cACnBpG,IACJgI,WAAwBlF,GAAW,iBAAiBmK;UAGhC,wBACnBA,KAAoBjN,MAAaiN;iDAGlC7H,KACE,6CACEtC,IACA,+EAEF;;QAQJR,eAAkBtC;YACL,IAAIuF,kBAAkBvF,GAAU8C,GAAW0C,GAAQL;iBAG5DqH,KAAY,GACZC,KAAc,QACczJ,OAAxBjE,IAAOmI,aAA4B;cAEnCtG,IAAY9B,QAAQC,IACpBqN,IAAanN,cAAcF,IAC3BiC,IAAWL,WACfC,GACAyE,kBAAkBtG,GAAMoG,eAEpB7D,IAAMF,SAAS0B,GAAW9B,IAC1BiH,IAAaD,WAAwBlF,GAAW9B,IAChDkM,IAAc9G,EAAOxF;2BAEvBN,wBAAyCuG,KAAoB7G,KAC/D6G,yBAAwC7G,GAAUY;cAKhD8L;qBACAQ,UAAmDlK,MAAtBjE,iBAE/B2N,IAAiBQ,SACclK,MAAtBjE,iBAET2N,IAAiBzE,SACQjF,MAAhBkK,IAETR,IAAiBC,sBACfxH,GACAnF,GACAY,GACAU,GACApC,gBAAgBH,IAChBuD,EAAK8J,IACLc,UAMWlK,OAFPe,IAAOiE,SAAsBlF,GAAW9B,MAG5C0L,IAAiBE,YACfzH,GACApB,GACA/D,GACAY,GACA1B,gBAAgBH,IAChBuD,EAAK8J,MAEwB,wBAA2B,SAAfnE,MAE3CyE,IAAiBzE;mBAQAjF,MAAnB0J,UACqB1J,MAArB6D,KACAA,kBAAiC7G,GAAUY;YAI3C6L,KAAc,GACdnK,EAAK8J,KAAc;iBACd;YAAA,SAAuBpJ,MAAnB0J;;;iBAKG;cACPN,KAAcM;;;QAInBD,MAAatH,aAAc;mBACC7C,SAAZU;;KA+CdmK,CAAmBhI,GAAK7D,GAAKkE,GAAQlD,GAAM8D;;2CAE/ChB,KACE,2CACE9D,IACA,uGAEF;GAOAsL,uBACJzH,GACApB,GACA/D,GACAY,GACA4E,GACAqH;MAEI7I,cAAcD,IAAO;;aAGAf,MAArB6D,KACAA,iBAAgC7G,GAAUY;aACtCwM,IAAcpJ,MAAMD,WACjBX,IAAI,GAAGC,IAAIU,UAAaX,IAAIC,GAAGD,KAAK;UACrC0F,IAAY8D,YAChBzH,GACApB,EAAKX,IACLpD,GACAY,GACA4E,QACaxC,MAAb6J,IAAyBA,EAASzJ,UAAKJ;eAEvBA,MAAd8F,KAA4BiE;QAG9BK,EAAQhK,UAAmBJ,MAAd8F,IAA0BA,IAAY;;;;;;;EAKlD,gBAAI/E,IACF,OAEAmI,cACL/G,GACApB,GACAyB,QACaxC,MAAb6J,IAAyBjL,aAAaiL;GAKtCG,uBAAenH;SACN,wBACC,wBAA6C;GC5gBvDwH,2BAAmBC,GAAeC;+BACnCD;IACHE,+BACKF;MACHG,4BACKH;QACHI,cAAcH;;;;GAMdI,wBAAgBL;+BACjBA;IACH7F,OAAOmG,oBAAeN;;GAIlBO,4BAAoBP;;GAGpBQ,4BAAoBR;SACH,YAArBA;GAOIS,4BAAoBT;0BACAA,MAAgC,mBAAzBO,iBAAiBP;GAS5CU,2BACJ9H,GACA+H;+BAEG/H;IACHsH,+BACKtH;qBACH+H;;;;;eAsMSX;0BAAuBA;;;eAS1BY;yBAAuBA,aAAeA;;;eADnCA;SAAuB,WAAhBA;;;eAQPA;SAAuB,WAAhBA;;;eAkCDZ;UAAOS,iBAAiBT;;;ICtN9Ba,sCACX9E,GACA5B;MAEM2G,IAA+C,IAC/CC,IAAyC,IACzCC,IAAW,IAAIC,iBAASlF;gBAG5B5B,GACA+G,0BAAkBF,GAAU;IAC1BG,gBAAO1P;UACDA,gBAAmB;YACfW,IAAOgP,YAAYJ;eACP;UAChBK,MAAMpP;UACNF,eAAe;YACbsP,MAAMpP;YACNyK,MAAM4E,SAASlP;;UAEjBsK,MAAM4E,SAAYlP;UAClBmP,cAAc9P;;;;IAIpB+P,6BAAoB/P;MAClBqP,OAAwBrP;;;SAKvB,EAACqP,GAAoBC;;;iBAmClBU;SAAoB,eAAfjQ,QAAQiQ;;;iBA8DUC,GAAKC;EACJ,yBAApBA,UACFD,MAAQC;;;;AA/FtB,IAAaC,+BACX7F,GACA5B,GACA0H,GACAC;aAkCoDC,GAAGC;UACvCC,IAAYJ,EAAoBG;;;SADqB,IAMlDlM,IAAI,GAAGC,IAAIkM,UAAkBnM,IAAIC,GAAGD,KAAK;mBAC3BmM,EAAUnM,aACzBoM,IAAe1Q,QAAQ8I,IACvB6H,IAAgBC,iBAAiB9H,IAG9B+H,IAAI,GAAGtM,IAAIoM,UAAsBE,IAAItM,GAAGsM,KAAK;YAC9C3F,IAAOyF,EAAcE;cACQ3F,OACjC4F,EAAsB5F,KAAQoF,EAAcpF;;MAKhD6F,EAAoBL,KAAgB5H;aAE7B;QACL+G,MAAMpP;QACNyK,MAAM4E,SAASY;;;;;MAxDvBlB,IAAW,IAAIC,iBAASlF,IAExBuG,IAGFhO,YAEEiO,IAGFjO,YAGEkO,IAAyC,IAAIjQ;uBAGjD4H,GACA+G,0BAAkBF,GAAU;IAC1BG,OAAO;MACLsB,gBAAOhR;YACAA;cAIC2M,IAAa3M;cAGf2M,aAAsB3M;gBAKpBiR,IADgBC,SAAS5G,GAAQiF,aA8BpC,KAEG4B,IAAqBhR,gBAAgBH;gBAGY,MAArDmR,WAA4BF,WACxBA,SAAsBE,KACtB,EACE;cACEvB,MAAMpP;cACNyK,MAAM4E,SAAS;;yCAKpB7P;0BACH2M;cACAmD,cAAc;gBACZF,MAAMpP;4BACN4Q;;;;;;;IAKRC,UAAU;MACRL,gBAAOhR;QACLA,8BAKG+Q;;MAELO,gBAAOtR;YAEMuC,GADLgP,IAAc,UAAIvR;aACbuC;UACTgP,OAAiBT,EAAoBvO;;aAClCzC,IAAMyC;UACTgP,OAAiBV,EAAsBtO;;qCAC7BvC;uBAAMuR;;;;;GAOtB1B,oBAAY7L;SAA6B;IAC7C4L,MAAMpP;WACNwD;;GAIIkN,oBAAY5G,GAAuBiF;EACjC5O,IAAOD,WAAW6O;iCACH5O,KAQd6Q,uBAAe7Q,KAAQ2J,mBAAwB3J,KAAQ,EAACA,gDAP7D0F,KACE,kFACA;EAEK;GAOLsJ,uBAAeJ;aACb5O,IAAOD,WAAW6O,kBAEbiC,uBAAe7Q,4CACxB,iFACA;;GAOEgQ,4BAAoB3Q;MAClB2I,IAAkB;gBAElB3I,GAAM;IACVyR,yBAAgBC;MACd/I,OAAW5I,QAAQ2R;;;;;;;;iCD9MKC;kBAAwCtR;eA2M7D8O;0CAEOyC,IAAS9C,iBAAiB3H;UACA;QAC9BA,WAAWmH,gBAAgBnH,GAAWqH;QACtCjL,MAAM4L;QACN1N,OAAO0N;QACP0C,YAAY1C;;UAID,wBAAXyC,KACY,kBAAXA,KAAwC,cAAZpD;QAE7BnH,WAAe,GACfyK,qBACE7C,gBAAgB9H,GAAW;;;;;UAvN5BwK,IAAO;QAEZ3K,IAAQ,IAAIgD,MAChB2H,WAAc,IAAI7F,iBAAiB6F,iBAAe1N,GAClD0N,aACAA,WACAA,cACAA;QAIEA,WAAc;UACV3L,IAAU2L;UAChBI,IAAY/L,0BAAoBC;QAC9BF,YAAYiB,QAAYhB,GAASC;;;QAI/B+L,IAAiB,IAAIlR,KACrBmR,IAAoB,IAAI7O,KACxB8O,IAA4BrP,YAE5BsP,aACJC,GACA9K;WAEqBrD,MAAjBqD,KAEFA,sBAAqB+K;YACbhP,IAAO6O,EAAKG;iBACLpO,MAATZ,GAAoB;UACtB6O,EAAKG,KAAO;cACC;eAARvI,IAAWxF,IAAIjB,UAAagB,IAAIC,GAAGD;YACtC+N,MAAsB/O,EAAKgB;;;;OAO/BiO,aACJnL,GACAiL;MAGAA,oBAA0B7P;YACpBA,MAAQ4E,OAAe;cACnBoH,IAAK0D,MAAQ1P;qBACfgM,MACF0D,SAAW1P,IACXuP,qBAA0B7C,gBAAgBV,GAAI;;;OAOhDgE,aAAoBpL;mBAtFEoH;wBATDA;iBACN,eAArBA;UAS2BA,MAAgC,mBAAzBO,iBAAiBP;OAsF7CiE,CAAqBrL,IAAY;2BAEVc,gBAAgBjB,GAAOG,GAAW5E;cACvD+E,WACF0K,MAAmBzP,IACb6P,IAAoB,IAAItR,KAC9BqR,EAAyBC,GAAmB9K,IAC5CgL,EAAyBnL,GAAWiL;;OAMpC7M,aAAsBgJ,GAAejH;MACzCA,oBAAqB+K;SACNH,EAAKG,OAASH,EAAKG,KAAO,UAC7B9D;cAEGA,UACX0D,MACE1D,OACyB,mBAAzBO,iBAAiBP,KACbU,gBAAgBV,GAAI,uBACpBA;;OAQNkE,aACJtL;cAEwCuB,MAAM1B,GAAOG;;eAGjD5D,IACFoL,IAAe,UAEfpJ,EAAmB4B,GAAWG,IAC9BqH,IACG1B,KAA2C,iBAAhC6B,iBAAiB3H,KAEzB,YADA;aAID;QACLqH,SAASG;mBACTxH;cACA5D;;OAKEmP,aAAyBrL;0DAEvBmG,IAAUuB,iBAAiB5H;YAKV5E,OACrByP,SAAsBzP,aT+KIgB,GAAoBC;eAE3CD,UAAaC;6BACCD,WAAcC;6BACdD,SAAYC;OSlL7BmP,CAAgB3L,QAAYzE;UAKjB,QAATgB,GAAqC;QACvCqP,IAAAA,IAAoB7L,MAAMC,GAAOG,GAAW5D;YAExCiK,GAAS;UACLqF,IAAAA,IAAcnK,MAAM1B,GAAOG;cAC1B0L;cACaA;;cAEbnK,MAAM1B,GAAOG,GAAW5D;;;MAK7B6O,IAAoB,IAAItR;QACLsR,GAAmBQ;WAE1CT,EAAyBC,GAAmBU;QAIrBzL,aAAkB+K;gBAGNnO,MAAtB6O,KACbvN,EAAmB8B,aAAkByL;aAGhC;cAAEvP;eAAM9B;oBAAOoQ;mBAAY1K;;;oBAG7B4L;MACCC,IAAwBC,YAANF;UAIlBG,IAAenB,IAKfoB,eAASC,gBAATD,CADAE,WAAK,EAALA,CADAC,aAAOC,kBAAYxB,GAAnBuB,CADAN,OAKDQ;UAMHP,YADAQ,UAAIlB,EAAJkB,CADA3P,UAAI8K,aAAJ9K,CADA4P,aAAO,EAACR,GAAcF;UAOlBW,IAIJV,YADAnP,UAAI2O,EAAJ3O,CADA8P,mBAAAA,CADAC;UAUA/P,gBAAAA,CADA8P,mBAAAA,CADAD;UAUA7P,YAAAA,CADA8P,mBAAAA,CADAD;UAwCA7P,UAAI4O,EAAJ5O,CATAgQ,EACEC,YAAM,EAGFH,mBAAAA,CADAC,IAGFG;yBAMO,EAACC,GAASC;;;;;;;;;oCCrUM7T;;kBAEMA;eAmByB8T;mBACnCA;;uBAnBrB7J,IAASyB,0BAAkBqI,IAE3BC,IAAmB,IAAIvT,KAEvBwT,IAAmB,IAAIxT,KAEvBuP,IAAiCxN,YAEjCuN,IAAuCvN,YAGvC0R,aAA0BhG;UACL,eAArBA;;;UAKOtD,GADLuJ,IAAoC3R;WAC/BoI;QACTuJ,EAAiBvJ,KAAQmF,EAAoBnF;;mCAM1CsD;QACH7F,OAAOyH,oBACL7F,GACAiE,SACAiG,GACAnE;;OAMAoE,aAAuBpU;;UACL,gCAItBiU,MAAqB/R,KACjB8R,MAAqB9R;QAIzB8R,MAAqB9R;iBAEsB6M,2BACzC9E,GACA5B;;aAdmE,IAiB5DrE,IAAI,GAAGC,IAAI+K,UAA2BhL,IAAIC,GAAGD,KAAK;cACnDwE,IAAWwG,EAAmBhL;YACtBtE,QAAQ8I,MAAaA;;QAG5BxE,IAAI;aAAGC,IAAIgL,UAAqBjL,IAAIC,GAAGD;UAExC1D,IAAOZ,SADP8I,IAAWyG,EAAajL,oBAExBqQ,IACJtE,EAAoBzP,OAAUyP,EAAoBzP,KAAQ,KAE3DkI,gBAA+B6L;UAChCA,OAAa;iBAAEnS;sBAAKsG;;;;OAIlB8L,aAA0BtU;wCAE5BiU;;oBAIGvB;eAKHjP,UAAIyQ,EAAJzQ,CADA2P,UAAIkB,EAAJlB,CADAA,UAAIgB,EAAJhB,CADAV;;;;;;;;;;;;;"}