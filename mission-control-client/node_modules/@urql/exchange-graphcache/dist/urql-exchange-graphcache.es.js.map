{"version":3,"file":"urql-exchange-graphcache.es.js","sources":["../src/ast/node.ts","../src/helpers/help.ts","../src/store/keys.ts","../src/store/timing.ts","../src/store/data.ts","../src/operations/shared.ts","../src/ast/traversal.ts","../src/operations/write.ts","../src/store/store.ts","../src/operations/invalidate.ts","../src/operations/query.ts","../src/ast/variables.ts","../src/ast/schemaPredicates.ts","../src/cacheExchange.ts","../src/populateExchange.ts"],"sourcesContent":["import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  GraphQLOutputType,\n  Kind,\n  isWrappingType,\n} from 'graphql';\n\nimport { SelectionSet, GraphQLFlatType } from '../types';\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias !== undefined ? node.alias.value : getName(node);\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet =>\n  node.selectionSet !== undefined ? node.selectionSet.selections : [];\n\nexport const getTypeCondition = ({\n  typeCondition,\n}: {\n  typeCondition?: NamedTypeNode;\n}): string | null =>\n  typeCondition !== undefined ? getName(typeCondition) : null;\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n\nexport const unwrapType = (\n  type: null | undefined | GraphQLOutputType\n): GraphQLFlatType | null => {\n  if (isWrappingType(type)) {\n    return unwrapType(type.ofType);\n  }\n\n  return type || null;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/help.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\nimport { ErrorCode } from '../types';\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\nconst helpUrl =\n  '\\nhttps://github.com/FormidableLabs/urql-exchange-graphcache/blob/master/docs/help.md#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import { stringifyVariables } from 'urql/core';\nimport { Variables, FieldInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: null | Variables) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\n/** Prefix key with its owner type Link / Record */\nexport const prefixKey = (owner: 'l' | 'r', key: string) => `${owner}|${key}`;\n","export const defer: (fn: () => void) => void =\n  process.env.NODE_ENV === 'production' && typeof Promise !== 'undefined'\n    ? Promise.prototype.then.bind(Promise.resolve())\n    : fn => setTimeout(fn, 0);\n","import {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n} from '../types';\nimport { invariant, currentDebugStack } from '../helpers/help';\nimport { fieldInfoOfKey, joinKeys, prefixKey } from './keys';\nimport { defer } from './timing';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OptimisticMap<T> = Record<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OptimisticMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n  keys: number[];\n}\n\nexport interface InMemoryData {\n  persistenceScheduled: boolean;\n  persistenceBatch: SerializedEntries;\n  gcScheduled: boolean;\n  gcBatch: Set<string>;\n  queryRootKey: string;\n  refCount: Dict<number>;\n  refLock: OptimisticMap<Dict<number>>;\n  records: NodeMap<EntityField>;\n  links: NodeMap<Link>;\n  storage: StorageAdapter | null;\n}\n\nexport const makeDict = (): any => Object.create(null);\n\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Set<string> = null;\nlet currentOptimisticKey: null | number = null;\n\nconst makeNodeMap = <T>(): NodeMap<T> => ({\n  optimistic: makeDict(),\n  base: new Map(),\n  keys: [],\n});\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  data: InMemoryData,\n  optimisticKey: number | null\n) => {\n  currentData = data;\n  currentDependencies = new Set();\n  currentOptimisticKey = optimisticKey;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  const data = currentData!;\n\n  if (!data.gcScheduled && data.gcBatch.size > 0) {\n    data.gcScheduled = true;\n    defer(() => {\n      gc(data);\n    });\n  }\n\n  if (data.storage && !data.persistenceScheduled) {\n    data.persistenceScheduled = true;\n    defer(() => {\n      data.storage!.write(data.persistenceBatch);\n      data.persistenceScheduled = false;\n      data.persistenceBatch = makeDict();\n    });\n  }\n\n  currentData = null;\n  currentDependencies = null;\n  currentOptimisticKey = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Set<string> => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  persistenceScheduled: false,\n  persistenceBatch: makeDict(),\n  gcScheduled: false,\n  queryRootKey,\n  gcBatch: new Set(),\n  refCount: makeDict(),\n  refLock: makeDict(),\n  links: makeNodeMap(),\n  records: makeNodeMap(),\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  let keymap: KeyMap<Dict<T | undefined>>;\n  if (currentOptimisticKey) {\n    // If the optimistic map doesn't exist yet, it' created, and\n    // the optimistic key is stored (in order of priority)\n    if (map.optimistic[currentOptimisticKey] === undefined) {\n      map.optimistic[currentOptimisticKey] = new Map();\n      map.keys.unshift(currentOptimisticKey);\n    }\n\n    keymap = map.optimistic[currentOptimisticKey];\n  } else {\n    keymap = map.base;\n  }\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    const node = optimistic.get(entityKey);\n    // If the node and node value exists it is returned, including undefined\n    if (node !== undefined && fieldKey in node) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  const node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Clears an optimistic layers from a NodeMap */\nconst clearOptimisticNodes = <T>(map: NodeMap<T>, optimisticKey: number) => {\n  // Check whether the optimistic layer exists on the NodeMap\n  const index = map.keys.indexOf(optimisticKey);\n  if (index > -1) {\n    // Then delete it and splice out the optimisticKey\n    delete map.optimistic[optimisticKey];\n    map.keys.splice(index, 1);\n  }\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gcBatch */\nconst updateRCForEntity = (\n  gcBatch: void | Set<string>,\n  refCount: Dict<number>,\n  entityKey: string,\n  by: number\n) => {\n  // Retrieve the reference count\n  const count = refCount[entityKey] !== undefined ? refCount[entityKey] : 0;\n  // Adjust it by the \"by\" value\n  const newCount = (refCount[entityKey] = (count + by) | 0);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gcBatch !== undefined) {\n    if (newCount <= 0) gcBatch.add(entityKey);\n    else if (count <= 0 && newCount > 0) gcBatch.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gcBatch */\nconst updateRCForLink = (\n  gcBatch: void | Set<string>,\n  refCount: Dict<number>,\n  link: Link | undefined,\n  by: number\n) => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gcBatch, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      const entityKey = link[i];\n      if (entityKey) {\n        updateRCForEntity(gcBatch, refCount, entityKey, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n) => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = (data: InMemoryData) => {\n  // Reset gcScheduled flag\n  data.gcScheduled = false;\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  data.gcBatch.forEach(entityKey => {\n    // Check first whether the reference count is still 0\n    const rc = data.refCount[entityKey] || 0;\n    if (rc <= 0) {\n      // Each optimistic layer may also still contain some references to marked entities\n      for (const optimisticKey in data.refLock) {\n        const refCount = data.refLock[optimisticKey];\n        const locks = refCount[entityKey] || 0;\n        // If the optimistic layer has any references to the entity, don't GC it,\n        // otherwise delete the reference count from the optimistic layer\n        if (locks > 0) return;\n        delete refCount[entityKey];\n      }\n\n      // All conditions are met: The entity can be deleted\n\n      // Delete the reference count, and delete the entity from the GC batch\n      delete data.refCount[entityKey];\n      data.gcBatch.delete(entityKey);\n\n      // Delete the record and for each of its fields, delete them on the persistence\n      // layer if one is present\n      // No optimistic data needs to be deleted, as the entity is not being referenced by\n      // anything and optimistic layers will eventually be deleted anyway\n      const recordsNode = data.records.base.get(entityKey);\n      if (recordsNode !== undefined) {\n        data.records.base.delete(entityKey);\n        if (data.storage) {\n          for (const fieldKey in recordsNode) {\n            const key = prefixKey('r', joinKeys(entityKey, fieldKey));\n            data.persistenceBatch[key] = undefined;\n          }\n        }\n      }\n\n      // Delete all the entity's links, but also update the reference count\n      // for those links (which can lead to an unrolled recursive GC of the children)\n      const linkNode = data.links.base.get(entityKey);\n      if (linkNode !== undefined) {\n        data.links.base.delete(entityKey);\n        for (const fieldKey in linkNode) {\n          // Delete all links from the persistence layer if one is present\n          if (data.storage) {\n            const key = prefixKey('l', joinKeys(entityKey, fieldKey));\n            data.persistenceBatch[key] = undefined;\n          }\n\n          updateRCForLink(data.gcBatch, data.refCount, linkNode[fieldKey], -1);\n        }\n      }\n    } else {\n      data.gcBatch.delete(entityKey);\n    }\n  });\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies!.add(entityKey);\n    } else if (fieldKey !== undefined) {\n      currentDependencies!.add(joinKeys(entityKey, fieldKey));\n    }\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n  if (currentData!.storage && !currentOptimisticKey) {\n    const key = prefixKey('r', joinKeys(entityKey, fieldKey));\n    currentData!.persistenceBatch[key] = value;\n  }\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: Dict<number>;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gcBatch: void | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount =\n      data.refLock[currentOptimisticKey] ||\n      (data.refLock[currentOptimisticKey] = makeDict());\n    links = data.links.optimistic[currentOptimisticKey];\n  } else {\n    if (data.storage) {\n      const key = prefixKey('l', joinKeys(entityKey, fieldKey));\n      data.persistenceBatch[key] = link;\n    }\n    refCount = data.refCount;\n    links = data.links.base;\n    gcBatch = data.gcBatch;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links !== undefined ? links.get(entityKey) : undefined;\n  const prevLink = prevLinkNode !== undefined ? prevLinkNode[fieldKey] : null;\n\n  // Update dependencies\n  updateDependencies(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gcBatch, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gcBatch, refCount, link, 1);\n};\n\n/** Removes an optimistic layer of links and records */\nexport const clearOptimistic = (data: InMemoryData, optimisticKey: number) => {\n  // We also delete the optimistic reference locks\n  delete data.refLock[optimisticKey];\n  clearOptimisticNodes(data.records, optimisticKey);\n  clearOptimisticNodes(data.links, optimisticKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState(data, 0);\n  for (const key in entries) {\n    const dotIndex = key.indexOf('.');\n    const entityKey = key.slice(2, dotIndex);\n    const fieldKey = key.slice(dotIndex + 1);\n    switch (key.charCodeAt(0)) {\n      case 108:\n        writeLink(entityKey, fieldKey, entries[key] as Link);\n        break;\n      case 114:\n        writeRecord(entityKey, fieldKey, entries[key]);\n        break;\n    }\n  }\n  clearDataState();\n  data.storage = storage;\n};\n","import { FieldNode, InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, pushDebugNode } from '../helpers/help';\nimport { hasField } from '../store/data';\nimport { Store, keyOfField } from '../store';\n\nimport {\n  Fragments,\n  Variables,\n  SelectionSet,\n  DataField,\n  NullArray,\n  Data,\n} from '../types';\n\nimport {\n  SchemaPredicates,\n  getTypeCondition,\n  getFieldArguments,\n  shouldInclude,\n  isFieldNode,\n  isInlineFragment,\n  getSelectionSet,\n  getName,\n} from '../ast';\n\ninterface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  ctx: Context\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return !getSelectionSet(node).some(node => {\n    if (!isFieldNode(node)) return false;\n    const fieldKey = keyOfField(\n      getName(node),\n      getFieldArguments(node, ctx.variables)\n    );\n    return !hasField(entityKey, fieldKey);\n  });\n};\n\nexport class SelectionIterator {\n  typename: void | string;\n  entityKey: string;\n  indexStack: number[];\n  context: Context;\n  selectionStack: SelectionSet[];\n\n  constructor(\n    typename: void | string,\n    entityKey: string,\n    select: SelectionSet,\n    ctx: Context\n  ) {\n    this.typename = typename;\n    this.entityKey = entityKey;\n    this.context = ctx;\n    this.indexStack = [0];\n    this.selectionStack = [select];\n  }\n\n  next(): void | FieldNode {\n    while (this.indexStack.length !== 0) {\n      const index = this.indexStack[this.indexStack.length - 1]++;\n      const select = this.selectionStack[this.selectionStack.length - 1];\n      if (index >= select.length) {\n        this.indexStack.pop();\n        this.selectionStack.pop();\n        continue;\n      } else {\n        const node = select[index];\n        if (!shouldInclude(node, this.context.variables)) {\n          continue;\n        } else if (!isFieldNode(node)) {\n          // A fragment is either referred to by FragmentSpread or inline\n          const fragmentNode = !isInlineFragment(node)\n            ? this.context.fragments[getName(node)]\n            : node;\n\n          if (fragmentNode !== undefined) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(this.typename, fragmentNode);\n            }\n\n            const isMatching =\n              this.context.schemaPredicates !== undefined\n                ? this.context.schemaPredicates.isInterfaceOfType(\n                    getTypeCondition(fragmentNode),\n                    this.typename\n                  )\n                : isFragmentHeuristicallyMatching(\n                    fragmentNode,\n                    this.typename,\n                    this.entityKey,\n                    this.context\n                  );\n\n            if (isMatching) {\n              this.indexStack.push(0);\n              this.selectionStack.push(getSelectionSet(fragmentNode));\n            }\n          }\n\n          continue;\n        } else if (getName(node) === '__typename') {\n          continue;\n        } else {\n          return node;\n        }\n      }\n    }\n\n    return undefined;\n  }\n}\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x === undefined ? null : (x as Data | NullArray<Data>);\n","import {\n  SelectionNode,\n  DefinitionNode,\n  DocumentNode,\n  FragmentDefinitionNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { invariant } from '../helpers/help';\nimport { getName } from './node';\nimport { Fragments, Variables } from '../types';\n\nconst isFragmentNode = (node: DefinitionNode): node is FragmentDefinitionNode =>\n  node.kind === Kind.FRAGMENT_DEFINITION;\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  const operation = doc.definitions.find(\n    node => node.kind === Kind.OPERATION_DEFINITION\n  ) as OperationDefinitionNode;\n\n  invariant(\n    !!operation,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n\n  return operation;\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments =>\n  doc.definitions.filter(isFragmentNode).reduce((map: Fragments, node) => {\n    map[getName(node)] = node;\n    return map;\n  }, {});\n\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  const { directives } = node;\n  if (directives === undefined) {\n    return true;\n  }\n\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0, l = directives.length; i < l; i++) {\n    const directive = directives[i];\n    const name = getName(directive);\n\n    // Ignore other directives\n    const isInclude = name === 'include';\n    if (!isInclude && name !== 'skip') continue;\n\n    // Get the first argument and expect it to be named \"if\"\n    const arg = directive.arguments ? directive.arguments[0] : null;\n    if (!arg || getName(arg) !== 'if') continue;\n\n    const value = valueFromASTUntyped(arg.value, vars);\n    if (typeof value !== 'boolean' && value !== null) continue;\n\n    // Return whether this directive forces us to skip\n    // `@include(if: false)` or `@skip(if: true)`\n    return isInclude ? !!value : !value;\n  }\n\n  return true;\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  getSelectionSet,\n  normalizeVariables,\n  getFragmentTypeName,\n  getName,\n  getFieldArguments,\n  SchemaPredicates,\n} from '../ast';\n\nimport {\n  NullArray,\n  Fragments,\n  Variables,\n  Data,\n  Link,\n  SelectionSet,\n  OperationRequest,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  makeDict,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { invariant, warn, pushDebugNode } from '../helpers/help';\nimport { SelectionIterator, ensureData } from './shared';\n\nexport interface WriteResult {\n  dependencies: Set<string>;\n}\n\ninterface Context {\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  result: WriteResult;\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  optimistic?: boolean;\n  schemaPredicates?: SchemaPredicates;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n): WriteResult => {\n  initDataState(store.data, 0);\n  const result = startWrite(store, request, data);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { dependencies: getCurrentDependencies() };\n\n  const select = getSelectionSet(operation);\n  const operationName = store.getRootKey(operation.operation);\n\n  const ctx: Context = {\n    parentTypeName: operationName,\n    parentKey: operationName,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    result,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  if (operationName === ctx.store.getRootKey('query')) {\n    writeSelection(ctx, operationName, select, data);\n  } else {\n    writeRoot(ctx, operationName, select, data);\n  }\n\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  optimisticKey: number\n): WriteResult => {\n  initDataState(store.data, optimisticKey);\n\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { dependencies: getCurrentDependencies() };\n\n  const mutationRootKey = store.getRootKey('mutation');\n  const operationName = store.getRootKey(operation.operation);\n  invariant(\n    operationName === mutationRootKey,\n    'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n      'This case is unsupported and should never occur.',\n    10\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  const ctx: Context = {\n    parentTypeName: mutationRootKey,\n    parentKey: mutationRootKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    result,\n    store,\n    schemaPredicates: store.schemaPredicates,\n    optimistic: true,\n  };\n\n  const data = makeDict();\n  const iter = new SelectionIterator(\n    operationName,\n    operationName,\n    getSelectionSet(operation),\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    if (node.selectionSet !== undefined) {\n      const fieldName = getName(node);\n      const resolver = ctx.store.optimisticMutations[fieldName];\n\n      if (resolver !== undefined) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        ctx.fieldName = fieldName;\n\n        const fieldArgs = getFieldArguments(node, ctx.variables);\n        const resolverValue = resolver(fieldArgs || makeDict(), ctx.store, ctx);\n        const resolverData = ensureData(resolverValue);\n        writeRootField(ctx, resolverData, getSelectionSet(node));\n        data[fieldName] = resolverValue;\n        const updater = ctx.store.updates[mutationRootKey][fieldName];\n        if (updater !== undefined) {\n          updater(data, fieldArgs || makeDict(), ctx.store, ctx);\n        }\n      }\n    }\n  }\n\n  clearDataState();\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Data,\n  variables?: Variables\n) => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    return warn(\n      'writeFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      11\n    );\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const writeData = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(writeData);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx: Context = {\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: variables || {},\n    fragments,\n    result: { dependencies: getCurrentDependencies() },\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), writeData);\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.getRootKey('query');\n  const typename = isQuery ? entityKey : data.__typename;\n  if (typeof typename !== 'string') return;\n\n  InMemoryData.writeRecord(entityKey, '__typename', typename);\n\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldValue = data[getFieldAlias(node)];\n    const key = joinKeys(entityKey, fieldKey);\n\n    if (process.env.NODE_ENV !== 'production') {\n      if (fieldValue === undefined) {\n        const advice = ctx.optimistic\n          ? '\\nYour optimistic result may be missing a field!'\n          : '';\n\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.' +\n            advice,\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.schemaPredicates && typename) {\n        ctx.schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n      }\n    }\n\n    if (node.selectionSet === undefined) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(entityKey, fieldKey, fieldValue);\n    } else {\n      // Process the field and write links for the child entities that have been written\n      const fieldData = ensureData(fieldValue);\n      const link = writeField(ctx, key, getSelectionSet(node), fieldData);\n      InMemoryData.writeLink(entityKey, fieldKey, link);\n    }\n  }\n};\n\nconst writeField = (\n  ctx: Context,\n  parentFieldKey: string,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>\n): Link => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      const item = data[i];\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = joinKeys(parentFieldKey, `${i}`);\n      // Recursively write array data\n      const links = writeField(ctx, indexKey, select, item);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n    }\n\n    return newData;\n  } else if (data === null) {\n    return null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const key = entityKey !== null ? entityKey : parentFieldKey;\n  const typename = data.__typename;\n\n  if (\n    ctx.store.keys[data.__typename] === undefined &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !typename.endsWith('Connection') &&\n    !typename.endsWith('Edge') &&\n    typename !== 'PageInfo'\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  writeSelection(ctx, key, select, data);\n  return key;\n};\n\n// This is like writeSelection but assumes no parent entity exists\nconst writeRoot = (\n  ctx: Context,\n  typename: string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isRootField =\n    typename === ctx.store.getRootKey('mutation') ||\n    typename === ctx.store.getRootKey('subscription');\n\n  const iter = new SelectionIterator(typename, typename, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = joinKeys(typename, keyOfField(fieldName, fieldArgs));\n    if (node.selectionSet !== undefined) {\n      const fieldValue = ensureData(data[getFieldAlias(node)]);\n      writeRootField(ctx, fieldValue, getSelectionSet(node));\n    }\n\n    if (isRootField) {\n      // We have to update the context to reflect up-to-date ResolveInfo\n      ctx.parentTypeName = typename;\n      ctx.parentKey = typename;\n      ctx.parentFieldKey = fieldKey;\n      ctx.fieldName = fieldName;\n\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater !== undefined) {\n        updater(data, fieldArgs || makeDict(), ctx.store, ctx);\n      }\n    }\n  }\n};\n\n// This is like writeField but doesn't fall back to a generated key\nconst writeRootField = (\n  ctx: Context,\n  data: null | Data | NullArray<Data>,\n  select: SelectionSet\n) => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++)\n      newData[i] = writeRootField(ctx, data[i], select);\n    return newData;\n  } else if (data === null) {\n    return;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(data);\n  if (entityKey !== null) {\n    writeSelection(ctx, entityKey, select, data);\n  } else {\n    const typename = data.__typename;\n    writeRoot(ctx, typename, select, data);\n  }\n};\n","import { DocumentNode } from 'graphql';\nimport { createRequest } from 'urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  Data,\n  QueryInput,\n  UpdatesConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n} from '../types';\n\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidate } from '../operations/invalidate';\nimport { SchemaPredicates } from '../ast';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport class Store implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: UpdatesConfig;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schemaPredicates?: SchemaPredicates;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(\n    schemaPredicates?: SchemaPredicates,\n    resolvers?: ResolverConfig,\n    updates?: Partial<UpdatesConfig>,\n    optimisticMutations?: OptimisticMutationConfig,\n    keys?: KeyingConfig\n  ) {\n    this.resolvers = resolvers || {};\n    this.optimisticMutations = optimisticMutations || {};\n    this.keys = keys || {};\n    this.schemaPredicates = schemaPredicates;\n\n    this.updates = {\n      Mutation: (updates && updates.Mutation) || {},\n      Subscription: (updates && updates.Subscription) || {},\n    } as UpdatesConfig;\n\n    if (schemaPredicates) {\n      const { schema } = schemaPredicates;\n      const queryType = schema.getQueryType();\n      const mutationType = schema.getMutationType();\n      const subscriptionType = schema.getSubscriptionType();\n\n      const queryName = queryType ? queryType.name : 'Query';\n      const mutationName = mutationType ? mutationType.name : 'Mutation';\n      const subscriptionName = subscriptionType\n        ? subscriptionType.name\n        : 'Subscription';\n\n      this.rootFields = {\n        query: queryName,\n        mutation: mutationName,\n        subscription: subscriptionName,\n      };\n\n      this.rootNames = {\n        [queryName]: 'query',\n        [mutationName]: 'mutation',\n        [subscriptionName]: 'subscription',\n      };\n    } else {\n      this.rootFields = {\n        query: 'Query',\n        mutation: 'Mutation',\n        subscription: 'Subscription',\n      };\n\n      this.rootNames = {\n        Query: 'query',\n        Mutation: 'mutation',\n        Subscription: 'subscription',\n      };\n    }\n\n    this.data = InMemoryData.make(this.getRootKey('query'));\n  }\n\n  gcScheduled = false;\n  gc = () => {\n    InMemoryData.gc(this.data);\n    this.gcScheduled = false;\n  };\n\n  keyOfField = keyOfField;\n\n  getRootKey(name: RootField) {\n    return this.rootFields[name];\n  }\n\n  keyOfEntity(data: Data) {\n    const { __typename: typename, id, _id } = data;\n    if (!typename) {\n      return null;\n    } else if (this.rootNames[typename] !== undefined) {\n      return typename;\n    }\n\n    let key: string | null | void;\n    if (this.keys[typename]) {\n      key = this.keys[typename](data);\n    } else if (id !== undefined && id !== null) {\n      key = `${id}`;\n    } else if (_id !== undefined && _id !== null) {\n      key = `${_id}`;\n    }\n\n    return key ? `${typename}:${key}` : null;\n  }\n\n  resolveFieldByKey(entity: Data | string | null, fieldKey: string): DataField {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    if (entityKey === null) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link ? link : null;\n  }\n\n  resolve(\n    entity: Data | string | null,\n    field: string,\n    args?: Variables\n  ): DataField {\n    return this.resolveFieldByKey(entity, keyOfField(field, args));\n  }\n\n  invalidateQuery(query: string | DocumentNode, variables?: Variables) {\n    invalidate(this, createRequest(query, variables));\n  }\n\n  inspectFields(entity: Data | string | null): FieldInfo[] {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    return entityKey !== null ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery(\n    input: QueryInput,\n    updater: (data: Data | null) => Data | null\n  ): void {\n    const request = createRequest(input.query, input.variables);\n    const output = updater(this.readQuery(request as QueryInput));\n    if (output !== null) {\n      startWrite(this, request, output);\n    }\n  }\n\n  readQuery(input: QueryInput): Data | null {\n    return read(this, createRequest(input.query, input.variables)).data;\n  }\n\n  readFragment(\n    dataFragment: DocumentNode,\n    entity: string | Data,\n    variables?: Variables\n  ): Data | null {\n    return readFragment(this, dataFragment, entity, variables);\n  }\n\n  writeFragment(\n    dataFragment: DocumentNode,\n    data: Data,\n    variables?: Variables\n  ): void {\n    writeFragment(this, dataFragment, data, variables);\n  }\n}\n","import { FieldNode } from 'graphql';\n\nimport {\n  getMainOperation,\n  normalizeVariables,\n  getFragments,\n  getSelectionSet,\n  getName,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  EntityField,\n  OperationRequest,\n  Variables,\n  Fragments,\n  SelectionSet,\n} from '../types';\n\nimport * as InMemoryData from '../store/data';\nimport { Store, keyOfField } from '../store';\nimport { SchemaPredicates } from '../ast';\nimport { SelectionIterator } from './shared';\n\ninterface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nexport const invalidate = (store: Store, request: OperationRequest) => {\n  const operation = getMainOperation(request.query);\n\n  const ctx: Context = {\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  invalidateSelection(\n    ctx,\n    ctx.store.getRootKey('query'),\n    getSelectionSet(operation)\n  );\n};\n\nexport const invalidateSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet\n) => {\n  const isQuery = entityKey === 'Query';\n\n  let typename: EntityField;\n  if (!isQuery) {\n    typename = InMemoryData.readRecord(entityKey, '__typename');\n    if (typeof typename !== 'string') {\n      return;\n    } else {\n      InMemoryData.writeRecord(entityKey, '__typename', undefined);\n    }\n  } else {\n    typename = entityKey;\n  }\n\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldKey = keyOfField(\n      fieldName,\n      getFieldArguments(node, ctx.variables)\n    );\n\n    if (\n      process.env.NODE_ENV !== 'production' &&\n      ctx.schemaPredicates &&\n      typename\n    ) {\n      ctx.schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    if (node.selectionSet === undefined) {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    } else {\n      const fieldSelect = getSelectionSet(node);\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n\n      if (Array.isArray(link)) {\n        for (let i = 0, l = link.length; i < l; i++) {\n          const childLink = link[i];\n          if (childLink !== null) {\n            invalidateSelection(ctx, childLink, fieldSelect);\n          }\n        }\n      } else if (link) {\n        invalidateSelection(ctx, link, fieldSelect);\n      }\n    }\n  }\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  getFragments,\n  getMainOperation,\n  getSelectionSet,\n  normalizeVariables,\n  getName,\n  getFieldArguments,\n  getFieldAlias,\n  getFragmentTypeName,\n} from '../ast';\n\nimport {\n  Fragments,\n  Variables,\n  Data,\n  DataField,\n  Link,\n  SelectionSet,\n  OperationRequest,\n  NullArray,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  makeDict,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode } from '../helpers/help';\nimport { SelectionIterator, ensureData } from './shared';\nimport { SchemaPredicates } from '../ast';\n\nexport interface QueryResult {\n  dependencies: Set<string>;\n  partial: boolean;\n  data: null | Data;\n}\n\ninterface Context {\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  partial: boolean;\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data\n): QueryResult => {\n  initDataState(store.data, 0);\n  const result = read(store, request, data);\n  clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.getRootKey(operation.operation);\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx: Context = {\n    parentTypeName: rootKey,\n    parentKey: rootKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    partial: false,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  let data = input || makeDict();\n  data =\n    rootKey !== ctx.store.getRootKey('query')\n      ? readRoot(ctx, rootKey, rootSelect, data)\n      : readSelection(ctx, rootKey, rootSelect, data);\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: data === undefined ? false : ctx.partial,\n    data: data === undefined ? null : data,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  originalData: Data\n): Data => {\n  if (typeof originalData.__typename !== 'string') {\n    return originalData;\n  }\n\n  const iter = new SelectionIterator(entityKey, entityKey, select, ctx);\n  const data = makeDict();\n  data.__typename = originalData.__typename;\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = originalData[fieldAlias];\n    if (node.selectionSet !== undefined && fieldValue !== null) {\n      const fieldData = ensureData(fieldValue);\n      data[fieldAlias] = readRootField(ctx, getSelectionSet(node), fieldData);\n    } else {\n      data[fieldAlias] = fieldValue;\n    }\n  }\n\n  return data;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: null | Data | NullArray<Data>\n): Data | NullArray<Data> | null => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    for (let i = 0, l = originalData.length; i < l; i++)\n      newData[i] = readRootField(ctx, select, originalData[i]);\n    return newData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    const fieldValue = readSelection(ctx, entityKey, select, makeDict());\n    return fieldValue === undefined ? null : fieldValue;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Data | string,\n  variables?: Variables\n): Data | null => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    warn(\n      'readFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      6\n    );\n\n    return null;\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename) {\n    entity.__typename = typename;\n  }\n\n  const entityKey =\n    typeof entity !== 'string'\n      ? store.keyOfEntity({ __typename: typename, ...entity } as Data)\n      : entity;\n\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx: Context = {\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: variables || {},\n    fragments,\n    partial: false,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  return (\n    readSelection(ctx, entityKey, getSelectionSet(fragment), makeDict()) || null\n  );\n};\n\nconst readSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  data: Data\n): Data | undefined => {\n  const { store, schemaPredicates } = ctx;\n  const isQuery = entityKey === store.getRootKey('query');\n\n  // Get the __typename field for a given entity to check that it exists\n  const typename = !isQuery\n    ? InMemoryData.readRecord(entityKey, '__typename')\n    : entityKey;\n  if (typeof typename !== 'string') {\n    return undefined;\n  }\n\n  data.__typename = typename;\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iter.next()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const key = joinKeys(entityKey, fieldKey);\n\n    if (process.env.NODE_ENV !== 'production' && schemaPredicates && typename) {\n      schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n\n    const resolvers = store.resolvers[typename];\n    if (resolvers !== undefined && typeof resolvers[fieldName] === 'function') {\n      // We have to update the information in context to reflect the info\n      // that the resolver will receive\n      ctx.parentTypeName = typename;\n      ctx.parentKey = entityKey;\n      ctx.parentFieldKey = key;\n      ctx.fieldName = fieldName;\n\n      // We have a resolver for this field.\n      // Prepare the actual fieldValue, so that the resolver can use it\n      if (fieldValue !== undefined) {\n        data[fieldAlias] = fieldValue;\n      }\n\n      dataFieldValue = resolvers[fieldName](\n        data,\n        fieldArgs || makeDict(),\n        store,\n        ctx\n      );\n\n      if (node.selectionSet !== undefined) {\n        // When it has a selection set we are resolving an entity with a\n        // subselection. This can either be a list or an object.\n        dataFieldValue = resolveResolverResult(\n          ctx,\n          typename,\n          fieldName,\n          key,\n          getSelectionSet(node),\n          (data[fieldAlias] as Data) || makeDict(),\n          dataFieldValue\n        );\n      }\n\n      if (\n        schemaPredicates !== undefined &&\n        dataFieldValue === null &&\n        !schemaPredicates.isFieldNullable(typename, fieldName)\n      ) {\n        // Special case for when null is not a valid value for the\n        // current field\n        return undefined;\n      }\n    } else if (node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly\n      dataFieldValue = fieldValue;\n    } else {\n      // We have a selection set which means that we'll be checking for links\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      schemaPredicates !== undefined &&\n      schemaPredicates.isFieldNullable(typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return isQuery && hasPartials && !hasFields ? undefined : data;\n};\n\nconst readResolverResult = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  data: Data,\n  result: Data\n): Data | undefined => {\n  const { store, schemaPredicates } = ctx;\n  const entityKey = store.keyOfEntity(result) || key;\n  const resolvedTypename = result.__typename;\n  const typename =\n    InMemoryData.readRecord(entityKey, '__typename') || resolvedTypename;\n\n  if (\n    typeof typename !== 'string' ||\n    (resolvedTypename && typename !== resolvedTypename)\n  ) {\n    // TODO: This may be an invalid error for resolvers that return interfaces\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return undefined;\n  }\n\n  // The following closely mirrors readSelection, but differs only slightly for the\n  // sake of resolving from an existing resolver result\n  data.__typename = typename;\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iter.next()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(\n      fieldName,\n      getFieldArguments(node, ctx.variables)\n    );\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result[fieldName];\n\n    if (process.env.NODE_ENV !== 'production' && schemaPredicates && typename) {\n      schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (node.selectionSet === undefined) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        data[fieldAlias] as Data,\n        resultValue\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      schemaPredicates !== undefined &&\n      schemaPredicates.isFieldNullable(typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return !hasFields ? undefined : data;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | Data | Data[],\n  result: void | DataField\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { schemaPredicates } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const isListNullable =\n      schemaPredicates === undefined ||\n      schemaPredicates.isListNullable(typename, fieldName);\n    const data = new Array(result.length);\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        // Get the inner previous data from prevData\n        prevData !== undefined ? prevData[i] : undefined,\n        result[i]\n      );\n\n      if (childResult === undefined && !isListNullable) {\n        return undefined;\n      } else {\n        data[i] = childResult !== undefined ? childResult : null;\n      }\n    }\n\n    return data;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (isDataOrKey(result)) {\n    const data = prevData === undefined ? makeDict() : prevData;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readResolverResult(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | Data | Data[]\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { schemaPredicates } = ctx;\n    const isListNullable =\n      schemaPredicates !== undefined &&\n      schemaPredicates.isListNullable(typename, fieldName);\n    const newLink = new Array(link.length);\n    for (let i = 0, l = link.length; i < l; i++) {\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData !== undefined ? prevData[i] : undefined\n      );\n      if (childLink === undefined && !isListNullable) {\n        return undefined;\n      } else {\n        newLink[i] = childLink !== undefined ? childLink : null;\n      }\n    }\n\n    return newLink;\n  } else if (link === null) {\n    return null;\n  } else {\n    return readSelection(\n      ctx,\n      link,\n      select,\n      prevData === undefined ? makeDict() : prevData\n    );\n  }\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\nimport { makeDict } from '../store';\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  if (node.arguments === undefined || node.arguments.length === 0) {\n    return null;\n  }\n\n  const args = makeDict();\n  let argsSize = 0;\n\n  for (let i = 0, l = node.arguments.length; i < l; i++) {\n    const arg = node.arguments[i];\n    const value = valueFromASTUntyped(arg.value, vars);\n    if (value !== undefined && value !== null) {\n      args[getName(arg)] = value;\n      argsSize++;\n    }\n  }\n\n  return argsSize > 0 ? args : null;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n): Variables => {\n  if (node.variableDefinitions === undefined) {\n    return {};\n  }\n\n  const args: Variables = (input as Variables) || {};\n\n  return node.variableDefinitions.reduce((vars, def) => {\n    const name = getName(def.variable);\n    let value = args[name];\n    if (value === undefined) {\n      if (def.defaultValue !== undefined) {\n        value = valueFromASTUntyped(def.defaultValue, args);\n      } else {\n        return vars;\n      }\n    }\n\n    vars[name] = value;\n    return vars;\n  }, makeDict());\n};\n","import {\n  buildClientSchema,\n  isNullableType,\n  isListType,\n  isNonNullType,\n  GraphQLSchema,\n  GraphQLAbstractType,\n  GraphQLObjectType,\n  GraphQLInterfaceType,\n  GraphQLUnionType,\n} from 'graphql';\n\nimport { invariant, warn } from '../helpers/help';\n\nexport class SchemaPredicates {\n  schema: GraphQLSchema;\n\n  constructor(schema: object) {\n    this.schema = buildClientSchema(schema as any);\n  }\n\n  isFieldNullable(typename: string, fieldName: string): boolean {\n    const field = getField(this.schema, typename, fieldName);\n    if (field === undefined) return false;\n    return isNullableType(field.type);\n  }\n\n  isListNullable(typename: string, fieldName: string): boolean {\n    const field = getField(this.schema, typename, fieldName);\n    if (field === undefined) return false;\n    const ofType = isNonNullType(field.type) ? field.type.ofType : field.type;\n    return isListType(ofType) && isNullableType(ofType.ofType);\n  }\n\n  isFieldAvailableOnType(typename: string, fieldname: string): boolean {\n    return !!getField(this.schema, typename, fieldname);\n  }\n\n  isInterfaceOfType(\n    typeCondition: null | string,\n    typename: string | void\n  ): boolean {\n    if (!typename || !typeCondition) return false;\n    if (typename === typeCondition) return true;\n\n    const abstractType = this.schema.getType(typeCondition);\n    const objectType = this.schema.getType(typename);\n\n    if (abstractType instanceof GraphQLObjectType) {\n      return abstractType === objectType;\n    }\n\n    expectAbstractType(abstractType, typeCondition);\n    expectObjectType(objectType, typename);\n    return this.schema.isPossibleType(abstractType, objectType);\n  }\n}\n\nconst getField = (\n  schema: GraphQLSchema,\n  typename: string,\n  fieldName: string\n) => {\n  const object = schema.getType(typename);\n  expectObjectType(object, typename);\n\n  const field = object.getFields()[fieldName];\n  if (field === undefined) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n\n    return undefined;\n  }\n\n  return field;\n};\n\nfunction expectObjectType(x: any, typename: string): asserts x is GraphQLObjectType {\n  invariant(\n    x instanceof GraphQLObjectType,\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(x: any, typename: string): asserts x is GraphQLAbstractType {\n  invariant(\n    x instanceof GraphQLInterfaceType || x instanceof GraphQLUnionType,\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n","import { IntrospectionQuery } from 'graphql';\n\nimport {\n  Exchange,\n  formatDocument,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from 'urql/core';\n\nimport {\n  filter,\n  map,\n  merge,\n  pipe,\n  share,\n  tap,\n  fromPromise,\n  fromArray,\n  buffer,\n  take,\n  mergeMap,\n  concat,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { SchemaPredicates } from './ast';\nimport { hydrateData } from './store/data';\nimport { makeDict, Store, clearOptimistic } from './store';\n\nimport {\n  UpdatesConfig,\n  ResolverConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  StorageAdapter,\n} from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n};\n\ntype OperationMap = Map<number, Operation>;\n\ninterface DependentOperations {\n  [key: string]: number[];\n}\n\n// Returns the given operation result with added cacheOutcome meta field\nconst addCacheOutcome = (op: Operation, outcome: CacheOutcome): Operation => ({\n  ...op,\n  context: {\n    ...op.context,\n    meta: {\n      ...op.context.meta,\n      cacheOutcome: outcome,\n    },\n  },\n});\n\n// Returns the given operation with added __typename fields on its query\nconst addTypeNames = (op: Operation): Operation => ({\n  ...op,\n  query: formatDocument(op.query),\n});\n\n// Retrieves the requestPolicy from an operation\nconst getRequestPolicy = (op: Operation) => op.context.requestPolicy;\n\n// Returns whether an operation is a query\nconst isQueryOperation = (op: Operation): boolean =>\n  op.operationName === 'query';\n\n// Returns whether an operation is a mutation\nconst isMutationOperation = (op: Operation): boolean =>\n  op.operationName === 'mutation';\n\n// Returns whether an operation can potentially be read from cache\nconst isCacheableQuery = (op: Operation): boolean => {\n  return isQueryOperation(op) && getRequestPolicy(op) !== 'network-only';\n};\n\n// Returns whether an operation potentially triggers an optimistic update\nconst isOptimisticMutation = (op: Operation): boolean => {\n  return isMutationOperation(op) && getRequestPolicy(op) !== 'network-only';\n};\n\n// Copy an operation and change the requestPolicy to skip the cache\nconst toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => ({\n  ...operation,\n  context: {\n    ...operation.context,\n    requestPolicy,\n  },\n});\n\nexport interface CacheExchangeOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionQuery;\n  storage?: StorageAdapter;\n}\n\nexport const cacheExchange = (opts?: CacheExchangeOpts): Exchange => ({\n  forward,\n  client,\n}) => {\n  if (!opts) opts = {};\n\n  const store = new Store(\n    opts.schema ? new SchemaPredicates(opts.schema) : undefined,\n    opts.resolvers,\n    opts.updates,\n    opts.optimistic,\n    opts.keys\n  );\n\n  let hydration: void | Promise<void>;\n  if (opts.storage) {\n    const storage = opts.storage;\n    hydration = storage.read().then(entries => {\n      hydrateData(store.data, storage, entries);\n    });\n  }\n\n  const optimisticKeys = new Set();\n  const ops: OperationMap = new Map();\n  const deps: DependentOperations = makeDict();\n\n  const collectPendingOperations = (\n    pendingOperations: Set<number>,\n    dependencies: void | Set<string>\n  ) => {\n    if (dependencies !== undefined) {\n      // Collect operations that will be updated due to cache changes\n      dependencies.forEach(dep => {\n        const keys = deps[dep];\n        if (keys !== undefined) {\n          deps[dep] = [];\n          for (let i = 0, l = keys.length; i < l; i++) {\n            pendingOperations.add(keys[i]);\n          }\n        }\n      });\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Set<number>\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    pendingOperations.forEach(key => {\n      if (key !== operation.key) {\n        const op = ops.get(key);\n        if (op !== undefined) {\n          ops.delete(key);\n          client.reexecuteOperation(toRequestPolicy(op, 'cache-first'));\n        }\n      }\n    });\n  };\n\n  // This executes an optimistic update for mutations and registers it if necessary\n  const optimisticUpdate = (operation: Operation) => {\n    if (isOptimisticMutation(operation)) {\n      const { key } = operation;\n      const { dependencies } = writeOptimistic(store, operation, key);\n      if (dependencies.size !== 0) {\n        optimisticKeys.add(key);\n        const pendingOperations = new Set<number>();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Set<string>) => {\n    dependencies.forEach(dep => {\n      const keys = deps[dep] || (deps[dep] = []);\n      keys.push(op.key);\n\n      if (!ops.has(op.key)) {\n        ops.set(\n          op.key,\n          getRequestPolicy(op) === 'network-only'\n            ? toRequestPolicy(op, 'cache-and-network')\n            : op\n        );\n      }\n    });\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const { data, dependencies, partial } = query(store, operation);\n    let cacheOutcome: CacheOutcome;\n\n    if (data === null) {\n      cacheOutcome = 'miss';\n    } else {\n      updateDependencies(operation, dependencies);\n      cacheOutcome =\n        !partial || getRequestPolicy(operation) === 'cache-only'\n          ? 'hit'\n          : 'partial';\n    }\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (result: OperationResult): OperationResult => {\n    const { operation, error, extensions } = result;\n    const isQuery = isQueryOperation(operation);\n    let { data } = result;\n\n    // Clear old optimistic values from the store\n    const { key } = operation;\n    if (optimisticKeys.has(key)) {\n      optimisticKeys.delete(key);\n      clearOptimistic(store.data, key);\n    }\n\n    let writeDependencies: Set<string> | void;\n    let queryDependencies: Set<string> | void;\n    if (data !== null && data !== undefined) {\n      writeDependencies = write(store, operation, data).dependencies;\n\n      if (isQuery) {\n        const queryResult = query(store, operation);\n        data = queryResult.data;\n        queryDependencies = queryResult.dependencies;\n      } else {\n        data = query(store, operation, data).data;\n      }\n    }\n\n    // Collect all write dependencies and query dependencies for queries\n    const pendingOperations = new Set<number>();\n    collectPendingOperations(pendingOperations, writeDependencies);\n    if (isQuery) {\n      collectPendingOperations(pendingOperations, queryDependencies);\n    }\n\n    // Execute all pending operations related to changed dependencies\n    executePendingOperations(result.operation, pendingOperations);\n\n    // Update this operation's dependencies if it's a query\n    if (isQuery && queryDependencies !== undefined) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Buffer operations while waiting on hydration to finish\n    // If no hydration takes place we replace this stream with an empty one\n    const bufferedOps$ = hydration\n      ? pipe(\n          sharedOps$,\n          buffer(fromPromise(hydration)),\n          take(1),\n          mergeMap(fromArray)\n        )\n      : (empty as Source<Operation>);\n\n    const inputOps$ = pipe(\n      concat([bufferedOps$, sharedOps$]),\n      map(addTypeNames),\n      tap(optimisticUpdate),\n      share\n    );\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cache$ = pipe(\n      inputOps$,\n      filter(op => isCacheableQuery(op)),\n      map(operationResultFromCache),\n      share\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheOps$ = pipe(\n      cache$,\n      filter(res => res.outcome === 'miss'),\n      map(res => addCacheOutcome(res.operation, res.outcome))\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cache$,\n      filter(res => res.outcome !== 'miss'),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome } = res;\n          const policy = getRequestPolicy(operation);\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            policy === 'cache-and-network' ||\n            (policy === 'cache-first' && outcome === 'partial')\n          ) {\n            result.stale = true;\n            client.reexecuteOperation(\n              toRequestPolicy(operation, 'network-only')\n            );\n          }\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      forward(\n        merge([\n          pipe(\n            inputOps$,\n            filter(op => !isCacheableQuery(op))\n          ),\n          cacheOps$,\n        ])\n      ),\n      map(updateCacheWithResult)\n    );\n\n    return merge([result$, cacheResult$]);\n  };\n};\n","import {\n  DocumentNode,\n  buildClientSchema,\n  visitWithTypeInfo,\n  TypeInfo,\n  FragmentDefinitionNode,\n  GraphQLSchema,\n  IntrospectionQuery,\n  FragmentSpreadNode,\n  NameNode,\n  ASTNode,\n  isCompositeType,\n  isAbstractType,\n  Kind,\n  visit,\n} from 'graphql';\n\nimport { pipe, tap, map } from 'wonka';\nimport { Exchange, Operation } from 'urql/core';\n\nimport { getName, getSelectionSet, unwrapType } from './ast';\nimport { makeDict } from './store';\nimport { invariant, warn } from './helpers/help';\n\ninterface PopulateExchangeOpts {\n  schema: IntrospectionQuery;\n}\n\n/** An exchange for auto-populating mutations with a required response body. */\nexport const populateExchange = ({\n  schema: ogSchema,\n}: PopulateExchangeOpts): Exchange => ({ forward }) => {\n  const schema = buildClientSchema(ogSchema);\n  /** List of operation keys that have already been parsed. */\n  const parsedOperations = new Set<number>();\n  /** List of operation keys that have not been torn down. */\n  const activeOperations = new Set<number>();\n  /** Collection of fragments used by the user. */\n  const userFragments: UserFragmentMap = makeDict();\n  /** Collection of actively in use type fragments. */\n  const activeTypeFragments: TypeFragmentMap = makeDict();\n\n  /** Handle mutation and inject selections + fragments. */\n  const handleIncomingMutation = (op: Operation) => {\n    if (op.operationName !== 'mutation') {\n      return op;\n    }\n\n    const activeSelections: TypeFragmentMap = makeDict();\n    for (const name in activeTypeFragments) {\n      activeSelections[name] = activeTypeFragments[name].filter(s =>\n        activeOperations.has(s.key)\n      );\n    }\n\n    return {\n      ...op,\n      query: addFragmentsToQuery(\n        schema,\n        op.query,\n        activeSelections,\n        userFragments\n      ),\n    };\n  };\n\n  /** Handle query and extract fragments. */\n  const handleIncomingQuery = ({ key, operationName, query }: Operation) => {\n    if (operationName !== 'query') {\n      return;\n    }\n\n    activeOperations.add(key);\n    if (parsedOperations.has(key)) {\n      return;\n    }\n\n    parsedOperations.add(key);\n\n    const [extractedFragments, newFragments] = extractSelectionsFromQuery(\n      schema,\n      query\n    );\n\n    for (let i = 0, l = extractedFragments.length; i < l; i++) {\n      const fragment = extractedFragments[i];\n      userFragments[getName(fragment)] = fragment;\n    }\n\n    for (let i = 0, l = newFragments.length; i < l; i++) {\n      const fragment = newFragments[i];\n      const type = getName(fragment.typeCondition);\n      const current =\n        activeTypeFragments[type] || (activeTypeFragments[type] = []);\n\n      (fragment as any).name.value += current.length;\n      current.push({ key, fragment });\n    }\n  };\n\n  const handleIncomingTeardown = ({ key, operationName }: Operation) => {\n    if (operationName === 'teardown') {\n      activeOperations.delete(key);\n    }\n  };\n\n  return ops$ => {\n    return pipe(\n      ops$,\n      tap(handleIncomingQuery),\n      tap(handleIncomingTeardown),\n      map(handleIncomingMutation),\n      forward\n    );\n  };\n};\n\ntype UserFragmentMap<T extends string = string> = Record<\n  T,\n  FragmentDefinitionNode\n>;\n\ntype TypeFragmentMap<T extends string = string> = Record<T, TypeFragment[]>;\n\ninterface TypeFragment {\n  /** Operation key where selection set is being used. */\n  key: number;\n  /** Selection set. */\n  fragment: FragmentDefinitionNode;\n}\n\n/** Gets typed selection sets and fragments from query */\nexport const extractSelectionsFromQuery = (\n  schema: GraphQLSchema,\n  query: DocumentNode\n) => {\n  const extractedFragments: FragmentDefinitionNode[] = [];\n  const newFragments: FragmentDefinitionNode[] = [];\n  const typeInfo = new TypeInfo(schema);\n\n  visit(\n    query,\n    visitWithTypeInfo(typeInfo, {\n      Field: node => {\n        if (node.selectionSet) {\n          const type = getTypeName(typeInfo);\n          newFragments.push({\n            kind: Kind.FRAGMENT_DEFINITION,\n            typeCondition: {\n              kind: Kind.NAMED_TYPE,\n              name: nameNode(type),\n            },\n            name: nameNode(`${type}_PopulateFragment_`),\n            selectionSet: node.selectionSet,\n          });\n        }\n      },\n      FragmentDefinition: node => {\n        extractedFragments.push(node);\n      },\n    })\n  );\n\n  return [extractedFragments, newFragments];\n};\n\n/** Replaces populate decorator with fragment spreads + fragments. */\nexport const addFragmentsToQuery = (\n  schema: GraphQLSchema,\n  query: DocumentNode,\n  activeTypeFragments: TypeFragmentMap,\n  userFragments: UserFragmentMap\n) => {\n  const typeInfo = new TypeInfo(schema);\n\n  const requiredUserFragments: Record<\n    string,\n    FragmentDefinitionNode\n  > = makeDict();\n\n  const additionalFragments: Record<\n    string,\n    FragmentDefinitionNode\n  > = makeDict();\n\n  /** Fragments provided and used by the current query */\n  const existingFragmentsForQuery: Set<string> = new Set();\n\n  return visit(\n    query,\n    visitWithTypeInfo(typeInfo, {\n      Field: {\n        enter: node => {\n          if (!node.directives) {\n            return;\n          }\n\n          const directives = node.directives.filter(\n            d => getName(d) !== 'populate'\n          );\n          if (directives.length === node.directives.length) {\n            return;\n          }\n\n          const possibleTypes = getTypes(schema, typeInfo);\n          const newSelections = possibleTypes.reduce((p, possibleType) => {\n            const typeFrags = activeTypeFragments[possibleType.name];\n            if (!typeFrags) {\n              return p;\n            }\n\n            for (let i = 0, l = typeFrags.length; i < l; i++) {\n              const { fragment } = typeFrags[i];\n              const fragmentName = getName(fragment);\n              const usedFragments = getUsedFragments(fragment);\n\n              // Add used fragment for insertion at Document node\n              for (let j = 0, l = usedFragments.length; j < l; j++) {\n                const name = usedFragments[j];\n                if (!existingFragmentsForQuery.has(name)) {\n                  requiredUserFragments[name] = userFragments[name];\n                }\n              }\n\n              // Add fragment for insertion at Document node\n              additionalFragments[fragmentName] = fragment;\n\n              p.push({\n                kind: Kind.FRAGMENT_SPREAD,\n                name: nameNode(fragmentName),\n              });\n            }\n\n            return p;\n          }, [] as FragmentSpreadNode[]);\n\n          const existingSelections = getSelectionSet(node);\n\n          const selections =\n            existingSelections.length + newSelections.length !== 0\n              ? [...newSelections, ...existingSelections]\n              : [\n                  {\n                    kind: Kind.FIELD,\n                    name: nameNode('__typename'),\n                  },\n                ];\n\n          return {\n            ...node,\n            directives,\n            selectionSet: {\n              kind: Kind.SELECTION_SET,\n              selections,\n            },\n          };\n        },\n      },\n      Document: {\n        enter: node => {\n          node.definitions.reduce((set, definition) => {\n            if (definition.kind === 'FragmentDefinition') {\n              set.add(definition.name.value);\n            }\n            return set;\n          }, existingFragmentsForQuery);\n        },\n        leave: node => {\n          const definitions = [...node.definitions];\n          for (const key in additionalFragments)\n            definitions.push(additionalFragments[key]);\n          for (const key in requiredUserFragments)\n            definitions.push(requiredUserFragments[key]);\n          return { ...node, definitions };\n        },\n      },\n    })\n  );\n};\n\nconst nameNode = (value: string): NameNode => ({\n  kind: Kind.NAME,\n  value,\n});\n\n/** Get all possible types for node with TypeInfo. */\nconst getTypes = (schema: GraphQLSchema, typeInfo: TypeInfo) => {\n  const type = unwrapType(typeInfo.getType());\n  if (!isCompositeType(type)) {\n    warn(\n      'Invalid type: The type ` + type + ` is used with @populate but does not exist.',\n      17\n    );\n    return [];\n  }\n\n  return isAbstractType(type) ? schema.getPossibleTypes(type) : [type];\n};\n\n/** Get name of non-abstract type for adding to 'activeTypeFragments'. */\nconst getTypeName = (typeInfo: TypeInfo) => {\n  const type = unwrapType(typeInfo.getType());\n  invariant(\n    type && !isAbstractType(type),\n    'Invalid TypeInfo state: Found no flat schema type when one was expected.',\n    18\n  );\n\n  return type.toString();\n};\n\n/** Get fragment names referenced by node. */\nconst getUsedFragments = (node: ASTNode) => {\n  const names: string[] = [];\n\n  visit(node, {\n    FragmentSpread: f => {\n      names.push(getName(f));\n    },\n  });\n\n  return names;\n};\n"],"names":["node","type","unwrapType","const","cache","Set","currentDebugStack","typename","identifier","Kind","invariant","condition","message","code","process","errorMessage","getDebugOutput","error","Error","helpUrl","console","fieldName","args","stringifyVariables","fieldKey","parenIndex","arguments","JSON","defer","Promise","fn","let","currentData","currentDependencies","currentOptimisticKey","optimistic","makeDict","base","Map","keys","data","optimisticKey","gc","map","entityKey","value","undefined","keymap","entity","i","l","index","gcBatch","refCount","by","count","newCount","link","updateRCForEntity","Array","fieldInfos","seenFieldKeys","fieldInfoOfKey","extractNodeFields","owner","updateRCForLink","linkNode","updateDependencies","links","prevLinkNode","prevLink","ctx","typeCondition","getTypeCondition","warn","getSelectionSet","keyOfField","getName","getFieldArguments","readRecord","readLink","SelectionIterator","select","this","variables","directives","directive","name","isInclude","arg","valueFromASTUntyped","vars","shouldInclude","fragmentNode","isFragmentHeuristicallyMatching","x","store","request","initDataState","startWrite","operation","getMainOperation","result","dependencies","getCurrentDependencies","operationName","parentTypeName","parentKey","parentFieldKey","normalizeVariables","fragments","getFragments","schemaPredicates","pushDebugNode","writeSelection","writeRoot","mutationRootKey","iter","resolver","ensureData","resolverValue","updater","fieldArgs","clearDataState","query","names","Object","fragment","_extends","__typename","writeData","InMemoryData","fieldValue","getFieldAlias","key","advice","expected","fieldData","writeField","newData","isRootField","writeRootField","Store","resolvers","updates","optimisticMutations","queryType","mutationType","schema","subscriptionType","queryName","mutationName","subscriptionName","queryRootKey","persistenceScheduled","persistenceBatch","gcScheduled","refLock","makeNodeMap","records","storage","id","_id","field","fieldSelect","childLink","invalidateSelection","createRequest","input","output","dataFragment","readSelection","partial","argsSize","def","SchemaPredicates","buildClientSchema","getField","isNullableType","isNonNullType","ofType","fieldname","abstractType","objectType","object","expectObjectType","doc","isFragmentNode","read","rootKey","rootSelect","readRoot","originalData","fieldAlias","readRootField","isQuery","hasFields","hasPartials","dataFieldValue","resolveResolverResult","resolveLink","prevData","childResult","isListNullable","readResolverResult","resolvedTypename","resultValue","newLink","op","outcome","context","meta","cacheOutcome","formatDocument","requestPolicy","res","isCacheableQuery","d","set","definition","activeTypeFragments","userFragments","p","possibleType","typeFrags","fragmentName","usedFragments","getUsedFragments","j","requiredUserFragments","additionalFragments","kind","nameNode","typeInfo","TypeInfo","existingFragmentsForQuery","visitWithTypeInfo","Field","enter","isAbstractType","possibleTypes","existingSelections","newSelections","selectionSet","selections","Document","leave","definitions","FragmentSpread","f","opts","ref","optimisticKeys","delete","clearOptimisticNodes","write","queryDependencies","collectPendingOperations","writeDependencies","executePendingOperations","pendingOperations","dep","deps","ops","toRequestPolicy","writeOptimistic","client","policy","addCacheOutcome","extensions","hydration","entries","dotIndex","writeLink","writeRecord","ops$","sharedOps$","share","bufferedOps$","mergeMap","fromArray","take","buffer","fromPromise","empty","tap","optimisticUpdate","addTypeNames","concat","cache$","operationResultFromCache","filter","inputOps$","updateCacheWithResult","forward","merge","cacheOps$","result$","cacheResult$","activeOperations","parsedOperations","extractedFragments","newFragments","FragmentDefinition","extractSelectionsFromQuery","c","current","activeSelections","addFragmentsToQuery","s","ogSchema","handleIncomingMutation","handleIncomingTeardown","handleIncomingQuery"],"mappings":";;;;uFAuBEA,eAA2BA,uEAMOA,2EAMlB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;YAWhBC;wBAEmBA,KACVC,GAAWD,YAGbA,KAAQ;;;ACxCjBE,IAAMC,KAAQ,IAAIC,KAELC,IAA8B;;WAEbC,GAAyBP;MACjDQ,IAAa;aACCC,uBAChBD,IAAaD,6BACcA,UACvB,oBACKP,WAAcS,4BAEvBD,KADaR,eAAgBA,qBAAqB,mBAC1BA,cACfA,WAAcS,6BACvBD,IAAa,MAAIR;OAIjBM,OAAuBE;;;;oBAMrB,mBAAmBF,OAAuB,QAAQ,MAClD;;;AAENI,WACEC,GACAC,GACAC;OAEKF;cACgBC,KAAW,oBAAoBC,IAAO,MAC5B,iBAAzBC,yBACFC,KAAgBC;KAGZC,IAAYC,MAAMH,IAvC1BI,2FAuCmDN,WACpC;IACPI;;;;WAIWL,GAAiBC;EAC/BT,OAAUQ,OACbQ,aAAaR,IAAUI,OA/CzBG,2FA+CsDN;EACpDT,OAAUQ;;;WCxDaS,GAAmBC;aAClCD,UAAaE,mBAAmBD,WAAWD;;;YAExBG;MACvBC,IAAaD,UAAiB;cAChCC,IACK;cACLD;IACAH,WAAWG,QAAe,GAAGC;IAC7BC,WAAWC,WAAWH,QAAeC,IAAa;MAG7C;cACLD;IACAH,WAAWG;IACXE,WAAW;;;;AClBVvB,IAAMyB,KACc,iBAAzBd,wBAA4D,gCACxDe,4BAA4BA,8BAC5BC;oBAAiBA,GAAI;;;;uBC+BsB;;;AAEjDC,IAAIC,IAAmC,MACnCC,IAA0C,MAC1CC,IAAsC;;;SAEA;IACxCC,YAAYC;IACZC,MAAM,IAAIC;IACVC,MAAM;;;;WAKNC,GACAC;EAEAT,IAAcQ;MACQ,IAAInC;MACHoC;mBACnB3B,yBACFR,WAA2B;;;;MAMvBkC,IAAOR;GAERQ,iBAAwC,IAApBA,mBACvBA,iBAAmB,GACnBZ;IACEc,GAAGF;;gBAIcA,2BACnBA,0BAA4B,GAC5BZ;IACEY,gBAAoBA;8BACQ;yBACJJ;;MAK5BH,IADAD,IAAc;mBAGVlB,yBACFR,WAA2B;;;;EAM7BI,EAC0B,SAAxBuB,2CACA,0KAGA;;;;YAqBFU,GACAC,GACApB,GACAqB;EAKIX,UAG2CY,MAAzCH,aAAeT,OACjBS,aAAeT,KAAwB,IAAII,KAC3CK,eAAiBT;EAGnBa,IAASJ,aAAeT,MAExBa,IAASJ;MAIPK,IAASD,MAAWH;aACpBI,KACFD,MAAWH,GAAYI,IAASZ;aAM9BS,KAAwBX,IAG1Bc,EAAOxB,KAAYqB,WAFZG,EAAOxB;;;YAQhBmB,GACAC,GACApB;OACe,IAENyB,IAAI,GAAGC,IAAIP,eAAiBM,IAAIC,GAAGD,KAAK;QAEzCjD,IADa2C,aAAeA,OAASM,QACfL;aAEfE,MAAT9C,KAAsBwB;eACZA;;;qBAKVxB,IAAO2C,WAAaC,MACE5C,EAAKwB,UAAYsB;;;YAIdH,GAAiBF;MAE1CU,IAAQR,eAAiBF;OAC3BU,aAEKR,aAAeF,IACtBE,cAAgBQ,GAAO;;;YAMzBC,GACAC,GACAT,GACAU;MAGMC,SAAgCT,MAAxBO,EAAST,KAA2BS,EAAST,KAAa;MAEtDS,EAAST,KAAcW,IAAQD,IAAM;aAGnDF,MACc,KAAZI,IAAeJ,MAAYR,KACb,KAATW,KAAyB,IAAXC,KAAcJ,SAAeR;;;YAMtDQ,GACAC,GACAI,GACAH;MAEoB;IAClBI,GAAkBN,GAASC,GAAUI,GAAMH;aAClCK,cAAcF;SAAO,IACrBR,IAAI,GAAGC,IAAIO,UAAaR,IAAIC,GAAGD,KAAK;UACrCL,IAAYa,EAAKR;WAErBS,GAAkBN,GAASC,GAAUT,GAAWU;;;;;YAQtDM,GACAC,GACA7D;WAEa8C,MAAT9C;SACGG,IAAMqB;MACJqC,MAAkBrC,OAGrBoC,OAAgBE,GAAetC,KAC/BqC,MAAkBrC;;;;;YAQxBoC,GACAC,GACAjB,GACAD;EAGAoB,GAAkBH,GAAYC,GAAelB,WAAaC;OAF1D,IAKSK,IAAI,GAAGC,IAAIP,eAAiBM,IAAIC,GAAGD;IAE1Cc,GAAkBH,GAAYC,GADXlB,aAAeA,OAASM,QACiBL;;;;YAK7CJ;EAEjBA,iBAAmB;8BAIEI;QAGT,MADCJ,WAAcI,MAAc,IAC1B;WAENzC,IAAMsC,gBAA+B;YAClCY,IAAWb,UAAaC;YAIlB,KAHEY,EAAST,MAAc;;;eAI9BS,EAAST;;aAMXJ,WAAcI;uBACDA;eAOAE,WADAN,mBAAsBI,QAExCJ,sBAAyBI,IACrBJ;aACGrC,IAAMqB;UAETgB,mBF3QmDwB,OE0QfpB,UAAWpB,UAClBsB;;;eAQlBA,WADAN,iBAAoBI,KACT;QAC1BJ,oBAAuBI;aAClBzC,IAAMqB;UAELgB,cAEFA,mBFzRmDwB,OEwRfpB,UAAWpB,UAClBsB,IAG/BmB,GAAgBzB,WAAcA,YAAe0B,EAAS1C;;;;uBAItCoB;;;;;WAKEA,GAAmBpB;EAC5B,iBAAbA,MACEoB,MAAcZ,iBAChBC,MAAyBW,UACHE,MAAbtB,KACTS,MAAkCW,UAAWpB;;;WAOjDoB,GACApB;EAEA2C,EAAmBvB,GAAWpB;YACfQ,WAAsBY,GAAWpB;;;WAKhDoB,GACApB;EAEA2C,EAAmBvB,GAAWpB;YACfQ,SAAoBY,GAAWpB;;;WAK9CoB,GACApB,GACAqB;EAEAsB,EAAmBvB,GAAWpB;KACtBQ,WAAsBY,GAAWpB,GAAUqB;gBACtBX,MAE3BF,mBF3U2DgC,OE0UvBpB,UAAWpB,KACVqB;;;YAUvCD,GACApB,GACAiC;MAEMjB,IAAOR;MAOTE,GAAsB;IAGxBmB,IAAAA,IACEb,UAAaN,OACZM,UAAaN,KAAwBE;QACxCgC,IAAQ5B,mBAAsBN;SACzB;IACDM,cAEFA,mBF1WyDwB,OEyWrBpB,UAAWpB,KAClBiC;QAEpBjB;QACHA;QACRY,IAAUZ;;WAKsBM,OAD5BuB,SAAyBvB,MAAVsB,IAAsBA,MAAUxB,UAAaE,KACpBuB,EAAa7C,KAAY;IAGpDoB,GAAWpB;KAEtBgB,SAAYI,GAAWpB,GAAUiC;KAEzBL,GAASC,GAAUiB;KAEnBlB,GAASC,GAAUI,GAAM;;;YCrXzCzD,GACAO,GACAqC,GACA2B;OAEKhE;YAAiB;;MAChBiE,IAAgBC,GAAiBzE;MACnCO,MAAaiE;YAAsB;;2CAEvCE,EACE,6EACEnE,IACA,wCAEAiE,IACA,6CACAA,IACA,iJAGF;UAGMG,EAAgB3E,kBAAWA;QAChBA,WLnBLS;cKmBmB;;QACdmE,EACfC,EAAQ7E,IACR8E,EAAkB9E,GAAMuE;kBD8SQzB,MAApCiC,EC5SmBnC,GAAWpB,WD6SIsB,MAAlCkC,EC7SmBpC,GAAWpB;;;;AAW9ByD,WACE1E,GACAqC,GACAsC,GACAX;kBAEgBhE;mBACCqC;iBACF2B;oBACG,EAAC;wBACG,EAACW;;;;QAIW,MAA3BC,0BAA8B;QAC7BhC,IAAQgC,gBAAgBA,yBAAyB,MACjDD,IAASC,oBAAoBA,6BAA6B;QAC5DhC,KAAS+B;;WAIN;MACClF,IAAOkF,EAAO/B;SCnDjB;QDoDsBiC,IAAAA;gBAANpF;iBClDN8C,MAAfuC;eAFK,IAOApC,IAAI,GAAGC,IAAImC,UAAmBpC,IAAIC,GAAGD,KAAK;gBAC3CqC,IAAYD,EAAWpC,IACvBsC,IAAOV,EAAQS,IAGfE,IAAqB,cAATD;iBACbC,KAAsB,WAATD,OAGZE,IAAMH,cAAsBA,YAAoB,KAAK,SAC9B,SAAjBT,EAAQY,OAGC,qBADf5C,IAAQ6C,oBAAoBD,SAAWE,OACD,SAAV9C;kBAI3B2C,MAAc3C,KAASA;;;;;aArBvB;;UDiDE+C;QAEE,IAAiB5F,WL5DhBS;mBKkEeqC,OAJf+C,IAAiC7F,WL1DFS,uBK2DjC0E,uBAAuBN,EAAQ7E,MAC/BA,OAG2B,iBAAzBc,0BACYqE,eAAeU;eAIK/C,MAAlCqC,gCACIA,gDACEV,GAAiBoB,IACjBV,iBAEFW,GACED,GACAV,eACAA,gBACAA;iCAIe,6BACIR,EAAgBkB;;mBAKlB,iBAAlBhB,EAAQ7E;;;;;;;;WAYA+F;oBACzBA,IAAkB,OAAQA;;;YEtF1BC,GACAC,GACAzD;EAEA0D,EAAcF,QAAY;MACXG,GAAWH,GAAOC,GAASzD;;;;;YAM1CwD,GACAC,GACAzD;MAEM4D,IAAYC,EAAiBJ,UAC7BK,IAAsB;IAAEC,cAAcC;KAEtCtB,IAASP,EAAgByB,IACzBK,IAAgBT,aAAiBI;MAElB;IACnBM,gBAAgBD;IAChBE,WAAWF;IACXG,gBAAgB;IAChBvF,WAAW;IACX+D,WAAWyB,EAAmBT,GAAWH;IACzCa,WAAWC,EAAad;YACxBK;WACAN;IACAgB,kBAAkBhB;;mBAGhBlF,wBACFmG,EAAcR,GAAeL;QAGT7B,mBAAqB,WACzC2C,GAAe3C,GAAKkC,GAAevB,GAAQ1C,KAE3C2E,GAAU5C,GAAKkC,GAAevB,GAAQ1C;;;;YAOxCwD,GACAC,GACAxD;EAEAyD,EAAcF,QAAYvD;MAEpB2D,IAAYC,EAAiBJ;MACP;IAAEM,cAAcC;;MAEtCY,IAAkBpB,aAAiB,aACnCS,IAAgBT,aAAiBI;IAErCK,MAAkBW,2CAClB,oIAEA;mBAGEtG,wBACFmG,EAAcR,GAAeL;MAGV;IACnBM,gBAAgBU;IAChBT,WAAWS;IACXR,gBAAgB;IAChBvF,WAAW;IACX+D,WAAWyB,EAAmBT,GAAWH;IACzCa,WAAWC,EAAad;YACxBK;WACAN;IACAgB,kBAAkBhB;IAClB7D,aAAY;;MAGDC;MACA,IAAI6C,EACfwB,GACAA,GACA9B,EAAgByB,IAChB7B;WAGEvE,QAC4B8C,OAAxB9C,IAAOqH;aACavE,MAAtB9C,gBAAiC;UAC7BqB,IAAYwD,EAAQ7E,IACpBsH,IAAW/C,4BAA8BlD;eAE9ByB,MAAbwE,GAAwB;QAE1B/C,cAAgBlD;WAKDkD,GADMgD,MADCD,OADJxC,EAAkB9E,GAAMuE,iBACEnC,KAAYmC,SAAWA,KAEjCI,EAAgB3E;UAC7CqB,KAAamG;wBACFjD,gBAAkB6C,GAAiB/F,OAEjDoG,EAAQjF,GAAMkF,KAAatF,KAAYmC,SAAWA;;;;EAM1DoD;;;;YAKA3B,GACA4B,GACApF,GACA4C;EAEM0B,IAAYC,EAAaa;MACzBC,IAAQC,YAAYhB;WAEThE,WADAgE,EAAUe,EAAM;mDAExBnD,EACL,mIAEA;;MAIEnE,IAA+BwH;MACnBC;IAAEC,YAAY1H;KAAaiC;MACvCI,IAAYoD,cAAkBkC;OAC/BtF;mDACI8B,EACL,sIAEEnE,IACA,MACF;;mBAIAO,wBACFmG,EAAc1G,GAAUwH;SAGL;IACnBrB,gBAAgBnG;IAChBoG,WAAW/D;IACXgE,gBAAgB;IAChBvF,WAAW;IACX+D,WAAWA,KAAa;eACxB0B;IACAR,QAAQ;MAAEC,cAAcC;;WACxBR;IACAgB,kBAAkBhB;KAGApD,GAAW+B,EAAgBoD,IAAWG;;;YAI1D3D,GACA3B,GACAsC,GACA1C;MAGMjC,IADUqC,MAAc2B,mBAAqB,WACxB3B,IAAYJ;MACf;IAExB2F,EAAyBvF,GAAW,cAAcrC;QAErC,IAAI0E,EAAkB1E,GAAUqC,GAAWsC,GAAQX;aAE5DvE,QAC4B8C,OAAxB9C,IAAOqH,aAA4B;UACnChG,IAAYwD,EAAQ7E,IACpB0H,IAAY5C,EAAkB9E,GAAMuE;UACzBK,EAAWvD,GAAWqG;UACjCU,IAAa5F,EAAK6F,EAAcrI,KAChCsI,IAAe1F,UAAWpB;UAEH,iBAAzBV;iBACiBgC,MAAfsF,GAA0B;UACtBG,IAAShE,eACX,qDACA;mBAGoBzB,MAAtB9C,iBACI,kCACA;mDAEN0E,EACE,sCACElD,IACA,uDACAgH,IACA,qBACAD,GACF;;;gCAI+BhI,KACjCgE,0CAA4ChE,GAAUc;;;iBAItDrB,iBAEFmI,EAAyBvF,GAAWpB,GAAU4G,MAGxCK,IAAYlB,EAAWa,IAE7BD,GAAuBvF,GAAWpB,GAD5BiC,IAAOiF,GAAWnE,GAAK+D,GAAK3D,EAAgB3E,IAAOyI;;;;;YAO7DlE,GACAqC,GACA1B,GACA1C;MAEImB,cAAcnB,IAAO;aACjBmG,IAAchF,MAAMnB,WACjBS,IAAI,GAAGC,IAAIV,UAAaS,IAAIC,GAAGD,KAAK;UAKrCmB,IAAQsE,GAAWnE,GAFCqC,UAAmB3D,GAELiC,GAJ3B1C,EAAKS;QAMVA,KAAKmB;;;;EAIV,IAAa,SAAT5B;;;MAKe,cADR+B,oBAAsB/B,MACPI,IAAYgE;MAC5BpE;aAGf+B,aAAe/B,iBACD,SAAdI,KACoB,wBACnBrC,WAAkB,iBAClBA,WAAkB,WACN,eAAbA,8CAEAmE,EACE,qDACEkC,IACA,6LAIArG,IACA,mIAGAA,IACA,+BACF;KAIWgE,GAAK+D,GAAKpD,GAAQ1C;;;;YAMjC+B,GACAhE,GACA2E,GACA1C;MAEMoG,IACJrI,MAAagE,mBAAqB,eAClChE,MAAagE,mBAAqB;MAEvB,IAAIU,EAAkB1E,GAAUA,GAAU2E,GAAQX;WAE3DvE,QAC4B8C,OAAxB9C,IAAOqH,aAA4B;QACnChG,IAAYwD,EAAQ7E,IACpB0H,IAAY5C,EAAkB9E,GAAMuE;QACNK,IAAAA,EAAWvD,GAAWqG;QAAhCnH,UL7UZ+H;aK8UYxF,MAAtB9C,gBAAiC;SAEpBuE,GADIgD,EAAW/E,EAAK6F,EAAcrI,MACjB2E,EAAgB3E;;IAG9C4I,MAEFrE,mBAAqBhE,GACrBgE,cAAgBhE,GAChBgE,mBAAqB/C,GACrB+C,cAAgBlD;SAKAyB,OADV2E,IAAUlD,gBAAkBhE,GAAUc,OAE1CoG,EAAQjF,GAAMkF,KAAatF,KAAYmC,SAAWA;;;;YAQxDA,GACA/B,GACA0C;MAEIvB,cAAcnB,IAAO;aACjBmG,IAAchF,MAAMnB,WACjBS,IAAI,GAAGC,IAAIV,UAAaS,IAAIC,GAAGD;MACtC0F,EAAQ1F,KAAK4F,GAAetE,GAAK/B,EAAKS,IAAIiC;;;;EAE1B,SAAT1C,MAMO,UADZI,IAAY2B,oBAAsB/B,MAEtC0E,GAAe3C,GAAK3B,GAAWsC,GAAQ1C,KAGvC2E,GAAU5C,GADO/B,cACQ0C,GAAQ1C;;;AC5WnCsG,WACE9B,GACA+B,GACAC,GACAC,GACA1G;;sBAoDY;;OAEI4C;qBACG;;oBAGRP;mBAxDMmE,KAAa;6BACHE,KAAuB;cACtC1G,KAAQ;0BACIyE;iBAET;cACFgC,KAAWA,cAAqB;kBAC5BA,KAAWA,kBAAyB;;OAK7CE,mCACAC,IAAeC,qBACfC,IAAmBD;oBAQP;WANZE,IAAYJ,IAAYA,SAAiB;cACzCK,IAAeJ,IAAeA,SAAoB;kBAClDK,IAAmBH,IACrBA,SACA;4BAQa,IACdC,KAAY,WACZC,KAAe,cACfC,KAAmB;2BAGJ;WACT;cACG;kBACI;sBAGC;WACR;cACG;kBACI;;0BJaDC;WAAwC;MAC3DC,uBAAsB;MACtBC,kBAAkBvH;MAClBwH,cAAa;oBACbH;MACArG,SAAS,IAAI/C;MACbgD,UAAUjB;MACVyH,SAASzH;MACTgC,OAAO0F;MACPC,SAASD;MACTE,SAAS;;GInBK7B,CAAkBhD,gBAAgB;;;kCAWrCI;yBACcA;;;mCAGb/C;;OAELjC;;;WAEmCuC,MAA7BqC,eAAe5E;;;MAItB+H;YACU/H,SACN4E,UAAU5E,GAAUiC,KACVM,QAAPmH,IACT3B,IAAM,KAAG2B,IACQnH,QAARoH,MACT5B,IAAM,KAAG4B;aAGK3J,UAAY+H,IAAQ;;;yCAGpBtF,GAA8BxB;MAK5B,UAJZoB,IACO,SAAXI,KAAqC,uBACjCmC,iBAAiBnC,KACjBA;;;MAEAoF,IAAaD,EAAwBvF,GAAWpB;oBAClD4G,IAAiCA,KAC/B3E,IAAO0E,EAAsBvF,GAAWpB,MAChCiC,IAAO;;;+BAIrBT,GACAmH,GACA7I;gCAE8B0B,GAAQ4B,EAAWuF,GAAO7I;;;uCAG1CsG,GAA8BxC;eCjG9Cb,GACA3B,GACAsC;QAE8B,YAAdtC,GAGF;MACZrC,IAAAA,IAAW4H,EAAwBvF,GAAW;UACtB;;;QAGGA,GAAW,mBAAcE;;UAGzCF;;QAGA,IAAIqC,EAAkB1E,GAAUqC,GAAWsC,GAAQX;aAE5DvE,QAC4B8C,OAAxB9C,IAAOqH,aAA4B;UACnChG,IAAYwD,EAAQ7E,IACpBwB,IAAWoD,EACfvD,GACAyD,EAAkB9E,GAAMuE;uBAIxBzD,wBACAyD,sBACAhE,KAEAgE,0CAA4ChE,GAAUc;eAG9ByB,MAAtB9C;QACFmI,EAAyBvF,GAAWpB,QAAUsB;iBAExCsH,IAAczF,EAAgB3E,IAC9ByD,IAAO0E,EAAsBvF,GAAWpB,IAE9C2G,GAAuBvF,GAAWpB,QAAUsB,IAC5CqF,EAAyBvF,GAAWpB,QAAUsB,IAE1Ca,cAAcF,IAAO;QACdR,IAAI;aAARlB,IAAWmB,IAAIO,UAAaR,IAAIC,GAAGD,KAAK;cACrCoH,IAAY5G,EAAKR;mBACnBoH,KACFC,GAAoB/F,GAAK8F,GAAWD;;;aAIxCE,GAAoB/F,GAAKd,GAAM2G;;;QApEhB;IACnBhF,WAAWyB,MAHKR,GDmHCkE,IAAAA,cAAc3C,GAAOxC,YChHGa;IACzCa,WAAWC,EAAad;WD+GbD;IC7GXgB,kBD6GWhB;KCxGXzB,mBAAqB,UACrBI,EAAgByB;;;qCD0GJpD;MAKS,UAJfJ,IACO,SAAXI,KAAqC,uBACjCmC,iBAAiBnC,KACjBA,IACCJ;oCJ0QHgB,IAA0B,IAC1BC,IAA6B,IAAIxD;MI3QT8H;OJgRTvE,GAAYC,GIhRHsE,GJgR6B/D;OACtCR,GAAYC,GIjRHsE,GJiR6B4B;QACpDnG;;QIlR+D;;;;;mCAIpE4G,GACA/C;EAEMxB,IAAUsE,cAAcC,SAAaA;gBAC5B/C,EAAQtC,eAAec,WAEzBd,MAAMc,GAASwE;;;iCAIpBD;YACIrF,MAAMoF,cAAcC,SAAaA;;;oCAI7CE,GACA1H,GACAoC;EETI0B,IAAYC,EFWU2D;MEVtB7C,IAAQC,YAAYhB;WAEThE,WADAgE,EAAUe,EAAM;6CAE/BnD,EACE,kIAEA;QAGK;;QAGHnE,IAA+BwH;4BFFK/E,iBAAAA,eEIpBzC;KAGhBqC,IACc,uBFREoD,iBESEgC;MAAEC,YAAY1H;OFTIyC,MAAAA,MEwBb,iBAAzBlC,wBACFmG,EAAc1G,GAAUwH,QAgBxB4C,EAbmBpG;MACnBmC,gBAAgBnG;MAChBoG,WAAW/D;MACXgE,gBAAgB;MAChBvF,WAAW;MACX+D,WFjCgDA,KEiCxB;iBACxB0B;MACA8D,UAAS;aFnCW5E;MEqCpBgB,kBFrCoBhB;OEyCDpD,GAAW+B,EAAgBoD,IAAW3F,QAAe,kDA5BxEsC,EACE,gIAEEnE,IACA,MACF;QAGK;;;;;qCFjBPmK,GACAlI,GACA4C;KAEcD,MAAMuF,GAAclI,GAAM4C;;;WG9K1CpF,GACA2F;WAEuB7C,MAAnB9C,eAA0D,MAA1BA;;;WAI9BsB,IAAOc,KACTyI,IAAW,GAEN5H,IAAI,GAAGC,IAAIlD,oBAAuBiD,IAAIC,GAAGD,KAAK;QAC/CwC,IAAMzF,YAAeiD,IACrBJ,IAAQ6C,oBAAoBD,SAAWE;YACzC9C,MACFvB,EAAKuD,EAAQY,MAAQ5C,GACrBgI;;aAIGA,IAAevJ,IAAO;;;WAK7BtB,GACAwK;WAEiC1H,MAA7B9C;WACK;;MAGHsB,IAAmBkJ,KAAuB;gDAER7E,GAAMmF;QACtCvF,IAAOV,EAAQiG,aACjBjI,IAAQvB,EAAKiE;aACHzC,MAAVD;eACuBC,MAArBgI;QACFjI,IAAQ6C,oBAAoBoF,gBAAkBxJ;;;;;MAM7CiE,KAAQ1C;;MAEZT;;;ACzCH2I,WAAY3B;gBACI4B,kBAAkB5B;;;uCAGlB7I,GAAkBc;qBAC1B8I,IAAQc,GAAS9F,aAAa5E,GAAUc,OACd,IACzB6J,eAAef;;;sCAGT5J,GAAkBc;WAEjByB,OADRqH,IAAQc,GAAS9F,aAAa5E,GAAUc;YACd;;MACjB8J,cAAchB,UAAcA,gBAAoBA;oBAC7CiB,MAAWF,eAAeE;;;8CAGvB7K,GAAkB8K;WAC9BJ,GAAS9F,aAAa5E,GAAU8K;;;yCAIzC7G,GACAjE;OAEKA,MAAaiE;YAAsB;;MACpCjE,MAAaiE;YAAsB;;MAEjC8G,IAAenG,oBAAoBX,IACnC+G,IAAapG,oBAAoB5E;MAEnC+K;iBACsBC;;IAGPD,qCAAAA,uEA+CnB,sCA/CiC9G,IAiD/B,2IAEF;KAlDiB+G,GAAYhL;oCACK+K,GAAcC;;;YAKlDnC,GACA7I,GACAc;KAEMmK,IAASpC,UAAe7I,IACLA;WAGXuC,WADA0I,cAAmBnK;6CAE/BqD,EACE,+BACErD,IACA,0BACAd,IACA,2HAGF;;;;;;AASNkL,YAA0B1F,GAAQxF;EAChCG,EACEqF,wEACA,oCACExF,IACA,6FAEF;;;YN9EoBP;oBACRS;;;YAOZT;oBAAsBS;;;WAHxBiL;OAEMtF,IAAYsF,iEAMhB,wIAEA;;;;YAQ6C/I,GAAgB3C;EAC7D2C,EAAIkC,EAAQ7E,MAASA;;;;WAFI0L;8BACJC,eAGpB;;;YIkBH3F,GACAC,GACAzD;EAEA0D,EAAcF,QAAY;MACX4F,GAAK5F,GAAOC,GAASzD;;;;;YAMpCwD,GACAC,GACAuE;MAEMpE,IAAYC,EAAiBJ,UAC7B4F,IAAU7F,aAAiBI,cAC3B0F,IAAanH,EAAgByB;MAEd;IACnBM,gBAAgBmF;IAChBlF,WAAWkF;IACXjF,gBAAgB;IAChBvF,WAAW;IACX+D,WAAWyB,EAAmBT,GAAWH;IACzCa,WAAWC,EAAad;IACxB2E,UAAS;WACT5E;IACAgB,kBAAkBhB;;mBAGhBlF,wBACFmG,EAAc4E,GAASzF;MAGdoE,KAASpI;MAElByJ,MAAYtH,mBAAqB,WAC7BwH,GAASxH,GAAKsH,GAASC,GAAYtJ,KACnCmI,EAAcpG,GAAKsH,GAASC,GAAYtJ;SAEvC;IACL+D,cAAcC;IACdoE,cAAkB9H,MAATN,KAAqB,IAAQ+B;IACtC/B,WAAeM,MAATN,IAAqB,OAAOA;;;;YAKpC+B,GACA3B,GACAsC,GACA8G;MAEuC;;;MAI1B,IAAI/G,EAAkBrC,GAAWA,GAAWsC,GAAQX;OACpDnC,kBACK4J;WAEdhM,QAC4B8C,OAAxB9C,IAAOqH,aAA4B;QACnC4E,IAAa5D,EAAcrI,IAC3BoI,IAAa4D,EAAaC;eAC5BjM,kBAAkD,SAAfoI,KAC/BK,IAAYlB,EAAWa,IAC7B5F,EAAKyJ,KAAcC,GAAc3H,GAAKI,EAAgB3E,IAAOyI,MAE7DjG,EAAKyJ,KAAc7D;;;;;YAQvB7D,GACAW,GACA8G;MAEIrI,cAAcqI,IAAe;aACzBrD,IAAchF,MAAMqI,WACjB/I,IAAI,GAAGC,IAAI8I,UAAqB/I,IAAIC,GAAGD;MAC9C0F,EAAQ1F,KAAKiJ,GAAc3H,GAAKW,GAAQ8G,EAAa/I;;;;EAElD,IAAqB,SAAjB+I;;;uBAKOzH,oBAAsByH,WAKhBlJ,OADhBsF,IAAauC,EAAcpG,GAAK3B,GAAWsC,GAAQ9C,QACvB,OAAOgG,IAElC2D,GAASxH,GAAKyH,cAAyB9G,GAAQ8G;;;WAmExDzH,GACA3B,GACAsC,GACA1C;2CAGM2J,IAAUvJ,MAAcoD,aAAiB,UAGzCzF,IAAY4L,IAEdvJ,IADAuF,EAAwBvF,GAAW;MAEf;IAIxBJ,eAAkBjC;QACL,IAAI0E,EAAkB1E,GAAUqC,GAAWsC,GAAQX;aAE5DvE,GACAoM,KAAY,GACZC,KAAc,QACcvJ,OAAxB9C,IAAOqH,aAA4B;UAEnChG,IAAYwD,EAAQ7E,IACpB0H,IAAY5C,EAAkB9E,GAAMuE,cACpC0H,IAAa5D,EAAcrI,IAC3BwB,IAAWoD,EAAWvD,GAAWqG,IACjCU,IAAaD,EAAwBvF,GAAWpB,IAChD8G,IAAe1F,UAAWpB;uBAE5BV,wBAAyCkG,KAAoBzG,KAC/DyG,yBAAwCzG,GAAUc;UAKhDiL,YAEEvD,IAAY/C,YAAgBzF;eAChBuC,MAAdiG,KAA2D,uBAAf1H;YAG9CkD,mBAAqBhE,GACrBgE,cAAgB3B,GAChB2B,mBAAqB+D,GACrB/D,cAAgBlD;aAIGyB,MAAfsF,MACF5F,EAAKyJ,KAAc7D,IAGrBkE,IAAiBvD,EAAU1H,GACzBmB,GACAkF,KAAatF,KACb4D,GACAzB,SAGwBzB,MAAtB9C,mBAGFsM,IAAiBC,GACfhI,GACAhE,GACAc,GACAiH,GACA3D,EAAgB3E,IACfwC,EAAKyJ,MAAwB7J,KAC9BkK;aAKmBxJ,MAArBkE,KACmB,SAAnBsF,MACCtF,kBAAiCzG,GAAUc;;;;mBAMrCrB,iBAETsM,IAAiBlE,SAIJtF,OADPW,IAAO0E,EAAsBvF,GAAWpB,MAE5C8K,IAAiBE,GACfjI,GACAd,GACAlD,GACAc,GACAsD,EAAgB3E,IAChBwC,EAAKyJ,MAEwB,wBAA2B,SAAf7D,MAE3CkE,IAAiBlE;;eAQAtF,MAAnBwJ,UACqBxJ,MAArBkE,KACAA,kBAAiCzG,GAAUc;QAI3CgL,KAAc,GACd7J,EAAKyJ,KAAc;aACd;QAAA,SAAuBnJ,MAAnBwJ;;;aAKG;UACPL,KAAcK;;;IAInBD,MAAa9H,aAAc;gBACb8H,MAAgBD,SAAYtJ,IAAYN;;;;YA0H1D+B,GACAhE,GACAc,GACAiH,GACApD,GACAuH,GACAnG;MAEI3C,cAAc2C,IAAS;;aAKFxD,MAArBkE,KACAA,iBAAgCzG,GAAUc;aACtCmB,IAAWmB,MAAM2C,WACdrD,IAAI,GAAGC,IAAIoD,UAAerD,IAAIC,GAAGD,KAAK;UAEvCyJ,IAAcH,GAClBhI,GACAhE,GACAc,GACSiH,UAAQrF,GACjBiC,QAEapC,MAAb2J,IAAyBA,EAASxJ,UAAKH,GACvCwD,EAAOrD;eAGWH,MAAhB4J,KAA8BC;QAGhCnK,EAAKS,UAAqBH,MAAhB4J,IAA4BA,IAAc;;;;;;;EAKnD,IAAe,QAAXpG;;;MAgEE,wBACC,wBAA6C,iCA/D3B;IACxB9D,SAAoBM,MAAb2J,IAAyBrK,MAAaqK;QAC1B;MACrB9B,IAAAA,EAAAA,GAAAA,GAAAA,GAAAA;;iBACAiC,oBA1JAhK,IA0JAgK,oBAAAA,MAAAA,GAzJAC,IAyJAD;MAxJArM,IACJ4H,EAAwBvF,GAAW,iBAAiBiK,GAGhC,wBACnBA,KAAoBtM,MAAasM;iDAGlCnI,EACE,6CACE9B,IACA,+EAEF;iBAGKE;;QAwIH8J,eAnIYrM;YACL,IAAI0E,EAAkB1E,GAAUqC,GAkIvCgK,GAAAA;aA9HFP,IADAD,KAAY,QAEgBtJ,OAAxB9C,IAAOqH,aAA4B;UAEnChG,IAAYwD,EAAQ7E;cACPqI,EAAcrI;cAC3BwB,IAAWoD,EACfvD,GACAyD,EAAkB9E,GAuHhB4M,eArHEtE,IAAe1F,UAAWpB,GAC1B4G,IAAaD,EAAwBvF,GAAWpB,IAChDsL,IAmHFF,EAnHuBvL;2BAEvBP,wBAAyCkG,KAAoBzG,KAC/DyG,yBAAwCzG,GAAUc;cAKhDiL;qBACAQ,UAAmDhK,MAAtB9C,iBAE/BsM,IAAiBQ,SACchK,MAAtB9C,iBAETsM,IAAiBlE,SACQtF,MAAhBgK,IAETR,IAAiBC,GAkGfK,GAhGArM,GACAc,GACAiH,GACA3D,EAAgB3E,IA6FhB4M,EA5FKX,IACLa,UAMWhK,OAFPW,IAAO0E,EAAsBvF,GAAWpB,MAG5C8K,IAAiBE,GAoFjBI,GAlFEnJ,GACAlD,GACAc,GACAsD,EAAgB3E,IA+ElB4M,EA9EOX,MAEwB,wBAA2B,SAAf7D,MAE3CkE,IAAiBlE;mBAQAtF,MAAnBwJ,UACqBxJ,MAArBkE,KACAA,kBAAiCzG,GAAUc;YAI3CgL,KAAc,GA4DZO,EA3DGX,KAAc;0BACSnJ,MAAnBwJ,GAA8B;qBAEhCxJ;;;iBAGK,GAqDV8J,EApDGX,KAAcK;;;QAInBD,MAgDEO,aAhDyB;YACvBR,IA+CFQ,SA/Cc9J;;;;;2CAiDlB4B,EACE,2CACE4D,IACA,uGAEF;;;YAQJ/D,GACAd,GACAlD,GACAc,GACA6D,GACAuH;MAEI9I,cAAcF,IAAO;;aAGAX,MAArBkE,KACAA,iBAAgCzG,GAAUc;aACtC0L,IAAcpJ,MAAMF,WACjBR,IAAI,GAAGC,IAAIO,UAAaR,IAAIC,GAAGD,KAAK;UACrCoH,IAAYmC,GAChBjI,GACAd,EAAKR,IACL1C,GACAc,GACA6D,QACapC,MAAb2J,IAAyBA,EAASxJ,UAAKH;eAEvBA,MAAduH,KAA4BsC;QAG9BI,EAAQ9J,UAAmBH,MAAduH,IAA0BA,IAAY;;;;;;;EAKlD,gBAAI5G,IACF,OAEAkH,EACLpG,GACAd,GACAyB,QACapC,MAAb2J,IAAyBrK,MAAaqK;;;YGrgBnBO,GAAeC;iBACnCD;IACHE,iBACKF;MACHG,cACKH;QACHI,cAAcH;;;;;;YAMED;iBACjBA;IACHpF,OAAOyF,eAAeL;;;;YAeEA;SAPH,YAQGA,mBAAgC,mBAARA;;;YAUhD5G,GACAkH;iBAEGlH;IACH8G,iBACK9G;qBACHkH;;;;;YAsMSN;YAAuBA;;;YAS1BO;YAAuBA,aAAeA;;;YADnCA;SAAuB,WAAhBA;;;YAQPA;SAAuB,WAAhBA;;;YAkCDP;UAAOQ,GAAiBR;;;YCpJ/BS;SAAoB,eAAf5I,EAAQ4I;;;YA8DUC,GAAKC;EACJ,yBAApBA,UACFD,MAAQC;;;;YA9FpBvE,GACAxB,GACAgG,GACAC;aAkCoDC,GAAGC;UACvCC,IAAYJ,EAAoBG;;;SADqB,IAMlD9K,IAAI,GAAGC,IAAI8K,UAAkB/K,IAAIC,GAAGD,KAAK;mBAC3B+K,EAAU/K,aACzBgL,IAAepJ,EAAQkD,IACvBmG,IAAgBC,GAAiBpG,IAG9BqG,IAAI,GAAGlL,IAAIgL,UAAsBE,IAAIlL,GAAGkL,KAAK;YAC9C7I,IAAO2I,EAAcE;cACQ7I,OACjC8I,EAAsB9I,KAAQsI,EAActI;;MAKhD+I,EAAoBL,KAAgBlG;aAE7B;QACLwG,MAAM9N;QACN8E,MAAMiJ,GAASP;;;;;MAxDvBQ,IAAW,IAAIC,SAAStF,IAExBiF,IAGFjM,KAEEkM,IAGFlM,KAGEuM,IAAyC,IAAItO;eAGjDuH,GACAgH,kBAAkBH,GAAU;IAC1BI,OAAO;MACLC,gBAAO9O;YACAA;cAICqF,IAAarF;cAGfqF,aAAsBrF;YAuF5BC,IAAAA,IAAOC,GAnFkCuO;4BAoF1BxO,SAQd8O,eAAe9O,KA5FiBmJ,mBA4FenJ,KAAQ,EAACA,gDAP7DyE,EACE,kFACA;gBAEK;gBAxFqBsK,YA6BnB;gBAEGC,IAAqBtK,EAAgB3E;gBAGY,MAArDiP,WAA4BC,WACxBA,SAAsBD,KACtB,EACE;cACEV,MAAM9N;cACN8E,MAAMiJ,GAAS;;2BAKpBxO;0BACHqF;cACA8J,cAAc;gBACZZ,MAAM9N;4BACN2O;;;;;;;IAKRC,UAAU;MACRP,gBAAO9O;QACLA,yBAKG2O;;MAELW,gBAAOtP;YAEMsI,GADLiH,IAAc,UAAIvP;aACbsI;UACTiH,OAAiBjB,EAAoBhG;;aAClCnI,IAAMmI;UACTiH,OAAiBlB,EAAsB/F;;uBAC7BtI;uBAAMuP;;;;;;;YAOV1M;SAA6B;IAC7C0L,MAAM9N;WACNoC;;;;YA8BwB7C;MAClB6H,IAAkB;QAElB7H,GAAM;IACVwP,yBAAgBC;MACd5H,OAAWhD,EAAQ4K;;;;;;;;;ED9MKC;IAAwCC;MAqHrCrJ,oEAEvB6F;oBAKiB7D;QAAnBsH,EAAyBC;QACLvH;;QACMA,KTiLhCwH;QSjLgCxH,KTkLhCwH;;MSlLgCxH;QAKT9F,WACCuN,GAAAA,GAAAA,GAAwBvN;;UAExC2J,WACkBvE,GAAAA;UAAaxB,IACjC5D,EAAAA;cACAwN,EAAAA;;cAEAxN,GAAOoF,GAAAA,GAAAA,GAAwBpF;;;YARM,SAczCyN;MAA4CC,UAE1CD;MAA4CD,EAI9CG,EAAAA;MAA2CC,qBAG5BJ,IACb7L;MAAqC6L;;;;;;;IAGL5J;MAhElCA,QAEwCwB,GAAM5B,GAAOI;;eAGjD5D,IACF4K,IAAe,UAEfjJ,EAAmBiC,GAAWG,IAC9B6G,IACGxC,KAA2C,iBAAfxE,0BAEzB,YADA;aAID;QACL6G,SAASG;mBACThH;cACA5D;;;eArCwBwK,GAAezG;MACzCA,oBAAqB8J;SACNC,EAAKD,OAASC,EAAKD,KAAO,UAC7BrD;cAEGA,UACXuD,MACEvD,OACyB,mBAARA,0BACbwD,GAAgBxD,GAAI,uBACpBA;;;eAxBc5G;UA9FL,eA+FMA,mBAtFgC,mBAsFhCA,yBAAY;2BAEVqK,GAAgBzK,GAAOI,GAAWkC;cACvD/B,WACFqJ,MAAmBtH,IAEnB2H,EADMG,IAAoB,IAAI/P,KACckG,IAC5C4J,EAAyB/J,GAAWgK;;;eAxBxChK,GACAgK;MAGAA,oBAA0B9H;YACpBA,MAAQlC,OAAe;cACnB4G,IAAKuD,MAAQjI;qBACf0E,MACFuD,SAAWjI,IACXoI,qBAA0BF,GAAgBxD,GAAI;;;;eA3BpDoD,GACA7J;WAEqBzD,MAAjByD,KAEFA,sBAAqB8J;YACb9N,IAAO+N,EAAKD;iBACLvN,MAATP,GAAoB;UACtB+N,EAAKD,KAAO;cACC;eAARtO,IAAWmB,IAAIX,UAAaU,IAAIC,GAAGD;YACtCmN,MAAsB7N,EAAKU;;;;;eAsK9BsK;0CAEOoD,IAA0BvK;UACA;QAC9BA,WAAWwK,GAAgBxK,GAAW6G;QACtCzK,MAAM+K;QACNtM,OAAOsM;QACPsD,YAAYtD;;UAID,wBAAXoD,KACY,kBAAXA,KAAwC,cAAZ1D;QAE7B3G,WAAe,GACfoK,qBACEF,GAAgBpK,GAAW;;;;;UAvN5BsJ,IAAO;QAEZ1J,IAAQ,IAAI8C,EAChB4G,WAAc,IAAI3E,EAAiB2E,iBAAe5M,GAClD4M,aACAA,WACAA,cACAA;QAIEA,WAAc;UACV1F,IAAU0F;UAChBoB,IAAY9G,0BAAoB+G;QAClB/K,IAAAA,IAAAA,QAAYgE,IAAAA;UToTdxH,GAAM;aACfrC,IAAMmI,QAAgB;cACnB0I,IAAW1I,UAAY,MACvB1F,IAAY0F,QAAU,GAAG0I;cACd1I,QAAU0I,IAAW;kBAC9B1I,aAAe;gBAChB;YACH2I,GAAUrO,GAAWpB,GS3TUuP,ET2TQzI;;;gBAEpC;YACH4I,EAAYtO,GAAWpB,GS9TQuP,ET8TUzI;;;QAI/CX;oBACeqC;;;QS/TT4F,IAAiB,IAAIvP,KACrBkQ,IAAoB,IAAIjO,KACxBgO,IAA4BlO;oBAyI3B+O;MACCC,IAAwBC,MAANF;UAIlBG,IAAeR,IAKfS,SAASC,UAATD,CADAE,KAAK,EAALA,CADAC,OAAOC,YAAYb,GAAnBY,CADAN,OAKDQ;UAMHP,MADAQ,IAAIC,EAAJD,CADAlP,IAAIoP,GAAJpP,CADAqP,OAAO,EAACV,GAAcF;UAOlBa,IAIJZ,MADA1O,IAAIuP,EAAJvP,CADAwP,UAAAA,CADAC;UAUAzP,OAAAA,CADAwP,UAAAA,CADAF;UAUAtP,MAAAA,CADAwP,UAAAA,CADAF;UAwCAtP,IAAI0P,EAAJ1P,CATA2P,EACEC,MAAM,EAGFJ,UAAAA,CADAC,IAGFI;mBAMO,EAACC,GAASC;;;;;;;;;;ECrUM/C;;IAEMA;;;IAuETrH;MAnCCqH;qDAK3BgD,EACIC,SAAAA;;QAIiBtK,gBAwDvBc,GACAxB;cAEMiL,IAA+C,IAC/CC,IAAyC,IACzCrE,IAAW,IAAIC,SAAStF;gBAG5BxB,GACAgH,kBAAkBH,GAAU;YAC1BI,gBAAO7O;kBACDA,gBAAmB;gBA6JvBC,IAAAA,IAAOC,GA5JoBuO;kBA8J/BxO,MAAS8O,eAAe9O,4CACxB,iFACA;oBAGKA;uBAlKmB;kBAChBsO,MAAM9N;kBACN+D,eAAe;oBACb+J,MAAM9N;oBACN8E,MAAMiJ,GAASvO;;kBAEjBsF,MAAMiJ,GAAYvO;kBAClBkP,cAAcnP;;;;YAIpB+S,6BAAoB/S;cAClB6S,OAAwB7S;;;iBAKvB,EAAC6S,GAAoBC;UApFiBE,GAAAA;QAEzCpL;;4BAdmEqL,sBAiBfhQ;;UAChBA,EACpC4K,EAAAA,MAAchJ;;QAAdgJ,IAFyD;QAKlD5K;UACuBA,IAM9BiQ,GAP8CjQ,MAO9CiQ,IALarO,oBAKbqO,EAAAA,OAAAA,OAHoDjT;YAGpDiT;;;;;;;IAAAA;MArD4BlG;;;MAErBA,IAIEzH;WAAAA;QACT4N,EAAiB5N,KAAQqI,EAAoBrI;;qBAM1CyH;QACHpF,OAAOwL,GACLhK,GACA4D,SACAmG,GACAtF;;;eAXwDwF;mBACnCA;;uBAnBrBjK,IAAS4B,kBAAkBsI,IAE3BV,IAAmB,IAAIvS,KAEvBsS,IAAmB,IAAItS,KAEvBwN,IAAiCzL,KAEjCwL,IAAuCxL;oBAkEtC+O;eAKHxO,IAAI4Q,EAAJ5Q,CADAkP,IAAI2B,EAAJ3B,CADAA,IAAI4B,EAAJ5B,CADAV;;;;;;;;;;;;;;;"}